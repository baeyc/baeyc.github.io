---
title: "Statistiques computationnelles"
author: <font size="5"> Charlotte Baey </font>
date: <font size="5"> M1 MA - 2024/2025 </font>
output:
  xaringan::moon_reader:
    css: [default, metropolis, metropolis-fonts, hygge]
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      slideNumberFormat: "%current%"
---

<style>

.remark-slide-content {
  background-color: #FFFFFF;
  border-top: 80px solid #16A085;
  font-size: 20px;
  line-height: 1.5;
  padding: 1em 2em 1em 2em
}

.my-one-page-font {
  font-size: 20px;
}

.remark-slide-content > h1 {
  font-size: 38px;
  margin-top: -85px;
}

.inverse {
  background-color: #16A085;
  border-top: 80px solid #16A085;
  text-shadow: none;
	background-position: 50% 75%;
  background-size: 150px;
  font-size: 40px
}

.title-slide {
  background-color: #16A085;
  border-top: 80px solid #16A085;
  background-image: none;
}

.remark-slide-number {
  position: absolute;
}

.remark-slide-number .progress-bar-container {
  position: absolute;
  bottom: 0;
  height: 4px;
  display: block;
  left: 0;
  right: 0;
}

.remark-slide-number .progress-bar {
  height: 100%;
  background-color: grey;
}

.left-column {
  width: 20%;
  height: 92%;
  float: left;
}
.left-column h2:last-of-type, .left-column h3:last-child {
  color: #000;
}
.right-column {
  width: 75%;
  float: right;
  padding-top: 1em;
}


.left-column2 {
  width: 60%;
  height: 92%;
  float: left;
}
.right-column2 {
  width: 35%;
  height: 92%;
  float: left;
}

</style>

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: {
    Macros: {
      yellow: ["{\\color{yellow}{#1}}", 1],
      orange: ["{\\color{orange}{#1}}", 1],
      green: ["{\\color{green}{#1}}", 1]
    },
    loader: {load: ['[tex]/color']},
    tex: {packages: {'[+]': ['color']}}
  }
});
</script>

```{css, echo=FALSE}
.left-code {
  width: 40%;
  height: 92%;
  float: left;
}
.right-plot {
  width: 59%;
  float: right;
  padding-left: 1%;
}
```

```{css, echo=FALSE}
.left-plot {
  width: 59%;
  float: left;
}
.right-code {
  width: 40%;
  float: right;
  padding-left: 1%;
}
```


```{r setup, include = FALSE}

# R markdown options
#knitr::opts_chunk$set(echo = TRUE, 
                      #cache = TRUE, 
                      #fig.width = 10,
                      #fig.height = 5,
                      #fig.align = "center", 
                      #message = FALSE)

# Load packages
library(gapminder)
library(gganimate)
library(ggplot2)
library(tidyverse)
library(xaringancolor)
library(flair)
yellow <- "#FFCC29"
orange <- "#F58634"
green <- "#007965"
cprior <- "#ffe933"
cobs <- "#2a6099"
cpost <- "#77bc65"
```

```{r, echo=FALSE}
library(reticulate)
Sys.setenv('RETICULATE_PYTHON'='/usr/bin/python3')
```

# Quelques informations pratiques

### Plan du cours
1. Méthodes de ré-échantillonnage
2. Méthodes de Monte-Carlo
3. Introduction aux statistiques bayésiennes
4. Algorithme EM (s'il reste du temps)

### Organisation

- 2 séances de cours d'1h30 par semaine ($\times$ 11 semaines)
- 2 séances de TD/TP de 2h par semaine ($\times$ 12 semaines)

### Evaluation

- 1 DS intermédiaire d'une durée de 2h
- 1 Projet **à effectuer en binôme**
- 1 DS final d'une durée de 3h

.red[**Aucun document autorisé lors des examens.**]

---
# Sommaire

.pull-left[
**1. Méthodes de ré-échantillonnage**
  - [Cours 1](#c1) (13/01/2025)
  - [Cours 2](#c2) (14/01/2025)
  - [Cours 3](#c3) (20/01/2025)
  - [Cours 4](#c4) (21/01/2025)
  - [Cours 5](#c5) (27/01/2025)
  
**2. Méthodes de Monte-Carlo**
  - [Cours 6](#c6) (28/01/2025)
  - [Cours 7](#c7) (03/02/2025)
  - [Cours 8](#c8) (04/02/2025)
  - [Cours 9](#c9) (10/02/2025)
  - [Cours 10](#c10) (11/02/2025)

**3. Méthodes MCMC**
  - [Cours 11](#c11) (24/02/2025)
  - [Cours 12](#c12) (25/02/2025)
]
.pull-right[
**3. Méthodes MCMC (suite)**
  - [Cours 13](#c13) (04/03/2025)
  - [Cours 14](#c14) (10/03/2025)
  
** 4. Statistiques bayésiennes**
  - [Cours 15](#c15) (11/03/2025)
]

---
name: c1
class: inverse, middle, center

# Introduction


---
class: my-one-page-font 
# C'est quoi les statistiques computationnelles ?

<br>

<br>

 - C'est le recours (plus ou moins) intensif à l'ordinateur pour répondre à des questions statistiques que l'on ne sait pas (ou difficilement) résoudre autrement.
 
 - On utilise/développe/étudie des algorithmes, des astuces numériques/statistiques/computationnelles 
 - L'objectif est de faire de l'inférence, d'étudier la robustesse de méthodes statistiques, de traiter de grands jeux de données, ...
 
 

---

class: inverse, middle, center

# I. Méthodes de ré-échantillonnage

---

# Notion d'échantillon

Qu'est-ce qu'un échantillon ?

- une suite de variables aléatoires $\mathcal{X} = (X_1, \dots, X_n)$
- dont on observe une réalisation $\mathcal{X}(\omega) = (X_1(\omega), \dots, X_n(\omega))$

---
# Notion d'échantillon

Qu'est-ce qu'un échantillon ?

- une suite de variables aléatoires $\mathcal{X} = (X_1, \dots, X_n)$
- dont on observe une réalisation $\mathcal{X}(\omega) = (X_1(\omega), \dots, X_n(\omega))$

Que représente $\omega$ ?
- l'aléa autour de l'expérience (ex. : $n$ lancers d'une pièce de monnaie)
- cet aléa $\omega \in \Omega$ est transporté dans $\mathbb{R}$ via $X_i$
<!-- - on a seulement accès à la variabilité sur $\mathbb{R}$ -->
- en général, on ne dispose que d'une seule réalisation, pour un $\omega$ donné

---
# Statistique paramétrique

```{r, out.width='2000px', echo = FALSE}
magick::image_read_pdf("/home/charlotte/Documents/Enseignement/Lille/M1 ISN /Statistiques computationnelles/Poly/slides.pdf",pages=34)
```

---
# Statistique paramétrique

```{r, out.width='2000px', echo = FALSE}
magick::image_read_pdf("/home/charlotte/Documents/Enseignement/Lille/M1 ISN /Statistiques computationnelles/Poly/slides.pdf",pages=35)
```

---
# Statistique paramétrique

```{r, out.width='2000px', echo = FALSE}
magick::image_read_pdf("/home/charlotte/Documents/Enseignement/Lille/M1 ISN /Statistiques computationnelles/Poly/slides.pdf",pages=36)
```

---
# Statistique paramétrique

```{r, out.width='2000px', echo = FALSE}
magick::image_read_pdf("/home/charlotte/Documents/Enseignement/Lille/M1 ISN /Statistiques computationnelles/Poly/slides.pdf",pages=37)
```

---
# Statistique paramétrique

```{r, out.width='2000px', echo = FALSE}
magick::image_read_pdf("/home/charlotte/Documents/Enseignement/Lille/M1 ISN /Statistiques computationnelles/Poly/slides.pdf",pages=38)
```

---
# Statistique paramétrique

```{r, out.width='2000px', echo = FALSE}
magick::image_read_pdf("/home/charlotte/Documents/Enseignement/Lille/M1 ISN /Statistiques computationnelles/Poly/slides.pdf",pages=39)
```


---
# Statistique paramétrique

```{r, out.width='2000px', echo = FALSE}
magick::image_read_pdf("/home/charlotte/Documents/Enseignement/Lille/M1 ISN /Statistiques computationnelles/Poly/slides.pdf",pages=40)
```

---
# Statistique paramétrique

```{r, out.width='2000px', echo = FALSE}
magick::image_read_pdf("/home/charlotte/Documents/Enseignement/Lille/M1 ISN /Statistiques computationnelles/Poly/slides.pdf",pages=41)
```

---
# Statistique paramétrique

```{r, out.width='2000px', echo = FALSE}
magick::image_read_pdf("/home/charlotte/Documents/Enseignement/Lille/M1 ISN /Statistiques computationnelles/Poly/slides.pdf",pages=42)
```

---
# Statistique paramétrique

```{r, out.width='2000px', echo = FALSE}
magick::image_read_pdf("/home/charlotte/Documents/Enseignement/Lille/M1 ISN /Statistiques computationnelles/Poly/slides.pdf",pages=43)
```


---
# Un exemple simple 

Soit $(X_1,\dots,X_n)$ un échantillon gaussien i.i.d. de loi $\mathcal{N}(\theta,1)$, et $\hat{\theta}$ l'EMV de $\theta$. Quelle est la loi de $\hat{\theta}$ ?

--

.pull-left[
```{r, eval=F}
theta_vrai <- 2; n <- 100; N <- 500
hat_theta <- rep(0,N)
for (i in 1:N){
  ech_i <- rnorm(n,theta_vrai,1)
  hat_theta[i] <- mean(ech_i)
}
hist(hat_theta,freq=F)
curve(dnorm(x,theta_vrai,1/sqrt(n)))
```
]
.pull-right[
```{r, fig.height=5,fig.width=6,fig.retina=2,results='hold',echo=F}
theta_vrai <- 2; n <- 100; N <- 500
hat_theta <- rep(0,N)
for (i in 1:N){
  ech_i <- rnorm(n,theta_vrai,1)
  hat_theta[i] <- mean(ech_i)
}
hist(hat_theta,freq=F,main="Histogramme des estimations",ylim=c(0,4))
curve(dnorm(x,theta_vrai,1/sqrt(n)),
      lwd=2,col="red",add=T)
```
]
---
class: my-one-page-font 
# Jackknife

Comment construire de nouveaux échantillons ?

```{r, fig.height=6,fig.width=10,fig.retina=2, echo=F}
library(RColorBrewer)
pal <- brewer.pal(n = 8, name = "Dark2")
n <- 8
set.seed(2)
ech <- sort(rnorm(n,0,1))
ech_j <- matrix(0,nrow=n,ncol=7)
for (i in 1:n){
  ech_j[i,] <- ech[-i]
}

mean_ech <- mean(ech)
mean_j <- apply(ech_j,1,mean)
par(oma=c(0,4,0,0))
plot(ech,rep(1.9,n),col=pal,pch=19,cex=4,ylim=c(0.1,2),yaxt="none",xlab="Observations",ylab="")
points(mean_ech,1.9,col="red",pch=15,cex=2)
mtext("éch. initial",side=2,at=1.9,las=1,adj=1.1)
```

---
class: my-one-page-font 
# Jackknife

Comment construire de nouveaux échantillons ?

```{r, fig.height=6,fig.width=10,fig.retina=2, echo=F}
par(oma=c(0,4,0,0))
plot(ech,rep(1.9,n),col=pal,pch=19,cex=4,ylim=c(0.1,2),yaxt="none",xlab="Observations",ylab="")
points(mean_ech,1.9,col="red",pch=15,cex=2)
mtext("éch. initial",side=2,at=1.9,las=1,adj=1.1)
for (i in 1:n){
  abline(h=1.8-0.2*i,col="grey")
  points(ech_j[i,],rep(1.8-0.2*i,n-1),col=pal[-i],cex=3,pch=19)
  points(mean_j[i],1.8-0.2*i,col="red",pch=15,cex=2)
  mtext(paste("éch. jackknife",i),side=2,at=1.8-0.2*i,las=1,adj=1.1)
}
```


---
# Jackknife 

#### 1. Réduction du biais

 - Estimation du biais : 

$$\hat{b}_{jack} = (n-1)(\hat{\theta}_{jack} - \hat{\theta}),$$ avec $\hat{\theta}_{jack} = \frac{1}{n}\sum_{i=1}^n \hat{\theta}_{(i)}$.

 - Pseudo-valeurs :

$$\tilde{\theta}_{(i)} = n\hat{\theta} - (n-1)\hat{\theta}_{(i)}$$
<!-- Sur l'exemple précédent, cela donne : -->
 <!-- \tilde{\theta}_{(1)} = \tilde{\theta}_{(2)} = \tilde{\theta}_{(3)} = \tilde{\theta}_{(7)} = \tilde{\theta}_{(8)} = \tilde{\theta}_{(10)} = \tilde{\theta}_{(12)} = 13\times0.5384 - 12\times0.50 = 0.9992
 \tilde{\theta}_{(4)} = \tilde{\theta}_{(5)} = \tilde{\theta}_{(6)} = \tilde{\theta}_{(9)} = \tilde{\theta}_{(11)} = \tilde{\theta}_{(13)} = 13\times0.5384 - 12\times0.5833 = -0.0004 -->


 - Estimateur jackknife corrigé du biais :

$$\hat{\theta}^*_{jack} = \hat{\theta} - \hat{b}_{jack} = \frac{1}{n}\sum_{i=1}^n \tilde{\theta}_{(i)}$$
.red[**Réduire le biais n'implique pas nécessairement une amélioration de l'estimateur (au sens du risque quadratique)**]


---
# Jackknife

#### 2. Estimation de la variance

 $$\hat{s}^2_{jack} = \frac{n-1}{n} \sum_{i=1}^n \big(\hat{\theta}_{(i)} - \hat{\theta}^*_{jack}\big)^2$$
 
avec les pseudo-valeurs : $\hat{s}^2_{jack} = \frac{1}{n(n-1)} \sum_{i=1}^n \big(\tilde{\theta}_{(i)} - \tilde{\theta}\big)^2$

--

#### 3. Construction d'intervalles de confiance

Si existence d'un TCL :
- en utilisant l'estimateur jackknife de la variance : 
 $$\hat{I}_{jack} = \left[\hat{\theta} - q^{\mathcal{N(0,1)}}_{1-\alpha/2} \hat{s}_{jack} ; \hat{\theta} + q^{\mathcal{N(0,1)}}_{1-\alpha/2} \hat{s}_{jack} \right]$$

- en utilisant les deux estimateurs :
  $$\hat{I}_{jack} = \left[\hat{\theta}_{jack} - q^{\mathcal{N(0,1)}}_{1-\alpha/2} \hat{s}_{jack} ; \hat{\theta}_{jack} + q^{\mathcal{N(0,1)}}_{1-\alpha/2} \hat{s}_{jack} \right]$$

<span style="color:#16A085">**fin du cours 1 (13/01/2025)**</span>


---
name: c2
# Résumé et remarques

<br>
- le jackknife est une méthode **non-paramétrique** permettant d'estimer le biais et la variance d'un estimateur à l'aide de simulations 

- la consistance est garantie pour un grand nombre d'estimateurs **suffisamment réguliers**

- les hypothèses de régularité sont toutefois plus strictes que pour un TCL par exemple :
 - la delta-méthode requiert la différentiabilité de $g$ en $\mu=\mathbb{E}(X_1)$
 - la consistance de $\hat{s}_{jack}$ requiert que $g'$ soit continue en $\mu$
 - ex. d'estimateur pour lequel $\hat{s}_{jack}$ n'est pas consistant : la médiane empirique (malgré existence TCL)
 
- extensions possibles reposant sur des hypothèses moins fortes : 
 - le *delete-d* jackknife
 - le jackknife infinitésimal
 <!-- - peut-être utilisé pour la médiane empirique -->
 <!-- - peut permettre d'estimer la distribution de $\hat{\theta}$ -->

<!--</br>
<span style="color:#16A085">**fin du cours 1 (22/01/2024)**</span>-->

---
# Du jackknife au bootstrap

 - $\hat{\theta} = T(\underbrace{X_1,\dots,X_n}_{\text{observations}},\underbrace{\omega_1,\dots,\omega_n}_{\text{poids des obs.}}) = T(X_1,\dots,X_n,\frac{1}{n},\dots,\frac{1}{n})$
 
 - réplication jackknife : $\hat{\theta}_{(i)} = T(X_1,\dots,X_{i-1},X_i,X_{i+1},X_n,\frac{1}{n-1},\dots,\frac{1}{n-1},0,\frac{1}{n-1},\dots,\frac{1}{n-1})$

--

 - Idée du bootstrap : mettre des poids **aléatoires**

--

 - Procédure de ré-échantillonnage : tirer uniformément et **avec remise** parmi les observations de $\mathcal{X}$, pour construire un échantillon *bootstrap* de taille $n$ noté $\mathcal{X}^*$ :
 
 - Puis sur chaque échantillon bootstrap, on construit la statistique bootstrapée $\hat{\theta}^* = T(\mathcal{X}^*)$

---
# Bootstrap

```{r, out.width='2000px', echo = FALSE}
magick::image_read_pdf("/home/charlotte/Documents/Enseignement/Lille/M1 ISN /Statistiques computationnelles/Poly/slides.pdf",pages=25)
```

---
# Bootstrap

```{r, out.width='2000px', echo = FALSE}
magick::image_read_pdf("/home/charlotte/Documents/Enseignement/Lille/M1 ISN /Statistiques computationnelles/Poly/slides.pdf",pages=26)
```

---
# Bootstrap

```{r, out.width='2000px', echo = FALSE}
magick::image_read_pdf("/home/charlotte/Documents/Enseignement/Lille/M1 ISN /Statistiques computationnelles/Poly/slides.pdf",pages=27)
```


---
# Bootstrap

```{r, out.width='2000px', echo = FALSE}
magick::image_read_pdf("/home/charlotte/Documents/Enseignement/Lille/M1 ISN /Statistiques computationnelles/Poly/slides.pdf",pages=28)
```

---
# Bootstrap

```{r, out.width='2000px', echo = FALSE}
magick::image_read_pdf("/home/charlotte/Documents/Enseignement/Lille/M1 ISN /Statistiques computationnelles/Poly/slides.pdf",pages=29)
```

---
# Bootstrap

```{r, out.width='2000px', echo = FALSE}
magick::image_read_pdf("/home/charlotte/Documents/Enseignement/Lille/M1 ISN /Statistiques computationnelles/Poly/slides.pdf",pages=30)
```

---
# Bootstrap

```{r, out.width='2000px', echo = FALSE}
magick::image_read_pdf("/home/charlotte/Documents/Enseignement/Lille/M1 ISN /Statistiques computationnelles/Poly/slides.pdf",pages=31)
```

---
# Bootstrap

```{r, out.width='2000px', echo = FALSE}
magick::image_read_pdf("/home/charlotte/Documents/Enseignement/Lille/M1 ISN /Statistiques computationnelles/Poly/Figures/boot_samples.pdf")
```



---
# Fonction de répartition empirique

 
```{r, echo=FALSE}
set.seed(10101)
xi <- rnorm(10)
print(round(xi,2))
```

--

```{r, echo=FALSE, warning=FALSE, fig.height=7, fig.width=10,fig.retina=2}
plot(ecdf(xi),xaxt='n',main="",col="white")
mtext(round(xi,2),at=xi,side=1,line=1,cex=0.8)
mtext(paste0("X(",1:10,")"),at=xi,side=1,line=2,cex=0.75)
axis(1,at=sort(xi),labels=rep("",10),cex.axis=0.75)
```

---
# Fonction de répartition empirique

 
```{r, echo=FALSE}
set.seed(10101)
xi <- rnorm(10)
print(round(xi,2))
```


```{r, echo=FALSE, warning=FALSE, fig.height=7, fig.width=10,fig.retina=2}
plot(ecdf(xi),xaxt='n',main="")
mtext(round(xi,2),at=xi,side=1,line=1,cex=0.8)
mtext(paste0("X(",1:10,")"),at=xi,side=1,line=2,cex=0.75)
axis(1,at=sort(xi),labels=rep("",10),cex.axis=0.75)
```

---

# Fonction de répartition empirique

 - Echantillon exponentiel
 
```{r, echo=FALSE, warning=FALSE, fig.height=4, fig.width=16,fig.retina=2}
n <- c(10,20,50,100,200,500)
d <- lapply(n,FUN = function(i){return(data.frame(x=rexp(i*100,1),rep=rep(1:100,each=i),n=rep(i,i*100)))})
d <- do.call(rbind,d)

d %>% ggplot(aes(x,group=rep)) + stat_ecdf(geom="step",col="darkgrey") + stat_function(fun=pexp) + facet_wrap(~n,nrow = 1) 
```

 - Echantillon uniforme
 
```{r, echo=FALSE, warning=FALSE, fig.width=16, fig.height=4,fig.retina=2}
d <- lapply(n,FUN = function(i){return(data.frame(x=runif(i*100),rep=rep(1:100,each=i),n=rep(i,i*100)))})
d <- do.call(rbind,d)

d %>% filter(n<1000) %>% ggplot(aes(x,group=rep)) + stat_ecdf(geom="step",col="darkgrey") + stat_function(fun=punif) + facet_wrap(~n,nrow=1) + xlim(c(-0.2,1.2))
```

---
# Bootstrap

.pull-left[
### Monde réel
.left[
<span style="color:white">**on raisonne conditionnellement à $F_n$**</span>
- échantillon $\mathcal{X} = (X_1,\dots,X_n)$
- $X_i$ de loi inconnue $F$
- paramètre $\theta(F)$
- estimateur $\hat{\theta} = T(\mathcal{X})$
- loi de $\hat{\theta}$ : $G$ inconnue
]
]
.pull-right[
### Monde Bootstrap 
.left[
<span style="color:red">**on raisonne conditionnellement à $F_n$**</span>
- échantillon bootstrap $\mathcal{X}^* = (X_{1}^*,\dots,X_{n}^*)$
- $X_{i}^*$ de loi connue $F_n$
- paramètre $\theta(F_n) = \hat{\theta}$
- statistique bootstrapée $\hat{\theta}^* = T(\mathcal{X}^*)$
- loi de $\hat{\theta}^*$ : $G^*$ connue
]
]

</br>
.center[
$G$ inconnue $\longrightarrow$ $G^*$ connue $\longrightarrow$ $\hat{G}^*_B$ approximation bootstrap
]

</br>
<span style="color:#16A085">**fin du cours 2 (14/01/2025)**</span>


---
name: c3
# Bootstrap : principe du plug-in

 - on remplace $F$ par sa version empirique $F_n$ 


```{r, echo=FALSE, fig.width=12, fig.height=6,fig.retina=2}
set.seed(1999)
x <- rnorm(15)
par(mfrow=c(1,2))
curve(pnorm(x),from=-2.5,to=2.5,xlab="",ylab="",main=expression(Loi~des~X[i]))
plot(ecdf(x),xaxt='n',xlim=c(-2.5,2.5),main=expression(Loi~des~X[bi]^"*"))
text(sort(x),y = par("usr")[3]-0.075,labels=paste0("X(",1:15,")"),xpd=NA,cex=0.75,srt=45)
axis(1,at=sort(x),labels=rep("",15),cex.axis=0.75)
lines(seq(-3,3,0.1),pnorm(seq(-3,3,0.1)),xlab="",ylab="",col="grey")
```
<!-- .pull-left[ -->
<!-- .center[ -->
<!-- espérance : $\theta = \int x dF(x)$ ]]-->
<!-- .pull-right[ -->
<!-- espérance : $\int x dF_n(x) = \frac{1}{n} \sum_{i=1}^n X_i$] -->

---
# Bootstrap : principe du plug-in

#### Exemple  
$\theta = \mathbb{E}(X_1) = \int x dF(x)$ 

--

Méthode du plug-in : on estime $\theta$ par $\hat{\theta} = \int x dF_n(x) = \frac{1}{n}\sum_{i=1}^n X_i$.

--

Dans l'échantillon bootstrap, les $X_{b,i}^*$ sont distribuées selon la loi $F_n$, et ont pour espérance

$\theta^* = \int x dF_n(x) = \hat{\theta}$

--

La statistique bootstrapée $\hat{\theta}^*_b = \frac{1}{n}\sum_{i=1}^n X_{b,i}^*$ est donc à $\theta^*$ ce que $\hat{\theta}$ est à $\theta$.

---
# Bootstrap 

#### Exemple 2 : estimation du biais de $\hat{\theta}$

Paramètre $\theta(F)$ estimé par $\hat{\theta} = \theta(F_n)$

--

Biais : $b(\hat{\theta}) = \mathbb{E}_F(\hat{\theta}) - \theta = \int xdG(x) - \theta(F)$

--

Dans le monde bootstrap : 

$$b^*(\hat{\theta}) = \int x dG^*(x) - \theta(F_n) = \int x dG^*(x) - \hat{\theta}$$

--

Estimateur du bootstrap :

$$\hat{b}^*(\hat{\theta}) = \int x d\hat{G}^*_B(x) - \hat{\theta} = \frac{1}{B} \sum_{i=1}^B \hat{\theta}^*_b - \hat{\theta}$$

---
# Bootstrap

#### Exemple 3 : estimation de la variance de $\hat{\theta}$

Variance : $\text{Var} (\hat{\theta}) = \mathbb{E}_F\big[(\hat{\theta} - \mathbb{E}(\hat{\theta}))^2\big] = \int (x - \int x dG(x))^2dG(x)$

--

Dans le monde bootstrap :

$$\text{Var}^*(\hat{\theta}) = \int (x - \int x dG^*(x))^2 dG^*(x)$$

--

Estimateur du bootstrap :

$$\hat{\text{Var}}^*(\hat{\theta}) = \int (x - \int x d\hat{G}_B^*(x))^2 d\hat{G}_B^*(x) = \frac{1}{B} \sum_{b=1}^B \big(\hat{\theta}^*_b - \frac{1}{B} \sum_{b=1}^B \hat{\theta}^*_b \big)^2$$

$\rightarrow$ c'est la variance empirique des statistiques bootstrapées $\hat{\theta}^*_1,\dots,\hat{\theta}^*_B$.

</br>
<span style="color:#16A085">**fin du cours 3 (20/01/2025)**</span>

---
name: c4
# Construction d'intervalles de confiance

#### Méthode du bootstrap classique
Point de départ : l'existence d'une quantité pivotale, par exemple $\hat{\theta} - \theta$, de loi $H$.

--
 
```{r echo=FALSE, fig.width=5,fig.height=5,warning=FALSE,fig.retina=2} 
library(ggplot2)
x <- seq(0,13,0.01)
y <- dgamma(x,2.5,1)
d <- data.frame(x=rep(x,2),y=rep(y,2),type=rep(c("HDI","Quantiles"),each=length(x)),
                xmin=c(qgamma(0.025,2.5,1)),xmax=c(qgamma(0.975,2.5,1)))
ggplot(data=d,aes(x,y)) + geom_line() + stat_function(fun = dgamma,args = list(shape=2.5, rate=1),geom = "area",
                                                         fill = "red", alpha = 0.5, xlim = c(0,qgamma(0.025,2.5,1))) +
                                        stat_function(fun = dgamma,args = list(shape=2.5, rate=1),geom = "area",
                                                         fill = "red", alpha = 0.5, xlim = c(qgamma(0.975,2.5,1),10)) +
  ggtitle(expression(paste("Loi (densité) de ",hat(theta)-theta))) + xlim(c(-0.75,10)) + annotate(geom="text",x=qgamma(0.025,2.5,1)+0.15,y=-0.015,label="H^-1~(alpha/2)",parse=TRUE) + geom_vline(xintercept = 0) + geom_segment(x=qgamma(0.025,2.5,1),xend=qgamma(0.025,2.5,1),y=-0.005,yend=0.005) + geom_hline(yintercept = 0) + 
  annotate(geom="text",x=qgamma(0.975,2.5,1),y=-0.01,label="H^-1~(1-alpha/2)",parse=TRUE) + 
  geom_segment(x=qgamma(0.975,2.5,1),xend=qgamma(0.975,2.5,1),y=-0.005,yend=0.005) +
  geom_segment(x=7,xend=8,y=0.005,yend=0.07,lwd=0.1) + geom_segment(x=0.25,xend=-0.5,y=0.0075,yend=0.07,lwd=0.1) +
  annotate(geom="text",x=-0.55,y=0.08,label="alpha/2",parse=TRUE) + annotate(geom="text",x=8.1,y=0.08,label="alpha/2",parse=TRUE)
```
 
--

IC : $I(1-\alpha) =  \big[\hat{\theta} - H^{-1}(1-\alpha/2); \hat{\theta} - H^{-1}(\alpha/2)\big]$
 
---
# Construction d'intervalles de confiance
 
Version bootstrap empirique : $\hat{I}^*(1-\alpha) =  \big[\hat{\theta} - (\hat{H}^*_B)^{-1}(1-\alpha/2); \hat{\theta} - (\hat{H}^*_B)^{-1}(\alpha/2)\big]$
 
#### Calcul des quantiles de $\hat{H}^*_B$
 
 - $H^*(x) = G^*(x + \hat{\theta})$
 
--
 
```{r echo=FALSE, fig.width=7,fig.height=5, warning=FALSE,fig.retina=2} 
d <- data.frame(x=rep(x,2),y=c(pgamma(x,2.5,1),pgamma(x-2.5,2.5,1)),type=rep(c("H","G"),each=length(x)),
                xmin=c(qgamma(0.025,2.5,1)),xmax=c(qgamma(0.975,2.5,1)))
ggplot(data=d,aes(x,y,col=type)) + geom_line(lwd=1.5) + scale_color_discrete("Fonction") + xlim(c(0,12)) +
   annotate(geom="text",x=1.4,y=-0.028,label="x",parse=TRUE) + geom_segment(x=1.45,y=0,xend=1.45,yend=0.31,linetype=2,col="black") + geom_segment(x=4,y=0,xend=4,yend=0.31,linetype=2,col="black") +
  annotate(geom="text",x=4,y=-0.01,label="x+hat(theta)",parse=TRUE) + geom_segment(x = 1.45, y = 0.31, xend = 4, yend = 0.31, arrow = arrow(length = unit(0.015, "npc"), ends = "both"), lwd=0.1, col="black") +   annotate(geom="text",x=2.75,y=0.32,label="hat(theta)",parse=TRUE)
```

--
 
 - $(H^*)^{-1}(y) = (G^*)^{-1}(y) - \hat{\theta}$
 
---
# Construction d'intervalles de confiance

#### Méthode du bootstrap classique

$$\hat{I}_{boot}^*(1-\alpha) =  \big[2\hat{\theta} - \hat{\theta}^*_{(\lceil B(1-\alpha/2)\rceil )} ; 2\hat{\theta} - \hat{\theta}^*_{(\lceil B \alpha/2\rceil )}  \big]$$
---
# Construction d'intervalles de confiance

#### Méthode percentile

<span style="color:red">Hypothèse : il existe une transformation $h$ **croissante** telle que la loi de $h(\hat{\theta})$ soit symétrique autour de $\eta = h(\theta)$</span>


IC pour $\eta$ : 
$$IC_\eta(1-\alpha) = \left[U - H^{-1}_U\left( 1 - \frac{\alpha}{2}\right) ; U - H^{-1}_U \left(\frac{\alpha}{2}\right)\right]. $$

 - $h$ inconnue ?
 - $H_U$ inconnue ?

---
# Construction d'intervalles de confiance

Démarche :
 1. construire les échantillons bootstrap $\mathcal{X}_1^*, \dots, \mathcal{X}_B^*$
 2. construire les statistiques bootstrapées $\hat{\theta}^*_1, \dots, \hat{\theta}^*_B$ et leurs transformations par $h$ : $U_b^* = h(\hat{\theta}^*_b), \forall b$
 3. définir la fonction de répartition de la statistique bootstrap : $$H^*_{U} (x) = \mathbb{P}(U_b^* - U \leq x \mid F_n)$$
 4. intervalle de confiance bootstrap pour $\eta = h(\theta)$ : $$IC_\eta^*(1-\alpha) = \left[U - H^{*-1}_{U}\left(1 -\frac{\alpha}{2} \right) ; U - H^{* \ -1}_{U}\left(\frac{\alpha}{2} \right) \right]$$
 
 
 
---
# Construction d'intervalles de confiance

.pull-left[
```{r echo=FALSE, fig.width=5,fig.height=5,warning=FALSE,fig.align='center',fig.retina=2} 
library(ggplot2)
x <- seq(-4,4,0.01)
y <- dnorm(x)
d <- data.frame(x=x,y=y,xmin=qnorm(0.025),xmax=qnorm(0.975))
ggplot(data=d,aes(x,y)) + geom_line() + stat_function(fun = dnorm,geom = "area", fill = "red", alpha = 0.5, xlim = c(-3.5,qnorm(0.025))) + stat_function(fun = dnorm,geom = "area", fill = "red", alpha = 0.5, xlim = c(qnorm(0.975),3.5)) +
  ggtitle(expression(paste("Loi de ",h(hat(theta))-eta))) + xlim(c(-3.5,3.5)) + annotate(geom="text",x=qnorm(0.025)+0.15,y=-0.015,label="H[U]^-1~(alpha/2)",parse=TRUE) +   annotate(geom="text",x=qnorm(0.975),y=-0.015,label="H[U]^-1~(1-alpha/2)",parse=TRUE) + geom_vline(xintercept = 0) + geom_segment(x=qnorm(0.025),xend=qnorm(0.025),y=-0.005,yend=0.005) + geom_hline(yintercept = 0) +  
  geom_segment(x=qnorm(0.975),xend=qnorm(0.975),y=-0.005,yend=0.005) + geom_segment(x=2.5,xend=3.1,y=0.005,yend=0.07,lwd=0.1) + geom_segment(x=-2.5,xend=-3.1,y=0.005,yend=0.07,lwd=0.1) +
  annotate(geom="text",x=-3.2,y=0.08,label="alpha/2",parse=TRUE) + annotate(geom="text",x=3.2,y=0.08,label="alpha/2",parse=TRUE)
```
]
.pull-right[
</br>
</br>
</br>
</br>
$H^{* \ -1}_{U}\left(\frac{\alpha}{2} \right) = - H^{* \ -1}_{U}\left(1 - \frac{\alpha}{2} \right)$
]

$$IC_{\eta}^*(1-\alpha) = \left[U + H^{*-1}_{U}\left(\frac{\alpha}{2} \right) ; U + H^{* \ -1}_{U}\left(1-\frac{\alpha}{2} \right) \right]$$

--

$$IC_{\eta}^*(1-\alpha) = \left[G^{*-1}_{U}\left(\frac{\alpha}{2} \right) ; G^{* \ -1}_{U}\left(1-\frac{\alpha}{2} \right) \right]$$
<span style="color:#16A085">**fin du cours 4 (21/01/2025)**</span>


---
name: c5
# Construction d'intervalles de confiance

#### Méthode percentile

$$\hat{I}_{perc}^*(1-\alpha) =  \big[\hat{\theta}^*_{(\lceil B\alpha/2\rceil )} ; \hat{\theta}^*_{(\lceil B(1- \alpha/2)\rceil )}  \big]$$
</br>
</br>
</br>
</br>
</br>




---
# Construction d'intervalles de confiance

#### Méthode $t$-percentile

Repose sur l'existence d'une quantité pivotale de la forme (de loi notée $J_n$):

$$S_n = \sqrt{n} \ \frac{\hat{\theta} - \theta}{\hat{\sigma}}$$
--

IC classique : $IC(1-\alpha) = \left[\hat{\theta} - \frac{\hat{\sigma}}{\sqrt{n}} J_n^{-1}\left(1-\frac{\alpha}{2}\right) ; \hat{\theta} - \frac{\hat{\sigma}}{\sqrt{n}} J_n^{-1}\left(\frac{\alpha}{2}\right) \right]$


--

Démarche bootstrap :
1. construire les échantillons bootstrap $\mathcal{X}_1^*, \dots, \mathcal{X}_B^*$
2. construire les statistiques bootstrapées $S^*_b = \sqrt{n} \ \frac{\hat{\theta}^*_b - \hat{\theta}}{\hat{\sigma}^*_b}, \quad b=1,\dots,B$
3. approcher $J_n^{-1}$ par les quantiles empiriques des statistiques bootstrapées
	
	
$$\hat{I}_{t-\text{boot}}^*(1-\alpha) =  \left[\hat{\theta} - \frac{\hat{\sigma}}{\sqrt{n}} S^*_{\big(\lceil B(1-\frac{\alpha}{2})\rceil\big)}; \hat{\theta} - \frac{\hat{\sigma}}{\sqrt{n}} S^*_{\big(\lceil B(\frac{\alpha}{2})\rceil \big)} \right]$$

---
# Intervalles de confiance bootstrap

- IC du bootstrap classique :
$$\hat{I}_{boot}^*(1-\alpha) =  \big[2\hat{\theta} - \hat{\theta}^*_{(\lceil B(1-\alpha/2)\rceil )} ; 2\hat{\theta} - \hat{\theta}^*_{(\lceil B \alpha/2\rceil )}  \big]$$

- IC du bootstrap percentile :
 $$\hat{I}_{perc}^*(1-\alpha) =  \big[\hat{\theta}^*_{(\lceil B\alpha/2\rceil )} ; \hat{\theta}^*_{(\lceil B(1- \alpha/2)\rceil )}  \big]$$
 
- IC du bootstrap $t$-percentile :
 $$\hat{I}_{t-\text{boot}}^*(1-\alpha) =  \left[\hat{\theta} - \frac{\hat{\sigma}}{\sqrt{n}} S^*_{(\lceil B(1-\frac{\alpha}{2})\rceil)}; \hat{\theta} - \frac{\hat{\sigma}}{\sqrt{n}} S^*_{(\lceil B(\frac{\alpha}{2})\rceil)} \right]$$


---
# Paramétrique ou non paramétrique ?

- Jusqu'à présent, on a décrit des procédures de bootstrap dit **non-paramétrique**

--

- En effet, on n'a fait aucune hypothèse sur la loi $F$, ni exploité sa forme paramétrique

--

- Au contraire, on a échantillonné les $X_{b,i}^*$ selon la loi $F_n$, qui est un estimateur **non-paramétrique** de $F$

 </br>
 
--

- Il est possible de faire du bootstrap **paramétrique**

--

- Dans ce cas, on va utiliser la forme paramétrique de la loi $F$, que l'on note $F_\theta$.

--

- On échantillonne alors les $X_{b,i}^*$ selon la loi $F_{\hat{\theta}}$, qui est un estimateur **paramétrique** de $F$

<!-- <table> -->
<!--   <tr style="font-weight:bold"> -->
<!--     <td> </td> -->
<!--     <td>Bootstrap non paramétrique</td> -->
<!--     <td>Bootstrap paramétrique</td> -->
<!--   </tr> -->
<!--   <tr> -->
<!--     <td>hypothèse sur $F$</td> -->
<!--     <td>pas d'hypothèse particulière</td> -->
<!--     <td>$F = F_\theta$</td> -->
<!--   </tr> -->
<!-- <tr> -->
<!--     <td>échantillonnage</td> -->
<!--     <td>selon loi $F_n$</td> -->
<!--     <td>selon loi $F_{\hat{\theta}}$</td> -->
<!--   </tr>   -->
<!-- </table> -->

</br>

--

.center[.red[**RQ : c'est parfois la seule approche possible**]]

---
# Tests bootstrap

Procédure de test usuelle pour un test de niveau $\alpha$ :

--

- choix des hypothèses $H_0$ et $H_1$

--

- choix d'une statistique de test $T(\mathcal{X})$

--

- identification de la loi (éventuellement asymptotique) de $T(\mathcal{X})$

--

- déterminer la région de rejet **ou** calculer la $p$-valeur du test

--

- conclure

</br>
--

.center[.red[&rarr;** quelles sont les difficultés possibles ?**]]

---
# Tests bootstrap

Procédure de test usuelle pour un test de niveau $\alpha$ :


- choix des hypothèses $H_0$ et $H_1$ .green[**&rarr; ok**]


- choix d'une statistique de test $T(\mathcal{X})$ .green[**&rarr; ok**]


- identification de la loi (éventuellement asymptotique) de $T(\mathcal{X})$ .red[**&rarr; loi inconnue**]

- déterminer la région de rejet **ou** calculer la $p$-valeur du test .red[**&rarr; non calculable**]


- conclure


---
# Tests bootstrap

#### Exemple avec du bootstrap non paramétrique

$X_1,\dots,X_n$ i.i.d. de loi $F_1$ et $Y_1,\dots,Y_m$ i.i.d. de loi $F_2$.

$$H_0 : F_1 =  F_2 \quad \text{vs.} \quad H_1 : F_1 \neq F_2$$
 
--

On suppose que :
 - l'égalité en loi se traduit par une égalité de certains paramètres
--

 - l'on dispose d'une statistique de test $T(\mathcal{X})$ 

--

</br>
**Objectif** : construire une version bootstrap de $T$ <span style="color:red">**sous $H_0$**</span>

---
# Tests bootstrap

#### Exemple avec du boostrap paramétrique

$X_1,\dots,X_n$ i.i.d. de loi $F_{\theta,\eta}$, et

$$H_0 : \theta =  \theta_0 \quad \text{vs.} \quad H_1 : \theta \neq \theta_0$$

&rarr; $\eta$ est un *paramètre de nuisance*.

</br>
--

Exemple du TRV :

$$ T(X_1,\dots,X_n) = \frac{L(X_1,\dots,X_n ; \hat{\theta}, \hat{\eta}_1)}{L(X_1,\dots,X_n ; \theta_0, \hat{\eta}_0)},$$
</br>
--

<span style="color:red">**&rarr; la loi de $T$ sous $H_0$ n'est pas connue en présence de paramètres de nuisnace**</span>

</br>
<span style="color:#16A085">**fin du cours 5 (27/01/2025)**</span>

---
name: c6
class: inverse, middle, center

# II. Méthodes de Monte-Carlo

---
# Un peu d'histoire ...

- première expérience de type Monte Carlo dûe à Buffon au XVIIIème siècle &rarr; l'aiguille de Buffon

.center[
<img src="https://upload.wikimedia.org/wikipedia/commons/thumb/5/58/Buffon_needle.svg/1920px-Buffon_needle.svg.png" width="200" align="center">]

--

- les méthodes actuelles sont nées pendant la seconde guerre mondiale au laboratoire américain de Los Alamos &rarr; nom de code *Monte-Carlo*

.center[
<img src="https://frenchriviera.travel/wp-content/uploads/2018/03/Monte-Carlo-Casino1.jpg" width="300" align="center">
]



---
# Objectif

- Question principale : **l'approximation d'intégrales**

$$\int h(x) dx $$

- Outil théorique à la base des méthodes de Monte-Carlo : **la loi forte des grands nombres**

- Outil pratique nécessaire : **la simulation de variables aléatoires**


---

class: inverse, middle, center

# 1. Génération de variables aléatoires

---
# Génération de variables aléatoires

Lois usuelles &rarr; disponibles sous la plupart des langages ou logiciels 

```{r, echo=FALSE}
library(reticulate)
```

```{r}
# Sous R
rexp(n=10, rate=1/5)
```

```{python, eval=FALSE}
# Sous Python
from scipy.stats import expon
expon.rvs(scale=5, size=10)
```

.red[**attention aux conventions utilisées dans chaque langage !**]

---
#  Méthode de la fonction inverse

Repose sur le résultat suivant :
> Soit $F$ une f.d.r. et $F^{-}$ son inverse (généralisée). Soit $U \sim \mathcal{U}([0,1])$. Alors $X = F^{-}(U)$ suit la loi $F$.

--

```{r, echo=FALSE, fig.align='center', fig.height=6, fig.width=7, fig.retina=2}
set.seed(10101)
U <- runif(15)
X <- qnorm(U,sd=0.5)

plot(seq(-2,2,0.001),pnorm(seq(-2,2,0.001),sd=0.5),type="l",xlab="",ylab="",yaxt="n")
```


---
#  Méthode de la fonction inverse

Repose sur le résultat suivant :
> Soit $F$ une f.d.r. et $F^{-}$ son inverse (généralisée). Soit $U \sim \mathcal{U}([0,1])$. Alors $X = F^{-}(U)$ suit la loi $F$.


```{r, echo=FALSE, fig.align='center', fig.height=6, fig.width=7, fig.retina=2}
plot(seq(-2,2,0.001),pnorm(seq(-2,2,0.001),sd=0.5),type="l",xlab="",ylab="",yaxt="n")
mtext(paste0("U[",1:15,"]"),at=U,side=2,line=2,cex=0.75,las=2)
axis(2,at=sort(U),labels=rep("",15),cex.axis=0.75)
```

---
#  Méthode de la fonction inverse

Repose sur le résultat suivant :
> Soit $F$ une f.d.r. et $F^{-}$ son inverse (généralisée). Soit $U \sim \mathcal{U}([0,1])$. Alors $X = F^{-}(U)$ suit la loi $F$.

```{r, echo=FALSE, fig.align='center', fig.height=6, fig.width=7, fig.retina=2}
plot(seq(-2,2,0.001),pnorm(seq(-2,2,0.001),sd=0.5),type="l",xlab="",ylab="",yaxt="n")
mtext(paste0("U[",1:15,"]"),at=U,side=2,line=2,cex=0.75,las=2)
axis(2,at=sort(U),labels=rep("",15),cex.axis=0.75)
mtext(paste0("X[",1:15,"]"),at=X,side=1,line=2,cex=0.75,las=2)
axis(1,at=sort(X),labels=rep("",15),cex.axis=0.75)
segments(-2,U,X,U,lty=2,col="grey")
segments(X,U,X,0,lty=2,col="grey")
```


---
#  Méthode de la fonction inverse

Repose sur le résultat suivant :
> Soit $F$ une f.d.r. et $F^{-}$ son inverse (généralisée). Soit $U \sim \mathcal{U}([0,1])$. Alors $X = F^{-}(U)$ suit la loi $F$.

```{r, echo=FALSE, fig.align='center', fig.height=6, fig.width=7, fig.retina=2}
plot(ecdf(X),xlim=c(-2,2),main="",ylab="",yaxt="n")
points(seq(-2,2,0.001),pnorm(seq(-2,2,0.001),sd=0.5),type="l",xlab="",ylab="",yaxt="n")
mtext(paste0("U[",1:15,"]"),at=U,side=2,line=2,cex=0.75,las=2)
axis(2,at=sort(U),labels=rep("",15),cex.axis=0.75)
mtext(paste0("X[",1:15,"]"),at=X,side=1,line=2,cex=0.75,las=2)
axis(1,at=sort(X),labels=rep("",15),cex.axis=0.75)
segments(-2,U,X,U,lty=2,col="grey")
segments(X,U,X,0,lty=2,col="grey")
```


---
# Exemple

Loi de Cauchy, de densité $f(x) = \frac{1}{\pi(1+x^2)}$

--

```{r}
u <- runif(10000)
finv <- function(u){tan(pi*(u-0.5))}
x <- finv(u)
```

--

```{r, echo=FALSE, fig.width=6, fig.height=5, fig.retina=2, fig.align='center', warning=FALSE}
d <- data.frame(x=x)
ggplot(d, aes(x)) +
  geom_histogram(aes(y = after_stat(density)), binwidth = 0.35, fill = "#999999") +
  stat_function(
    fun = dcauchy,
    lwd = 1
  ) + xlim(c(-10,10))
```


---
# Méthode d'acceptation-rejet

Proposition :
> Soit $X$ une v.a. de densité $f$ et soit $g$ une densité de probabilité et une constante $M \geq 1$ t.q. $\forall x, f(x) \leq M g(x)$. Alors pour simuler selon la loi $f$ il suffit de :
1. simuler $Y \sim g$
2. simuler $U | Y=y \sim \mathcal{U}([0,Mg(y)])$
3. si $0 < U < f(Y)$, poser $X=Y$, sinon reprendre l'étape 1.

*Preuve*  <div class="horizontalgap" style="width:10px"></div> 

--

Remarques :
- on a seulement besoin de connaître $f$ à une constante multiplicative près
- $\forall x, f(x) \leq M g(x) \Rightarrow \text{supp}(f) \subset \text{supp}(g)$
- probabilité d'accepter un candidat : $\frac{1}{M}$ (influence du choix de $g$)


<span style="color:#16A085">**fin du cours 6 (28/01/2025)**</span>


---
name: c7
# Méthode d'acceptation-rejet

Illustration

```{r, fig.retina=2, fig.height=7, fig.width=10, echo=FALSE, fig.align='center', warning=FALSE, message=FALSE}
fx <- function(x){0.65*dnorm(x,0,1.1)+0.35*dnorm(x,4,1)}
gx <- function(x){1.83*dnorm(x,1.4,2.5)}
x <- seq(-8,10,length.out=1000)
dx <- data.frame(x=c(x,x),y=c(fx(x),gx(x)),Fonction=c(rep("f",length(x)),rep("Mg",length(x))))
ggplot(data=dx,aes(x=x,y=y,col=Fonction)) + geom_line(size=1) #+ geom_point(x=xg,y=0,cex=4) + 
  #annotate(geom="text",x=xg,y=0.01,label="Y",cex=5) 
```

---
# Méthode d'acceptation-rejet

- Tirage de $Y$ selon la loi $g$

```{r, fig.retina=2, fig.height=7, fig.width=10, echo=FALSE, fig.align='center'}
set.seed(10)
xg <- rnorm(1,1.4,2.5)
ggplot(data=dx,aes(x=x,y=y,col=Fonction)) + geom_line(size=1) + geom_point(x=xg,y=0,cex=4) + 
  annotate(geom="text",x=xg,y=0.01,label="Y1",cex=5) 
```


---
# Méthode d'acceptation-rejet

- Tirage de $U$ conditionnellement à $Y=y$ selon la loi $\mathcal{U}([0,Mg(y)])$ &rarr; on rejette

```{r, fig.retina=2, fig.height=7, fig.width=10, echo=FALSE, fig.align='center'}
set.seed(110)
yg <- runif(1,0,gx(xg))
ggplot(data=dx,aes(x=x,y=y,col=Fonction)) + geom_line(size=1) + geom_point(x=xg,y=yg,cex=4) + geom_segment(x=xg,y=0,xend=xg,yend=gx(xg)) + geom_point(x=xg,y=0,cex=4) + 
  annotate(geom="text",x=xg,y=0.01,label="Y1",cex=5) 
```

---
# Méthode d'acceptation-rejet

- Tirage de $U$ conditionnellement à $Y=y$ selon la loi $\mathcal{U}([0,Mg(y)])$ &rarr; on accepte 

```{r, fig.retina=2, fig.height=7, fig.width=10, echo=FALSE, fig.align='center'}
set.seed(1000)
xg <- rnorm(1,1.4,2.5)
yg <- runif(1,0,gx(xg))
ggplot(data=dx,aes(x=x,y=y,col=Fonction)) + geom_line(size=1) + geom_point(x=xg,y=yg,cex=4) +  geom_segment(x=xg,y=0,xend=xg,yend=gx(xg)) + geom_point(x=xg,y=0,cex=4) + 
  annotate(geom="text",x=xg,y=0.01,label="Y2",cex=5) 
```



---
# Méthode d'acceptation-rejet

A la fin :

```{r, fig.align='center', fig.retina=2, fig.height=7, fig.width=10, echo=FALSE, warning=FALSE, message=FALSE}
library(gganimate)
x <- seq(-8,10,0.01)
fxs <- fx(x)
gxs <- gx(x)
set.seed(0)
xg <- rnorm(5000,1.4,2.5)
yg <- runif(5000,0,gx(xg))
u <- ifelse(yg<fx(xg),1,0)
ddens <- data.frame(x=x,y=fxs,accept=TRUE)
ddens$accept <- factor(ddens$accept,labels=c("Accepté"))
ddens1 <- data.frame(x=x,y=gxs,accept=FALSE)
ddens1$accept <- factor(ddens1$accept,labels=c("Rejeté"))
dpoints <- data.frame(x=xg,y=yg,accept=u)
dpoints$accept <- factor(dpoints$accept,labels=c("Rejeté","Accepté"))
dpoints$n <- rep(seq(100,5000,by=100),each=100)
ggplot(data = dpoints, aes(x=x,y=y,col=accept))  + geom_point(size=0.9) + geom_line(data=ddens, linewidth=1.1) +  geom_line(data=ddens1, linewidth=1.1) + scale_color_hue(direction=-1) #+ scale_color_brewer(name="",palette="Dark2")# + transition_states(n) + shadow_mark() 
```



---
# Exemple 

On veut simuler $X$ selon la loi normale centrée réduite à l'aide d'une loi exponentielle de paramètre 1.

1. par symétrie de la loi normale, il suffit de savoir simuler selon la loi de $|X|$ 

--

2. on génère ensuite $Z$, une Bernoulli de paramètre 1/2 et on pose $X = |X|$ si $Z=1$ et $X=-|X|$ si $Z=0$.

--

3. il faut trouver une constante $M$ telle que $f(x)\leq M g(x)$ pour tout $x$ avec :
- densité cible $f(x) = \frac{2}{\sqrt{2\pi}} e^{-x^2/2} \mathbb{1}_{x\geq 0}$
- densité de la loi exponentielle $g(x) = e^{-x} \mathbb{1}_{x>0}$


---
class: my-one-page-font
# Exemple

```{r}
n <- 1000
f <- function(x){ifelse(x>0,sqrt(2/pi)*exp(-x^2/2),0)}
g <- function(x){ifelse(x>0,exp(-x),0)}

M <- sqrt(2*exp(1)/pi) # env. 1.31 -> 1/M = 0.76
U1 <- runif(n)
Y <- -log(U1)
U2 <- runif(n,min = 0, max = M*g(Y))
absX <- Y[U2<f(Y)]
Z <- 2*rbinom(length(absX),size = 1, p=1/2) - 1
X <- Z*absX
```

--

```{r, echo=FALSE, fig.height=4,fig.width=9,fig.retina=2,fig.align='center'}
#hist(X,breaks = 20,freq = F, main="")
#points(seq(-2.5,2.5,0.001),dnorm(seq(-2.5,2.5,0.001)),col="red",type="l")
x <- seq(0,3,0.001)
dar <- data.frame(x=rep(x,2),y=c(2*dnorm(x),sqrt(2*exp(1)/pi)*dexp(x)),Fonction=rep(c("f","Mg"),each=length(x)))
p1 <- ggplot(data=dar,aes(x,y,color=Fonction)) + geom_line()
p2 <- ggplot(data=data.frame(X=X),aes(x=X)) + geom_histogram(aes(y = after_stat(density)), binwidth = 0.35, fill = "grey",col="darkgrey") + stat_function(fun = dnorm) + ggtitle(paste0("valeurs acceptées : ",length(absX),"/",1000))
gridExtra::grid.arrange(p1,p2,ncol=2)
```

---
# Un cas particulier

Un cas particulier intéressant : le cas $f$ bornée à support compact.

--

- on prend pour $g$ la loi uniforme sur le support de $f$
- et on simule $U$ uniforme sur $[0,m]$ où $m=\max_x f(x)$

--

ex. : $\displaystyle f(x) = \frac{1}{8} |x^4 - 5x^2 + 4| \ \mathbb{1}_{[-2,2]}(x)$
```{r, echo=FALSE, fig.height=6,fig.width=8,fig.align='center',fig.retina=2}
fdens <- function(x){
  ifelse(abs(x)<2,abs(x^4-5*x^2+4)/8,0)
}

curve(fdens,from=-2.5,2.5,lwd=2)
```

---
# Un cas particulier

Un cas particulier intéressant : le cas $f$ bornée à support compact.

- on prend pour $g$ la loi uniforme sur le support de $f$
- et on simule $U$ uniforme sur $[0,m]$ où $m=\max_x f(x)$

ex. : $\displaystyle f(x) = \frac{1}{8} |x^4 - 5x^2 + 4| \ \mathbb{1}_{[-2,2]}(x)$
```{r, echo=FALSE, fig.height=6,fig.width=8,fig.align='center',fig.retina=2}
curve(fdens,from=-2.5,2.5,lwd=2)
curve(2*dunif(x,-2,2),from=-2.5,to=2.5,col="red",lwd=2,add=T)
```

---
# Un cas particulier

$f(x) = \frac{1}{8} |x^4 - 5x^2 + 4| \ \mathbb{1}_{[-2,2]}(x)$

.pull-left[
```{r paged.print=FALSE, eval=FALSE}
Y <- runif(10000,-2,2)
U <- runif(10000,0,0.5)

accept <- (U<fdens(Y))
X <- Y[accept]

hist(X,breaks=50,freq=F)
points(x,fx,type="l",col="red")
```

```{r, echo=FALSE}
Y <- runif(10000,-2,2)
U <- runif(10000,0,0.5)
accept <- (U<fdens(Y))
```

```{r}
mean(accept)
```

&rarr; la moitié des points simulés est 'perdue'
]

.pull-right[
```{r, echo=FALSE}
Y <- runif(10000,-2,2)
U <- runif(10000,0,0.5)

accept <- (U<fdens(Y))
X <- Y[accept]

hist(X,breaks=50,,freq=F,xlab="x",ylab="",main="")
points(seq(-2,2,0.01),fdens(seq(-2,2,0.01)),type="l",col="red",lwd=2)
```
]


---
# Un cas particulier

Visuellement : on peut représenter les points acceptés dans le rectangle $[-2,2] \times [0,1/2]$

</br>

```{r, fig.align='center',fig.height=6,fig.width=8, fig.retina=1,echo=FALSE, warning=FALSE, message=FALSE}
library(gganimate)
x <- seq(-2,2,0.01)
fx <- fdens(x)
ddens <- data.frame(x=x,y=fx,accept=TRUE)
ddens$accept <- factor(ddens$accept,labels=c("Accepté"))
dpoints <- data.frame(x=Y,y=U,accept=accept)
dpoints$accept <- factor(dpoints$accept,labels=c("Rejeté","Accepté"))
dpoints$n <- rep(seq(100,10000,by=100),each=100)
ggplot(data = dpoints, aes(x=x,y=y,col=accept))  + geom_point(size=0.9) + geom_line(data=ddens, linewidth=1.1) + scale_color_discrete(name="") + scale_color_brewer(name="",palette="Dark2")# + transition_states(n) + shadow_mark() 
```

---

class: inverse, middle, center

# 2. Méthodes de Monte-Carlo

---
# Monte Carlo classique

**Objectif** : calculer une intégrale de la forme $\int h(x) f(x) dx$ où $f$ est une densité de probabilité.

--

> *Définition.* Soit $X_1,\dots,X_n$ un échantillon i.i.d. de loi $f$. L'estimateur de Monte Carlo de $\mathbb{E}(h(X))$ est :
$$\hat{h}_n = \frac{1}{n} \sum_{i=1}^n h(X_i)$$


Propriétés :
- estimateur sans biais
- fortement consistant
- IC avec le TCL

Remarques :
- vitesse de convergence en $\sqrt{n}$, indépendante de la dimension
- ne dépend pas de la régularité de $h$

---
# Exemples d'applications

#### Approcher la $p$-valeur d'un test

ex. avec le test du rapport de vraisemblance d'un modèle gaussien :
$$H_0 : \quad (\mu,\sigma^2)=(\mu_0,\sigma_0^2)\quad\textrm{ contre }\quad H_1 : \quad (\mu,\sigma^2)\neq(\mu_0,\sigma_0^2).$$

--

On a la statistique de test suivante (voir DS de Stat Math 2024) : 
$$2\ln V_n= T_n^2-n\ln T_n^2 + Z^2 + n\ln n -n,$$

avec $Z=\sqrt{n}\big(\frac{\hat{\mu}_n-\mu_0}{\sigma_0}\big) \quad \textrm{et}\quad T_n^2=\frac{n\hat{\sigma}^2_n}{\sigma_0^2}$.

Problème : la loi de la statistique de test n'est pas connue (tabulée).

---
# Exemples d'applications

Les données :
```{r, echo=FALSE}
x <- c(98.23,97.91,98.24,99.00,102.64,103.44,103.81,100.64,100.86,98.79,103.48, 98.10,101.11, 98.30,96.56,98.77,100.15,101.58,97.51,100.87,101.99, 98.59,98.72,103.97,94.75,97.92,102.30, 96.88,101.44,100.12)
x
n <- length(x)
```
--

.pull-left[
```{r}
Tn2 <- (n-1)*var(x)/4
V_obs <- Tn2 - n*log(Tn2) + n*(mean(x)-100)^2/4 + n*log(n) - n
print(V_obs)

N <- 100000
T <- rchisq(N,n-1)   
Z <- rnorm(N,0,1)    
V <- T - n*log(T) + Z^2 + n*log(n) - n
p_val <- mean(V > V_obs)
print(p_val)
```
]
.pull-right[
```{r, echo=FALSE, warnings=FALSE,message=FALSE}
ggplot(data=NULL,aes(x=V)) + geom_histogram(aes(y=after_stat(density)),fill="grey",col="darkgrey") + geom_vline(xintercept = V_obs, col="red")
```
]

---
# Exemples d'applications

On peut quantifier l'erreur d'approximation :

.pull-left[
```{r paged.print=FALSE, eval=FALSE}
N <- c(100,1000,2000,5000,10000,20000,
       50000,100000,200000,300000)
p_val <- rep(0,length(N))
for (i in 1:length(N)){
  T <- rchisq(N[i],n-1)   
  Z <- rnorm(N[i],0,1)    
  V <- T-n*log(T)+Z^2+n*log(n)-n
  p_val[i] <- mean(V > V_obs)
}
```
]
.pull-right[
```{r, echo=FALSE, fig.width=6,fig.height=4, fig.retina=2}
N <- c(100,1000,2000,5000,10000,20000,
       50000,100000,200000,300000)

p_val <- rep(0,length(N))
for (i in 1:length(N)){
  T <- rchisq(N[i],n-1)   
  Z <- rnorm(N[i],0,1)    
  V <- T-n*log(T)+Z^2+n*log(n)-n
  p_val[i] <- mean(V > V_obs)
}
d <- data.frame(phat = p_val, lb = p_val-qnorm(0.975)*sqrt(p_val*(1-p_val)/N), ub = p_val+qnorm(0.975)*sqrt(p_val*(1-p_val)/N), n = N)
ggplot(data=d,aes(n,phat)) + geom_line() + geom_line(aes(n,lb),col="red") + geom_line(aes(n,ub),col="red") + ylab("Estimation de p") 
```
]

---
# Exemples d'application

Plus généralement, les méthodes de Monte Carlo s'utilisent pour :

- approcher le niveau ou la puissance d'un test

- approcher une intégrale en grande dimension

- faire de l'optimisation (e.g. algorithme du recuit simulé, descente de gradient stochastique, ...)

- ...

<span style="color:#16A085">**fin du cours 7 (03/02/2025)**</span>

---
name: c8
# Monte Carlo classique

Estimation de $\mu = \int h(x) f(x) dx$ à l'aide d'un échantillon $X_1,\dots,X_n$ i.i.d. de loi $f$ :
$$\hat{h}_n = \frac{1}{n} \sum_{i=1}^n h(X_i),$$

--

 - estimateur sans biais, fortement consistant
 - variance :
 $$\text{Var} (\hat{h}_n) = \frac{1}{n} \left(\int h^2(x)f(x)dx - \mu^2\right) = \frac{\sigma_f^2}{n}$$
 - $\sigma_f^2$ estimée par : $$v_n = \frac{1}{n} \sum_{i=1}^n h^2(X_i) - \hat{h}_n^2$$
 - IC de niveau $1-\alpha$ : $$\hat{I}_\mu =\left[\hat{h}_n - q_{1-\alpha/2}^{\mathcal{N}(0,1)} \sqrt{v_n/n} ; \hat{h}_n + q_{1-\alpha/2}^{\mathcal{N}(0,1)} \sqrt{v_n/n} \right]$$


---
# Monte Carlo classique

**Exemple :** on veut calculer $\int_0^{2\pi} (\sin^2(x) + 2\cos(3x))^2 dx$

--
```{r,fig.align='center',fig.height=5,fig.width=7, fig.retina=2,echo=FALSE}
curve((sin(x)^2+2*cos(3*x))^2,from=0,to=2*pi,n = 1000)
```

On peut ré-écrire l'intégrale sous la forme  
$$
\begin{eqnarray}
I & = & 2\pi \int_{\mathbb{R}} (\sin^2(x) + 2\cos(3x))^2 \frac{1}{2\pi}1_{[0,2\pi]}(x)dx \\
& = & 2\pi \ \mathbb{E}((\sin^2(X) + 2\cos(3X))^2), \quad X \sim \mathcal{U}([0,2\pi])
\end{eqnarray}
$$

---
# Monte Carlo classique

**Exemple :** on veut calculer $I=2\pi \ \mathbb{E}((\sin^2(X) + 2\cos(3X))^2), \quad X \sim \mathcal{U}([0,2\pi])$

On peut utiliser l'estimateur de Monte-Carlo suivant :
 1. on génère $X_1,\dots,X_n$ i.i.d. de loi uniforme sur $[0,2\pi]$


```{r,fig.align='center',fig.height=6,fig.width=7, fig.retina=2,echo=FALSE}
curve((sin(x)^2+2*cos(3*x))^2,from=0,to=2*pi,n = 1000)
u <- runif(100,0,2*pi)
```

---
# Monte Carlo classique

**Exemple :** on veut calculer $I=2\pi \ \mathbb{E}((\sin^2(X) + 2\cos(3X))^2), \quad X \sim \mathcal{U}([0,2\pi])$

On peut utiliser l'estimateur de Monte-Carlo suivant :
 1. on génère $X_1,\dots,X_n$ i.i.d. de loi uniforme sur $[0,2\pi]$


```{r,fig.align='center',fig.height=6,fig.width=7, fig.retina=2,echo=FALSE}
curve((sin(x)^2+2*cos(3*x))^2,from=0,to=2*pi,n = 1000)
u <- runif(100,0,2*pi)
points(u,0*u,col="red",pch=19,cex=0.5)
legend(0,8,legend=c("Xi"),col=c("red"),pch=19,bty="n")
```

---
# Monte Carlo classique

**Exemple :** on veut calculer $I=2\pi \ \mathbb{E}((\sin^2(X) + 2\cos(3X))^2), \quad X \sim \mathcal{U}([0,2\pi])$

On peut utiliser l'estimateur de Monte-Carlo suivant :
 1. on génère $X_1,\dots,X_n$ i.i.d. de loi uniforme sur $[0,2\pi]$
 2. on approche $I$ par $\hat{I} = 2 \pi \frac{1}{n}\sum_{i=1}^n (\sin^2(X_i) + 2\cos(3X_i))^2$

```{r,fig.align='center',fig.height=6,fig.width=7, fig.retina=2,echo=FALSE}
curve((sin(x)^2+2*cos(3*x))^2,from=0,to=2*pi,n = 1000)
u <- runif(100,0,2*pi)
points(u,0*u,col="red",pch=19,cex=0.5)
segments(u,0*u,u,(sin(u)^2+2*cos(3*u))^2,col="red",lwd=0.2)
points(u,(sin(u)^2+2*cos(3*u))^2,col="black",pch=19,cex=0.5)
legend(0,8,legend=c("Xi","h(Xi)"),col=c("red","black"),pch=19,bty="n")
```

---
# Monte Carlo classique

```{r,fig.align='center',fig.height=6,fig.width=7, fig.retina=2,echo=FALSE}
curve((sin(x)^2+2*cos(3*x))^2,from=0,to=2*pi,n = 1000)
points(u,0*u,col="red",pch=19,cex=0.5)
segments(u,0*u,u,(sin(u)^2+2*cos(3*u))^2,col="red",lwd=0.2)
points(u,(sin(u)^2+2*cos(3*u))^2,col="black",pch=19,cex=0.5)
legend(0,8,legend=c("Xi","h(Xi)"),col=c("red","black"),pch=19,bty="n")
```
On trouve : 
```{r, echo=F}
mi = mean((sin(u)^2+2*cos(3*u))^2)*2*pi
vari = var((sin(u)^2+2*cos(3*u))^2)*4*pi*pi/100
print(paste0("Estimation :",round(mi,5),", variance :",round(vari,5),", IC = ",paste0("[",paste0(round(mi+qnorm(c(0.025,0.975))*sqrt(vari),3),collapse=","),"]")))
print(paste0("La vraie valeur est : ",round(integrate(function(x){(sin(x)^2+2*cos(3*x))^2},lower=0,upper=2*pi)$value,5)))
```

---

class: inverse, middle, center

# 3. Echantillonnage préférentiel

---
# Echantillonnage préférentiel

**Objectif** : toujours d'évaluer $\mu = \int h(x) f(x) dx$


On va **ré-écrire l'intégrale** sous la forme :

$$\mu = \int h(x) f(x) dx = \int h(x) \frac{f(x)}{g(x)} g(x) dx,$$
avec $g$ densité de probabilité, telle que $(g(x) = 0) \Rightarrow (h(x)f(x) = 0)$.

--

Pourquoi ?

 - pas toujours possible de simuler selon la loi $f$, ou d'utiliser la méthode d'acceptation-rejet
 - ou on s'intéresse à un évènement de probabilité trop faible
 - dans certains cas cela permet de réduire la variance de l'estimateur

---
# Echantillonnage préférentiel

> *Définition.* Soit $Z_1,\dots,Z_n$ un échantillon i.i.d. de loi $g$. L'estimateur par échantillonnage préférentiel de $\mu$ est :
$$\tilde{\mu}_n = \frac{1}{n} \sum_{i=1}^n h(Z_i) \frac{f(Z_i)}{g(Z_i)} = \frac{1}{n} \sum_{i=1}^n w_i h(Z_i)$$

Propriétés : (si $\text{supp}(f) \subset \text{supp}(g)$)

- estimateur sans biais
- fortement consistant

Remarques :

- On appelle *poids d'importance* les quantités $w_i = \frac{f(Z_i)}{g(Z_i)}$
- La loi $g$ est appelée *loi instrumentale*


---
class: my-one-page-font
# Echantillonnage préférentiel

**Exemple 1** : on veut calculer $\int_0^1 e^{-x}x^{-a} dx$ pour $0 < a < 1$.

--

Monte Carlo classique &rarr; $f$ densité de la loi uniforme sur $[0,1]$ et $h(x) = e^{-x}x^{-a}$.

--

```{r} 
set.seed(04022025)
a <- 1/2
n <- 1000
X <- runif(n)
hX <- exp(-X) * X^(-a)
print(paste0("Estimation : ",mean(hX),", variance : ",var(hX)/n))
```

--

Peut-on faire mieux ? 

---
# Echantillonnage préférentiel

A quoi ressemble la fonction $h(x) = e^{-x}x^{-a}$ ?

```{r,fig.align='center',fig.height=4,fig.width=5, fig.retina=2,echo=FALSE}
xabs <- seq(0,1,0.001)
yord <- exp(-xabs) * xabs^(-a)
dd <- data.frame(x=xabs,y=yord)
drug <- data.frame(x=X)
ggplot(data=NULL,aes(x=xabs,y=yord)) + geom_line() + ggtitle("Fonction h") + xlab("x") + ylab("h")
```


---
# Echantillonnage préférentiel

A quoi ressemble la fonction $h(x) = e^{-x}x^{-a}$ ? 

```{r,fig.align='center',fig.height=4,fig.width=7, fig.retina=2,echo=FALSE}
xabs <- seq(0,1,0.001)
yord <- exp(-xabs) * xabs^(-a)
dd <- data.frame(x=xabs,y=yord)
drug <- data.frame(x=X,Loi="Uniforme [0,1]")
ggplot(data=NULL,aes(x=xabs,y=yord)) + geom_line() + geom_rug(data=drug,aes(x=x,col=Loi), inherit.aes=FALSE, linewidth=0.1) + ggtitle("Fonction h") + xlab("x") + ylab("h")

#xpred <- rbinom(10000,10,ech_finv)
#ggplot(data=NULL,aes(x=as.factor(xpred))) + geom_bar(fill="grey",col="darkgrey") + #geom_rug(aes(x=as.factor(X)),col="red",lwd=2,length=unit(0.05, "npc")) + xlab("Prédictions")
```
&rarr; en échantillonnant selon la loi uniforme, on obtient des points répartis de façon **uniforme** sur $[0,1]$.

&rarr; Peut-on trouver une loi d'échantillonnage qui produise plus de points proches de 0 ?


---
class: my-one-page-font
# Echantillonnage préférentiel

Echantillonnage préférentiel : $g(x) = (1-a) \ x^{-a} \mathbb{1}_{[0,1]}(x)$ et $\tilde{h}(x) = e^{-x}/ (1-a).$

--

```{r, fig.align='center',fig.height=4,fig.width=5, fig.retina=2,echo=FALSE}
yord2 <- (1-a)*xabs^(-a)
ggplot(data=NULL,aes(x=xabs,y=yord2)) + geom_line() + ggtitle("Densité g") + xlab("x") + ylab("g")
```
--

```{r} 
Z <- X^(1/(1-a))
hZ <- exp(-Z) / (1-a)
print(paste0("Estimation : ",mean(hZ),", variance : ",var(hZ)/n))
```

On a diminué la variance par 100 pour une même taille d'échantillon.

--
Vraie valeur : 1.49365 (intégration numérique)


---
# Echantillonnage préférentiel

Avant, on intégrait $h(x) = e^{-x}x^{-a}$ en utilisant la loi uniforme sur $[0,1]$.

Maintenant, on intègre $\tilde{h}(x) = e^{-x}/(1-a)$ en utilisant la loi de densité $(1-a) \ x^{-a} \mathbb{1}_{[0,1]}(x)$.

```{r,fig.align='center',fig.height=4,fig.width=12, fig.retina=2,echo=FALSE, message=F, warning=F}
xabs2 <- seq(0,1,0.001)
yord2 <- exp(-xabs2) / (1-a)
drug2 <- data.frame(x=Z,Loi="Puissance")
p1 <- ggplot(data=NULL,aes(x=xabs,y=yord)) + geom_line() + geom_rug(data=drug,aes(x=x,col=Loi), inherit.aes=FALSE, linewidth=0.1) + ggtitle("Fonction h") + xlab("x") + ylab("h")
p2 <- ggplot(data=NULL,aes(x=xabs2,y=yord2)) + geom_line() + geom_rug(data=drug2,aes(x=x,col=Loi), inherit.aes=FALSE, linewidth=0.1) + ggtitle(str2expression(paste("Fonction","tilde(h)",sep="~"))) + xlab("x") + ylab(expression(tilde(h)))
library(gridExtra)
grid.arrange(p1,p2,ncol=2)
```

---
# Echantillonnage préférentiel

**Exemple 2** : calculer $\mathbb{P}(X>10)$ pour $X \sim \mathcal{E}(1)$.
--

```{r, results='hold'}
1 - pexp(10) # vraie valeur calculée à l'aide de la fonction de répartition
mean(rexp(1000)>10) # estimation par Monte Carlo naïf
```
--

L'estimateur est nul ... on n'a peut-être pas eu de chance sur ce tirage de 1000 ? Et si on ré-essayait ?


---
# Echantillonnage préférentiel

On tire 100 fois un échantillon de taille $n=1000$ et on regarde ce que vaut l'estimateur par Monte Carlo sur chacun de ces 100 tirages.

```{r}
repet_1000 <- sapply(1:100,FUN = function(i){mean(rexp(1000)>10)})
```

--

```{r echo=FALSE, fig.align='center',fig.height=4,fig.width=4, fig.retina=2}
hist(repet_1000,xlab="",main="",ylim=c(0,100))
```
&rarr; l'estimateur vaut presque tout le temps 0 (96 fois sur 100), le reste du temps il vaut 0.001 (dans ces 4 cas, cela veut dire qu'une seule observation sur les 1000 du tirage était plus grande que 10).

---
# Echantillonnage préférentiel


Que se passe t-il si on augmente la taille des échantillons ?

```{r}
repet_1000 <- sapply(1:100,FUN = function(i){mean(rexp(1000)>10)})
repet_10000 <- sapply(1:100,FUN = function(i){mean(rexp(10000)>10)})
repet_100000 <- sapply(1:100,FUN = function(i){mean(rexp(100000)>10)})
```

```{r echo=FALSE, fig.height=4,fig.width=12, fig.retina=2}
par(mfrow=c(1,3))
hist(repet_1000,main="n=1000",xlab="",freq=F)
hist(repet_10000,main="n=10000",xlab="",freq=F)
hist(repet_100000,main="n=100000",xlab="",freq=F)
```

&rarr; on obtient des résultats "raisonnables" quand $n=100 000$, mais la variance reste très élevée.


---
# Echantillonnage préférentiel

En prenant comme loi instrumentale $\mathcal{E}(1/10)$, on peut ré-écrire :

$$\mathbb{P}(X>10) =\mathbb{E}\left(\mathbf{1}_{Z>10} \ 10 \ e^{-9Z/10}\right) \quad \text{où } Z \sim \mathcal{E}(1/10)$$
En utilisant Monte-Carlo, on approche cette espérance à l'aide de la moyenne empirique d'un échantillon $Z_1,\dots,Z_n$ de taille $n$ de v.a. iid de loi exponentielle de paramètre 1/10.

$$\hat{p} = \frac{1}{n} \sum_{i=1}^n 10 \ e^{-0.9 Z_i} \mathbf{1}_{Z_i > 10}$$

---
# Echantillonnage préférentiel

Code correspondant (pour un tirage, donc une estimation) :

```{r}
Z <- rexp(n,1/10)
w <- 10*exp(-9*Z/10)  # ou plus généralement : dexp(Z,1)/dexp(Z,1/10)
mean(w*(Z>10))
```

--

On fait plusieurs tirages (plusieurs estimations de $p$) :

```{r, echo=F}
repet_1000 <- sapply(1:100,FUN = function(i){Z<-rexp(1000,1/10);mean(10*exp(-0.9*Z)*(Z>10))})
```

```{r, echo=F, fig.retina=2, fig.height=5, fig.width=5, fig.align='center'}
hist(repet_1000,main="n=1000",xlab="",freq=F)
```

C'est beaucoup mieux qu'avec la loi "naïve" !

---
# Echantillonnage préférentiel

Avec la loi $\mathcal{E}(1)$ :
```{r, echo=FALSE}
repet_1000 <- sapply(1:100,FUN = function(i){mean(rexp(1000)>10)})
repet_10000 <- sapply(1:100,FUN = function(i){mean(rexp(10000)>10)})
repet_100000 <- sapply(1:100,FUN = function(i){mean(rexp(100000)>10)})
```

```{r echo=FALSE, fig.height=3.5,fig.width=12, fig.retina=2}
ptrue <- 1-pexp(10)
par(mfrow=c(1,3))
hist(repet_1000,main="n=1000",xlab="",freq=F)
abline(v=ptrue,col="red",lwd=2)
hist(repet_10000,main="n=10000",xlab="",freq=F)
abline(v=ptrue,col="red",lwd=2)
hist(repet_100000,main="n=100000",xlab="",freq=F)
abline(v=ptrue,col="red",lwd=2)
```
Avec la loi $\mathcal{E}(1/10)$ :

```{r, echo=FALSE}
repet_1000 <- sapply(1:100,FUN = function(i){Z<-rexp(1000,1/10);mean(10*exp(-0.9*Z)*(Z>10))})
repet_10000 <- sapply(1:100,FUN = function(i){Z<-rexp(10000,1/10);mean(10*exp(-0.9*Z)*(Z>10))})
repet_100000 <- sapply(1:100,FUN = function(i){Z<-rexp(100000,1/10);mean(10*exp(-0.9*Z)*(Z>10))})
```

```{r, echo=FALSE, fig.height=3.5,fig.width=12, fig.retina=2}
par(mfrow=c(1,3))
hist(repet_1000,main="n=1000",xlab="",freq=F)
abline(v=ptrue,col="red",lwd=2)
hist(repet_10000,main="n=10000",xlab="",freq=F,xlim=c(3e-5,6e-5))
abline(v=ptrue,col="red",lwd=2)
hist(repet_100000,main="n=100000",xlab="",freq=F,xlim=c(3e-5,6e-5))
abline(v=ptrue,col="red",lwd=2)
```



---
# Choix de la loi instrumentale

- Calcul de la variance de $\tilde{h}_n$

--

- Choix optimal pour $g$ &rarr; celui qui minimise cette variance.


<span style="color:#16A085">**fin du cours 8 (04/02/2025)**</span>


---
name: c9
# Choix de la loi instrumentale

- Variance finie si le ratio $f/g$ est borné (i.e. queues de distribution de $g$ plus lourdes)

- La variance est optimale lorsque $g \propto |h|f$ ... mais cette fonction est inconnue

--

- En pratique : choisir $g$ telle que $|h|f/g$ soit (presque) constant, et de variance finie.



---
class: my-one-page-font
# Choix de la loi instrumentale 

Que se passe t-il lorsque le ratio $f/g$ n'est pas borné ?

--

 **Exemple** : on veut approcher $\int x^2 e^{-x^2/2} dx = \sqrt{2\pi} \ \mathbb{E}(X^2)$ avec $X \sim \mathcal{N}(0,1)$. 

--

 1. en utilisant comme loi instrumentale la loi $\mathcal{N}(0,0.5^2)$.

```{r, fig.width=8,fig.height=5,fig.retina=2, fig.align='center', echo=FALSE, warning=FALSE}
x <- seq(-4,4,length.out=1000)
d <- data.frame(x=rep(x,2),y=c(dnorm(x),dnorm(x,0,0.5)),Loi=rep(c("N(0,1)","N(0,0.5²)"),each=1000))
ggplot(data=d,aes(x=x,y=y,linetype=Loi)) + geom_line(lwd=1) + scale_linetype_manual(values=c("dashed","solid"))
```

---
# Choix de la loi instrumentale 

Code et approximation :

```{r, results='hold'}
set.seed(10022025)
n <- 1000
Z1 <- rnorm(n,0,sd=0.5)
w1 <- dnorm(Z1,0,1)/dnorm(Z1,0,0.5)
print(paste0("Estimation : ",sqrt(2*pi)*mean(Z1^2*w1),", variance : ",2*pi*var(Z1^2*w1)/n))
print(paste0("IC : [",paste0(round(sqrt(2*pi)*mean(Z1^2*w1)+qnorm(c(0.025,0.975))*sqrt(2*pi*var(Z1^2*w1)/n),3),collapse=","),"]"))
```

Vraie valeur : $\sqrt{2 \pi} =$ `r sqrt(2*pi)`.

---
# Choix de la loi instrumentale 

2. avec la loi instrumentale $\mathcal{N}(0,1.2^2)$

```{r, fig.width=8,fig.height=5,fig.retina=2, fig.align='center', echo=FALSE, warning=FALSE}
x <- seq(-4,4,length.out=1000)
d <- data.frame(x=rep(x,2),y=c(dnorm(x),dnorm(x,0,1.2)),Loi=rep(c("N(0,1)","N(0,1.2²)"),each=1000))
ggplot(data=d,aes(x=x,y=y,linetype=Loi)) + geom_line(lwd=1) + scale_linetype_manual(values=c("dashed","solid"))
```


---
# Choix de la loi instrumentale 

Code et approximation :

```{r, results='hold'}
set.seed(10022025)
n <- 1000
Z2 <- rnorm(n,0,sd=1.2)
w2 <- dnorm(Z2,0,1)/dnorm(Z2,0,1.2)
print(paste0("Estimation : ",sqrt(2*pi)*mean(Z2^2*w2),", variance : ",2*pi*var(Z2^2*w2)/n))
print(paste0("IC : [",paste0(round(sqrt(2*pi)*mean(Z2^2*w2)+qnorm(c(0.025,0.975))*sqrt(2*pi*var(Z2^2*w2)/n),3),collapse=","),"]"))
```

On a divisé la variance par `r floor(var(Z1^2*w1)/var(Z2^2*w2))`.

Vraie valeur : $\sqrt{2 \pi} =$ `r sqrt(2*pi)`.

&rarr; calcul de la loi Gaussienne optimale.


---
class: my-one-page-font
# Choix de la loi instrumentale

Si le ratio $f/g$ n'est pas borné, certains poids d'importance peuvent être *trop élevés* ce qui donne trop de poids à certaines observations extrêmes.

```{r, echo=F, fig.retina=2, fig.width=8,fig.height=3.4, fig.align='center'}
dw <- data.frame(w=c(w1,w2),type=rep(c("cas N(0,0.5^2)","cas N(0,1.2^2)"),each=n))
ggplot(data=dw,aes(x=type,y=w)) + geom_boxplot(fill="lightgrey",width=0.5) + xlab("") + facet_wrap(~type,scales = "free")
```

```{r, echo=F, fig.retina=2, fig.height=3.4,fig.width=8,fig.env='figure',strip.white=T,fig.align='center'}
dw <- data.frame(w=c(w1,w2),Z=c(Z1,Z2),type=rep(c("cas N(0,0.5^2)","cas N(0,1.2^2)"),each=n))
ggplot(data=dw,aes(x=Z,y=w)) + geom_point() + xlab("") + facet_wrap(~type,scales = "free") + xlab("valeurs échantillonnées") + ylab("poids d'importance")
```


---
# Version auto-normalisée

On peut définir une version *auto-normalisée* de l'estimateur (sous l'hypothèse que $g>0$ dès que $f>0$):

$$\bar{\mu}_n = \frac{\sum_{i=1}^n h(Z_i) f(Z_i)/g(Z_i)}{\sum_{i=1}^n f(Z_i)/g(Z_i)} = \frac{\sum_{i=1}^n w_i h(Z_i)}{\sum_{i=1}^n w_i}$$
--

Sur notre exemple, cela donne :

```{r, echo=FALSE, fig.retina=2, fig.height=5,fig.width=10, fig.align='center'}
set.seed(10022025)
Z <- matrix(rnorm(2000*1000,mean=0,sd=0.5),nrow=2000)
w0.5 <- dnorm(Z)/dnorm(Z,0,0.5)

Zc <- apply(w0.5*Z^2,2,cumsum)/(1:2000)
Za <- apply(w0.5*Z^2,2,cumsum)/apply(w0.5,2,cumsum)
d <- data.frame(x=rep(1:2000,2),ymin=c(apply(Zc,1,min),apply(Za,1,min)),
                                ymax=c(apply(Zc,1,max),apply(Za,1,max)),
                                ymean=c(apply(Zc,1,mean),apply(Za,1,mean)),
                                Method=rep(c("EP classique","ES auto-normalisé"),each=2000),Loi="N(0,0.5²)")

Z <- matrix(rnorm(2000*1000,mean=0,sd=0.8),nrow=2000)
w0.8 <- dnorm(Z)/dnorm(Z,0,0.8)

Zc <- apply(w0.8*Z^2,2,cumsum)/(1:2000)
Za <- apply(w0.8*Z^2,2,cumsum)/apply(w0.8,2,cumsum)
d11 <- data.frame(x=rep(1:2000,2),ymin=c(apply(Zc,1,min),apply(Za,1,min)),
                                ymax=c(apply(Zc,1,max),apply(Za,1,max)),
                                ymean=c(apply(Zc,1,mean),apply(Za,1,mean)),
                                Method=rep(c("EP classique","ES auto-normalisé"),each=2000),Loi="N(0,0.8²)")

Z <- matrix(rnorm(2000*1000,mean=0,sd=1.2),nrow=2000)
w1.2 <- dnorm(Z)/dnorm(Z,0,1.2)

Zc <- apply(w1.2*Z^2,2,cumsum)/(1:2000)
Za <- apply(w1.2*Z^2,2,cumsum)/apply(w1.2,2,cumsum)
d2 <- data.frame(x=rep(1:2000,2),ymin=c(apply(Zc,1,min),apply(Za,1,min)),
                                ymax=c(apply(Zc,1,max),apply(Za,1,max)),
                                ymean=c(apply(Zc,1,mean),apply(Za,1,mean)),
                                Method=rep(c("EP classique","ES auto-normalisé"),each=2000),Loi="N(0,1.2²)")

dall <- rbind(d,d11,d2)

ggplot(data=dall[dall$x>10,],aes(x=x,y=ymean,col=Method)) + geom_line() + 
  geom_ribbon(aes(x,ymin=ymin,ymax=ymax,fill=Method),alpha=0.3) + facet_wrap(~Loi,scales = "free_y") + scale_color_manual(values=c("#F58634","#007965")) + scale_fill_manual(values=c("#F58634","#007965"))
#orange <- "#FFCC29"
```

---
# Version auto-normalisée

En utilisant la version auto-normalisée, on obtient :
 - un estimateur **biaisé**
 - fortement consistant

Intérêts/avantages :
  - peut s'utiliser lorsque $f/g$ est connue à une constante multiplicative près
  - une variance plus faible .red[**dans certains cas**]
  - permet également de *simuler* selon la loi $f$
  
Inconvénients :
  - hypothèse plus forte sur $g$ ($g>0$ dès que $f>0$, alors qu'en EP classique l'hypothèse $g>0$ dès que $hf>0$ suffit)
  - la variance est plus compliquée à calculer et ne peut pas être arbitrairement faible
  
  
---
# Version auto-normalisée

Simulation selon la loi $f$ :

 &rarr; en échantillonnant selon une loi **discrète** à support sur l'ensemble des $\{Z_1,\dots,Z_n\}$, et où la probabilité de tirer la valeur $Z_i$ est $w_i /\sum_j w_j$.

```{r}
Z <- rnorm(10000,0,sd=1.5)
w2 <- dnorm(Z,0,1)/dnorm(Z,0,1.5)
X <- sample(Z, size = 2000, prob = w2/sum(w2))
```

```{r, echo=FALSE, fig.width=10, fig.height=4, fig.retina=2, fig.align='center'}
par(mfrow=c(1,2))
hist(Z, freq=FALSE,ylim=c(0,0.4),main="Loi des Z (g)")
lines(seq(-3,3,0.01),dnorm(seq(-3,3,0.01),0,1),col="red",lwd=2)
hist(X, freq=FALSE,ylim=c(0,0.4),main="Loi des X (f)")
lines(seq(-3,3,0.01),dnorm(seq(-3,3,0.01),0,1),col="red",lwd=2)
```

---
# Version auto-normalisée

.red[**Attention au biais introduit par la méthode**]

- Il faut une **taille d'échantillon suffisante** car la méthode est seulement **asymptotiquement sans biais**
- Pour échantillonner à l'aide des poids $w_i$, il faut tirer un échantillon plus petit que l'échantillon des $(Z_1,\dots,Z_n)$
- On perd le caractère "i.i.d."

---
# Version auto-normalisée

**Exemple : ** on utilise les poids $w_i$ obtenus après un EP de taille $n=1000$ ou $n=10000$ pour tirer un échantillon de taille 800 selon la loi $f$

```{r}
Z <- rnorm(1000,0,sd=1.5)
w_1000 <- dnorm(Z,0,1)/dnorm(Z,0,1.5)
X_1000 <- sample(Z, size = 800, prob = w_1000/sum(w_1000))

Z <- rnorm(10000,0,sd=1.5)
w_10000 <- dnorm(Z,0,1)/dnorm(Z,0,1.5)
X_10000 <- sample(Z, size = 800, prob = w_10000/sum(w_10000))
```

```{r, echo=F, fig.width=10, fig.height=4, fig.retina=2, fig.align='center'}
par(mfrow=c(1,2))
hist(X_1000, freq=FALSE,ylim=c(0,0.4),main="EP auto-normalisé de taille 1000",xlab="")
lines(seq(-3,3,0.01),dnorm(seq(-3,3,0.01),0,1),col="red",lwd=2)
hist(X_10000, freq=FALSE,ylim=c(0,0.4),main="EP auto-normalisé de taille 10000",xlab="")
lines(seq(-3,3,0.01),dnorm(seq(-3,3,0.01),0,1),col="red",lwd=2)
```


---
# Taille effective de l'échantillon 

- *effective sampling size* : pour identifier un mauvais choix de $g$
  - par exemple, on a $\mathbb{E}(f(Z)/g(Z)) = 1$ pour $Z \sim g$ &rarr; on peut comparer la moyenne empirique des $w_i$ à 1
  - critère ESS :
    $$ESS = \frac{\left( \sum_{i=1} w_i \right)^2}{\sum_{i=1}^n w_i^2} = \frac{1}{\sum_{i=1}^n \bar{w}_i^2},$$
      
      avec $\bar{w}_i = \frac{w_i}{\sum_{i=1}^n w_i}$ le poids d'importance normalisé


Sur l'exemple précédent :
```{r, echo=F}
wbar0.5 <- w0.5[2000,]/sum(w0.5[2000,])
wbar0.8 <- w0.8[2000,]/sum(w0.8[2000,])
wbar1.2 <- w1.2[2000,]/sum(w1.2[2000,])
ESS0.5 <- 1/sum(wbar0.5^2)
ESS0.8 <- 1/sum(wbar0.8^2)
ESS1.2 <- 1/sum(wbar1.2^2)
ess_tab <- data.frame(loi=c("N(0,0.5²)","N(0,0.8²)","N(0,1.2²)"),moy=c(mean(w0.5[2000,]),mean(w0.8[2000,]),mean(w1.2[2000,])),
            ess=c(ESS0.5,ESS0.8,ESS1.2))
knitr::kable(ess_tab,row.names=NA,align='ccc',col.names=c("Loi","Moy. empirique des poids","ESS"))
```

---
class: inverse, middle, center

# 4. Réduction de variance

---
# Variables antithétiques

- **Objectif : ** approcher $\mu = \int h(x)f(x)dx$

- Permet de réduire la variance de l'estimateur en exploitant les propriétés de *symétrie* de la loi d'échantillonnage

- Elle s'utilise lorsque la loi considérée est stable par transformation, i.e. s'il existe une transformation $\nu$ telle que si $X \sim f$ alors $\nu(X) \sim f$.

- Quelques exemples de telles lois : uniformes, normales, student, (lois symétriques de façon générale), ...

>  *Définition.* Soit $X_1,\dots,X_n$ un échantillon i.i.d. de loi $f$. L'estimateur de $\mu$ par la méthode des variables antithétiques est :
$$\hat{\mu}_a = \frac{1}{n} \sum_{i=1}^n \frac{h(X_i) + h(\nu(X_i))}{2}$$

---
# Variables antithétiques

Propriétés :
 - Sans biais
 - Fortement consistant
 - IC par TCL :
 
 $$\sqrt{n} (\hat{\mu}_a - \mu) \longrightarrow \mathcal{N}(0,\sigma_a^2)$$
 
Variance de l'estimateur :

--

$$\text{Var}(\hat{\mu}_a) =  \frac{1}{2n} \text{Var}(h(X))(1+\rho),$$
où $\rho = \text{Cov}(h(X),h(\nu(X)))/\text{Var}(h(X))$

<span style="color:#16A085">**fin du cours 9 (10/02/2025)**</span>


---
name: c10
# Variables antithétiques

Dans quel(s) cas cela fonctionne t-il ?

> **Proposition :** si $h$ est monotone et si $\nu$ est décroissante, alors la méthode des variables antithétiques fournit un estimateur de variance plus faible qu'avec l'approche classique.

*Preuve*

---
# Variables antithétiques

**Exemple : ** estimer $\int_0^1 \frac{1}{1+x^2}dx$.

```{r}
n <- 1000
u <- runif(n)
h <- function(x){1/(1+x^2)}
est1 <- mean(h(u))
var1 <- mean((h(u)-est1)^2)/n
u1 <- u[1:(n/2)]; u2 <- 1-u1
v <- c(u1,u2)
est2 <- mean(h(v))
var2 <- mean((0.5*(h(u1)+h(u2)) - est2)^2)/(n/2)
```

```{r, echo=F}
print(paste0("Estimateur 1 (MC classique) : ",round(est1,6)," variance : ",round(var1,10)))
print(paste0("Estimateur 2 (variables antithétiques): ",round(est2,6)," variance : ",round(var2,10)))
```

Vraie valeur : $\arctan(1) =$ `r atan(1)`.

---
# Variables de contrôle

L'objectif est d'utiliser une fonction auxiliaire dont on connaît l'espérance pour réduire la variance de l'estimateur.

> **Définition : ** soit $h_0$ une fonction t.q. $\mathbb{E}(h_0(X)) = m$ est connue et de variance finie. Soit $c \in \mathbb{R}$. L'estimateur par variable de contrôle est :
$$\hat{\mu}_c = \frac{1}{n} \sum_{i=1}^n [h(X_i) - c(h_0(X_i) - m)]$$

Propriétés :
 - Sans biais
 - fortement consistant
 - IC par TCL :
  $$\sqrt{n} (\hat{\mu}_c - \mu) \longrightarrow \mathcal{N}(0,\sigma_c^2)$$

---
# Variables de contrôle

Dans quel(s) cas cela fonctionne t-il ?

> **Proposition : ** l'estimateur optimal (au sens de la variance) est obtenu pour $c^* = \frac{\text{Cov}(h(X),h_0(X))}{\text{Var}(h_0(X))}$

```{r}
n <- 1000
X <- rnorm(n)
hX <- X>2
p1 <- mean(hX)
var1 <- var(hX)/n
h0X <- X>0
copt <- 2*(1-pnorm(2))
p2 <- mean(hX - copt*(h0X-0.5))
var2 <- mean((hX - copt*(h0X-0.5) - p2)^2)/n
```

```{r, echo=F}
print(paste0("Estimateur 1 (MC classique) : ",round(p1,6)," variance : ",round(var1,10)))
print(paste0("Estimateur 2 (variable de contrôle): ",round(p2,6)," variance : ",round(var2,10)))
```

Vraie valeur : `r (1-pnorm(2))` et $c^* =$ `r 2*(1-pnorm(2))`.

---
# Variables de contrôle

 - <span style="color:red">**En pratique le calcul de $c^*$ est impossible**</span>
 - On peut l'estimer sur un premier échantillon de taille réduite (compromis entre les simulations supplémentaires nécessaires pour estimer $c^*$ et le gain en variance)

```{r}
n <- 300
X <- rnorm(n)
hX <- X>2
h0X <- X>0
copt_hat <- cov(hX,h0X)/var(h0X)
n <- 1000
X <- rnorm(n)
hX <- X>2
h0X <- X>0
p2b <- mean(hX - copt*(h0X-0.5))
var2b <- mean((hX - copt*(h0X-0.5) - p2)^2)/n
```

Approximation de $c^*$ : `r copt_hat`.

```{r, echo=F}
print(paste0("Estimateur : ",round(p2b,6)," variance : ",round(var2b,10)))
```

<span style="color:#16A085">**fin du cours 10 (11/02/2025)**</span>



---
name: c11
class: inverse, middle, center

# 5. Chaînes de Markov

---
# Introduction

 - Quelques éléments théoriques pour comprendre les algos MCMC

 - Une chaîne de Markov est une séquence de variables aléatoires qui évolue avec le temps, et dont la probabilité de transition ne dépend que de la valeur de la chaîne à l'instant présent

 - chaîne à espace d'états discret ou continu, à temps discret ou continu

 &rarr; on va se concentrer sur le cas espace d'états continu et temps discret


---
# Introduction

Pourquoi s'intéresse t-on aux chaînes de Markov ?

 - pour générer des variables aléatoires *dépendantes* selon une chaîne de Markov convergente dont la loi stationnaire est la loi qui nous intéresse

 - le théorème ergodique nous permet alors d'utiliser cet échantillon dépendant pour approcher les mêmes quantités qu'avec le MC classique

---
# Introduction


Différence avec les méthodes de Monte Carlo : la façon dont on explore l'espace.

Exemple avec l'approximation du nombre $\pi$.
--

```{r, echo=FALSE, out.width='750px', fig.align='center'}
magick::image_read_pdf("/home/charlotte/Documents/Enseignement/Lille/M1 ISN /Statistiques computationnelles/Poly/pebble.pdf")
```


---
class: my-one-page-font
# Noyau de transition

 - chaîne de Markov $X_1, X_2, \dots, X_n$ de noyau de transition $K$ :
 $$X_n | X_{n-1}, X_{n-2}, \dots, X_2, X_1 \sim K(X_{n-1},X_{n})$$

--

 - chaîne homogène en temps si $\mathbb{P}(X_n \in A | X_{n-1} = x) = \mathbb{P}(X_1 \in A | X_0 = x)$

--

 - exemple : marche aléatoire simple
 $$X_n = X_{n-1} + \varepsilon_n, \quad \varepsilon \sim \mathcal{N}(0,1)$$

 le noyau de transition $K(X_{n-1},X_n)$ est celui de la loi $\mathcal{N}(X_{n-1},1)$.
```{r, echo=FALSE, fig.height=4, fig.width=5, fig.retina=2, fig.align='center'}
X0 <- rnorm(1)
n <- 1000
X <- rep(0,n)
for (i in 1:n){
  if (i == 1){
    X[i] <- X0 + rnorm(1)
  }else{
    X[i] <- X[i-1] + rnorm(1)
  }
}
plot(X,type="l",xlab="n",ylab="Xn")
```

---
# Noyau de transition

 - Propriété : $\forall n > 1, \forall x \in E, \forall A \in \mathcal{E}, \quad K^n(x,A) := \int_E K^{n-1}(y,A) K(x,dy)$

 &rarr; pour passer de l'état $x$ à l'ensemble $A$ en $n$ étapes, on passe d'abord de $x$ à $y$ en une étape, puis de $y$ à $A$ en $n-1$ étapes

 - Plus généralement, équation de Chapman-Kolmogorov :

 $K^{n+m}(x,A) := \int_E K^{m}(y,A) K^n(x,dy)$

 &rarr; pour passer de $x$ à $A$ en $n+m$ étapes, il faut passer par un état intermédiaire $y$ à l'étape $n$

---
# Temps d'arrêt et nombre de passages

 - pour un ensemble $A$, on définit le *temps d'arrêt* en $A$ par :

 $\tau_A = \inf \{ n | X_n \in A \}$
 &rarr; c'est le premier temps d'atteinte de l'ensemble $A$
 
```{r, echo=FALSE, fig.height=4, fig.width=5, fig.retina=2, fig.align='center'}
set.seed(160224)
X0 <- rnorm(1)
n <- 1000
X <- rep(0,n)
for (i in 1:n){
  if (i == 1){
    X[i] <- X0 + rnorm(1)
  }else{
    X[i] <- X[i-1] + rnorm(1)
  }
}
plot(X,type="l",xlab="n",ylab="Xn")
abline(h=5,col="#6BD7AF")
polygon(c(-100,1100,1100,-100),c(5,5,35,35),col="#6BD7AF")
lines(X,type="l",xlab="n",ylab="Xn")
abline(v=min(which(X>5)))
```
 - de même on définit le *nombre de passages* en $A$ par :
  $$\eta_A = \sum_{n=1}^{+\infty} \mathbb{1}_{X_n \in A}$$

---
# Mesure invariante

$\pi$ est une mesure de probabilité **invariante** pour la chaîne de noyau $K$ si
 $$\forall B \in \mathcal{E}, \pi(B) = \int_E K(x,B) \pi(dx)$$

 - exemple : $X_n = \rho X_{n-1} + \varepsilon_n$, avec $\varepsilon \sim \mathcal{N}(0,1)$

--

```{r}
rho <- 0.75; X <- rep(0,10000)
for (i in 1:10000){
  Xold <- ifelse(i==1,rnorm(1,0,1/(1-0.75^2)),X[i-1])
  X[i] <- rho*Xold + rnorm(1)}
```

```{r, echo=FALSE, fig.height=4, fig.width=5, fig.retina=2,fig.align='center'}
hist(X,freq=F,main="",ylim=c(0,0.275))
lines(seq(-5,5,0.001),dnorm(seq(-5,5,0.001),0,sqrt(1/(1-rho^2))),col="red",lwd=2)
```

---
# Irréductibilité

 - mesure la sensibilité de la chaîne aux conditions initiales

 - très important en pratique pour les algorithmes MCMC car apporte une garantie de convergence quel que soit le point de départ

 - dans le cas espace d'états discret, la chaîne est dite irréductible si tous les états communiquent

 - définition équivalente : $\forall x, \forall A$ t.q. $\mu(A)>0$, $\mathbb{E}_x(\eta_A) > 0$

 - <span style="color:red">condition suffisante : $K(x,\cdot) > 0$</span>

---
# Apériodicité

 - Irréductible : tout ensemble de mesure non nulle peut être atteint à partir de n'importe quel point de $E$ en un temps fini

--

 - Mais la chaîne peut rester bloquée dans un cycle

--

 &rarr; apériodicité

--

$\Rightarrow$ irréductibilité et apériodicité assurent que la chaîne peut aller de n'importe quel point $x$ à n'importe quel point $y$ **en une seule itération**

---
# Récurrence

 - Déplacement possible de $x$ à $y$ en une étape, pour tous $x$ et $y$

--

 - Mais ces déplacements doivent être possibles **une infinité de fois**

 &rarr; récurrence

---
# Résumé

 - Si une chaîne de Markov est récurrente et admet une loi stationnaire, cette loi est également la loi limite de la chaîne

 - C'est ce qu'on appelle l'**ergodicité**

 - Alors, si on peut générer une chaîne de Markov dont la loi stationnaire est la loi cible, on obtient un échantillon .red[**(non i.i.d.)**] selon cette loi

 &rarr; théorème ergodique
 
 
---
class: inverse, middle, center

# 6. Algorithmes MCMC

---
# Introduction

**Objectif** : évaluer $\int h(x) f(x) dx$

 - Idée : construire une chaîne de Markov ergodique, admettant pour loi stationnaire la loi cible $f$

 - Utiliser les réalisations de la chaîne pour approcher l'intégrale par une moyenne empirique

 - Intérêts :
  - réduction de variance
  - plus facile à mettre en place
  - plus adapté en grande dimension

 - Inconvénients :
  - vitesse de convergence qui peut être lente
  - forte corrélation possible dans l'échantillon

---
# Algorithme de Metropolis-Hastings

 - Article fondateur en 1953 par N Metropolis, A Rosenbluth, M Rosenbluth, A Teller, E Teller pour des distributions apparaissant en physique nucléaire

```{r, out.width='800px', echo=FALSE, fig.align='center'}
magick::image_read_pdf("/home/charlotte/Documents/Enseignement/Lille/M1 ISN /Statistiques computationnelles/Poly/photos_Met.pdf")
```

 - Généralisé en 1970 par W Hastings pour des distributions quelconques

```{r, out.width='150px', echo=FALSE, fig.align='center'}
magick::image_read_pdf("/home/charlotte/Documents/Enseignement/Lille/M1 ISN /Statistiques computationnelles/Poly/hastings.pdf")
```
---
# Algorithme de  Metropolis-Hastings

Construction d'une chaîne de Markov selon l'algorithme suivant :

 1. **initialisation** : $X_0$
 2. **pour** $n=1, \dots, N$ :
  - $Y_n \sim q(\cdot | X_{n-1})$ où $q$ est la *loi de proposition*
  - $\begin{equation}X_n = \begin{cases}Y_n & \text{avec probabilité } \alpha(X_{n-1},Y_n) \\ X_{n-1} & \text{avec probabilité } 1-\alpha(X_{n-1},Y_n) \end{cases}\end{equation}$

  où :
  $$\alpha(x,y) = \min \left(1, \frac{f(y) q(x | y)}{f(x) q(y | x)} \right) $$


RQ : fonctionne si on connaît $f$ et/ou $q$ à une constante près.

---
# Algorithme de Metropolis-Hastings

**Exemple** : loi cible $f$ loi $\mathrm{Beta}(2.5,8)$ et loi de proposition $q$ la loi $\mathcal{U}([0,1])$

```{r}
a=2.5; b=8
N <- 5000
X <- rep(0,N); X[1]=runif(1) # initialisation
for (i in 2:N){
  Y <- runif(1) # candidat généré selon la loi q
  alpha <- dbeta(Y,a,b)/dbeta(X[i-1],a,b)
  U <- runif(1)
  X[i] <- ifelse(U<alpha,Y,X[i-1])}
```

```{r, echo=FALSE, fig.height=3.5, fig.width=8, fig.retina=2, fig.align='center'}
dx <- data.frame(x=c(X,X[4801:5000]),n=c(1:5000,4801:5000),type=c(rep(".Toute la trajectoire",5000),rep("200 dernières itérations",200)))
ggplot(data=dx,aes(x=n,y=x))+geom_line()+facet_wrap(~type,scales = "free")
```

<span style="color:#16A085">**fin du cours 11 (24/02/2025)**</span>

---
name: c12
# Algorithme de Metropolis-Hastings

Construction d'une chaîne de Markov (**dans** $\mathbb{R}^p$) selon l'algorithme suivant :

 1. **initialisation** : $X_0$
 2. **pour** $n=1, \dots, N$ :
  - $Y_n \sim q(\cdot | X_{n-1})$ où $q$ est la *loi de proposition*

  - $\begin{equation}X_n = \begin{cases}Y_n & \text{avec probabilité } \alpha(X_{n-1},Y_n) \\ X_{n-1} & \text{avec probabilité } 1-\alpha(X_{n-1},Y_n) \end{cases}\end{equation}$

  où : $\alpha(X_{n-1},Y_n) = \min \left(1, \frac{f(Y_n)}{f(X_{n-1})}\frac{ q(X_{n-1} | Y_n)}{q(Y_n | X_{n-1})} \right)$


Toutes choses égales par ailleurs, $\alpha(X_{n-1},Y_n)$ augmente :
- avec le ratio $\frac{f(Y_n)}{f(X_{n-1})}$,
- avec le ratio $\frac{q(X_{n-1} | Y_n)}{q(Y_n | X_{n-1})}$
- <span style="color:red">**un candidat t.q. $\frac{f(Y_n)}{q(Y_n | X_{n-1})} > \frac{f(X_{n-1})}{q(X_{n-1} | Y_n)}$ est toujours accepté**</span>

---
# Algorithme de Metropolis-Hastings

**Exemple** : loi cible $f$ loi $\mathrm{Beta}(2.5,8)$ et loi de proposition $q$ la loi $\mathcal{U}([0.8X_{n-1},1.2X_{n-1}])$

--

```{r}
set.seed(19022024)
a=2.5; b=8
N <- 5000
X <- rep(0,N); X[1] <- runif(1)
for (i in 2:10){
    Y <- runif(1,0.8*X[i-1],1.2*X[i-1]) # candidat généré selon la loi q
    alpha <- (dbeta(Y,a,b)/dbeta(X[i-1],a,b)) *
      (dunif(X[i-1],0.8*Y,1.2*Y)/dunif(Y,0.8*X[i-1],1.2*X[i-1]))
    U <- runif(1)
    X[i] <- ifelse(U<alpha,Y,X[i-1])}
print(X[1:10])
```

---
# Algorithme de Metropolis-Hastings

Etat courant : $X_{n-1} =$ `r round(X[10],4)`, loi instrumentale : $\mathcal{U}$([`r 0.8*round(X[10],4)`,`r 1.2*round(X[10],4)`]).
--
```{r, echo=FALSE, fig.height=5, fig.width=6, fig.retina=2, fig.align='center'}
plot(seq(0,1,0.001),dbeta(seq(0,1,0.001),2.5,8),type="l",ylim=c(0,5),xlab="",ylab="")
lines(seq(0,1,0.001),dunif(seq(0,1,0.001),0.8*X[10],1.2*X[10]),col="darkgrey")
points(X[2:10],rep(0,9),col="darkgrey",pch=16)
points(X[1],0,col="black",pch=16)
points(X[10],0,col="black",lwd=3,pch=4,cex=1.2)
set.seed(0);Y <- runif(1,0.8*X[10],1.2*X[10])
legend("topright",bty="n",pch=c(16,4),legend=c("Initialisation","Etat courant"))
```

--
Candidat à l'itération $n$ : $Y_n$ = `r Y`


---
# Algorithme de Metropolis-Hastings

Etat courant : $X_{n-1} =$ `r round(X[10],4)`, loi instrumentale : $\mathcal{U}$([`r 0.8*round(X[10],4)`,`r 1.2*round(X[10],4)`]).
```{r, echo=FALSE, fig.height=5, fig.width=6, fig.retina=2, fig.align='center'}
plot(seq(0,1,0.001),dbeta(seq(0,1,0.001),2.5,8),type="l",ylim=c(0,5),xlab="",ylab="")
lines(seq(0,1,0.001),dunif(seq(0,1,0.001),0.8*X[10],1.2*X[10]),col="darkgrey")
points(X[2:10],rep(0,9),col="darkgrey",pch=16)
points(X[1],0,col="black",pch=16)
points(X[10],0,col="black",lwd=3,pch=4,cex=1.2)
points(Y,0,col="red",lwd=3,pch=4,cex=1.2)
legend("topright",bty="n",pch=c(16,4,4),col=c("black","black","red"),legend=c("Initialisation","Etat courant","Candidat"))
```
Candidat à l'itération $n$ : $Y_n$ = `r Y`

$\frac{f(Y_n)}{q(Y_n | X_{n-1})}$ = `r round(dbeta(Y,a,b)/dunif(Y,0.8*X[10],1.2*X[10]),6)`, $\frac{f(X_{n-1})}{q(X_{n-1} | Y_{n})}$ = `r round(dbeta(X[10],a,b)/dunif(X[10],0.8*Y,1.2*Y),6)`, $\frac{f(Y_n)}{f(X_{n-1})}$ = `r round(dbeta(Y,a,b)/dbeta(X[10],a,b),6)` et $\frac{q(X_{n-1} | Y_n)}{q(Y_n | X_{n-1})}$ = `r round(dunif(X[10],0.8*Y,1.2*Y)/dunif(Y,0.8*X[10],1.2*X[10]),6)`

$\alpha(X_{n-1},Y_n)$ = `r (dbeta(Y,a,b)/dbeta(X[10],a,b)) * (dunif(X[10],0.8*Y,1.2*Y)/dunif(Y,0.8*X[10],1.2*X[10]))`

on génère $U \sim \mathcal{U}([0,1])$ : `r runif(1)`


---
# Algorithme de Metropolis-Hastings

Preuve de convergence :
--

 1. noyau de transition de la chaîne
--

 2. condition suffisante d'existence d'une loi stationnaire : *detailed-balanced condition*
--
 
 3. condition suffisante pour l'apériodicité
--
 
 4. condition suffisante pour l'irréductibilité

--

Choix de la loi $q$ : deux cas particuliers
 - loi instrumentale indépendante de $X_{n-1}$
 - loi instrumentale symétrique 

---
# MH à loi instrumentale indépendante

Cas où $q$ est indépendant de $X_{n-1}$ &rarr; $q(x | y) = q(x)$

 - la loi instrumentale ne dépend pas de l'état courant de la chaîne.
 - voir l'exemple de la slide 141 (loi cible $\text{Beta}(2.5,8)$ et loi de proposition $q$ la loi $\mathcal{U}([0,1])$)
 - la probabilité d'acceptation s'écrit : $\alpha(X_{n-1},Y_n) = \min \left(1, \frac{f(Y_n)}{f(X_{n-1})}\frac{ q(X_{n-1})}{q(Y_n)} \right)$

 - s'il existe $M$ telle que $f\leq Mq$ alors la chaîne est *uniformément ergodique* (propriété très forte)
 - comparaison avec l'algorithme d'acceptation-rejet &rarr; voir TD

---
# MH à loi instrumentale symétrique

Cas où $q$ est symétrique en $x$ et $y$ &rarr; $q(y | x) = q(x | y)$

 - souvent on a $q(y | x) = q(|y - x|)$
 &rarr; exemple de la marche aléatoire symétrique
 
 - voir l'exemple de la slide 143 (loi cible $\text{Beta}(2.5,8)$ et loi de proposition $q$ la loi $\mathcal{U}([0.8X_{n-1},1.2X_{n-1}])$)
 - la probabilité d'acceptation se simplifie : $\alpha(X_{n-1},Y_n) = \min \left(1, \frac{f(Y_n)}{f(X_{n-1})}\right)$

 - si $\text{supp}(f) \subset \text{supp}(q)$ alors la chaîne est ergodique de mesure invariante $f$

 - RQ : si $f$ est à support dans $\mathbb{R}$, la chaîne ne pourra jamais être uniformément ergodique

 - .red[**attention au choix de la variance de la marche aléatoire**]

---
# MH à marche aléatoire (MH-MA)

**Exemple** : loi cible $f$ la loi normale centrée réduite, et loi instrumentale $q$ la loi normale centrée en $X_{n-1}$ et de variance $\sigma^2$, pour différentes valeurs de $\sigma$.

```{r}
X1 <- X2 <- X3 <- rep(0,2000);
for (i in 2:2000){
  # chaine 1
  Y <- rnorm(1,X1[i-1],0.1)
  alpha <- dnorm(Y)/dnorm(X1[i-1])
  X1[i] <- ifelse(runif(1)<alpha,Y,X1[i-1])
  # chaine 2
  Y <- rnorm(1,X2[i-1],2)
  alpha <- dnorm(Y)/dnorm(X2[i-1])
  X2[i] <- ifelse(runif(1)<alpha,Y,X2[i-1])
  # chaine 3
  Y <- rnorm(1,X3[i-1],15)
  alpha <- dnorm(Y)/dnorm(X3[i-1])
  X3[i] <- ifelse(runif(1)<alpha,Y,X3[i-1])
}
```


---
# MH à marche aléatoire (MH-MA)


```{r echo=FALSE, fig.height=5, fig.width=15, fig.retina=2, fig.align='center'}
par(mfrow=c(1,3))
plot(X1,type="l",xlab="Itération")
plot(X2,type="l",xlab="Itération")
plot(X3,type="l",xlab="Itération")
```

--

```{r, echo=FALSE, fig.height=5, fig.width=15, fig.retina=2, fig.align='center'}
x <- seq(-3,3,0.01)
par(mfrow=c(1,3))
hist(X1,freq = FALSE, xlim=c(-3,3))
lines(x,dnorm(x),col="red",lwd=2)
hist(X2,freq = FALSE)
lines(x,dnorm(x),col="red",lwd=2)
hist(X3,freq = FALSE)
lines(x,dnorm(x),col="red",lwd=2)
```

---
# MH à marche aléatoire (MH-MA)

Pour diagnostiquer une mauvaise exploration :
- calculer le **taux d'acceptation** (la proportion de candidats ayant été acceptés)
   - ne doit être ni trop haut, ni trop bas
   - sur l'exemple précédent : `r length(unique(X1))/2000`, `r length(unique(X2))/2000` et `r length(unique(X3))/2000` pour les chaînes 1, 2 et 3
   - recommandation pratique : <span style="color:#16A085"> $\approx$ 0.45 en dim 1 et 2, $\approx$ 0.25 si $p>2$</span>
--

- étudier l'**autocorrélation** des réalisations de la chaîne (fonction `acf()` sous R, `plot_acf()` du module `statsmodels.graphics.tsaplots` sous Python)

```{r, echo=FALSE, fig.height=4.5, fig.width=13.5, fig.retina=2, fig.align='center'}
par(mfrow=c(1,3))
acf(X1)
acf(X2)
acf(X3)
```

<span style="color:#16A085">**fin du cours 12 (25/02/2025)**</span>


---
name: c13

# De MH à l'échantillonneur de Gibbs

 - Algorithme générique qui fonctionne pour une grande classe de lois $f$
 - Le choix de $q$ ne requiert pas une grande connaissance de $f$
 - On peut améliorer l'efficacité des algorithmes (e.g. la convergence) en exploitant les caractéristiques de $f$

 &rarr; algorithmes utilisant la forme de $f$ et sa géométrie

---
# Slice sampler

> **Théorème** : simuler $X$ selon la loi $f$ est équivalent à simuler le couple $(X,U)$ selon la loi uniforme sur l'ensemble $\mathcal{D} = \{(x,u) : 0 \leq u \leq f(x)\}$

*Preuve*

--

- Pour simuler $X$ selon la loi $f$, on peut alors simuler un point $(X,U)$ uniformément **sous le graphe** de la densité $f$ et garder uniquement l'abscisse des points (la loi marginale de $X$)

- C'est déjà ce que l'on fait dans la méthode d'acceptation-rejet en simulant dans une région **plus grande** et en ne gardant que les points **sous la courbe**

- L'algorithme du slice sampler permet d'atteindre le même objectif mais de façon plus efficace en proposant un déplacement de type marche aléatoire sur l'ensemble $\mathcal{D}$.

---
# Slice sampler

Exemple de la loi $\text{Beta}(a,b)$. Avec l'acceptation-rejet (voir TD) :
 - on simule $Y \sim \mathcal{U}([0,1])$
 - puis $U \sim \mathcal{U}([0,M_{a,b}])$
 - et on garde les couples $(Y,U)$ tels que $U < f(Y)$
 
```{r, echo=FALSE, fig.height=5,fig.width=7,fig.retina=2,fig.align='center'}
set.seed(0)
a <- 5; b <- 2
mode <- (a-1)/(a+b-2)
Mab <- dbeta(mode,a,b)
n <- 1000
Y <- runif(n,0,1)
U <- runif(n,0,Mab)
accept <- U<dbeta(Y,a,b)
X <- Y[accept]
Ux <- U[accept]
plot(Y,U,pch=19) 
lines(seq(0,1,0.01),dbeta(seq(0,1,0.01),a,b),col="red",lwd=4) # on veut les points sous la courbe rouge
points(X,Ux,pch=19,col="red") 
``` 

&rarr; on perd $1-1/M$ soit environ `r 100*(1 - round(1/Mab,3))`% des points ...

---
# Slice sampler

En pratique :
  1. **initialisation** : $X_0=x_0$
  2. **pour** $n=1,\dots,N$
    - $U_n \sim \mathcal{U}([0,f(x_{n-1})])$
    - $X_n \sim \mathcal{U}(\mathcal{D}_n)$ où
    $\mathcal{D}_n = \{x : f(x) \geq u_n\}$

```{r, out.width='500px', echo=FALSE, fig.align='center'}
magick::image_read("/home/charlotte/Documents/Enseignement/Lille/M1 ISN /Statistiques computationnelles/Poly/slice_sampler.png")
```
*(source : Andrieu et al. 2003)*


<span style="color:red">**RQ. : on peut utiliser l'algorithme si $f$ est connue à une constante multiplicative près**</span>


---
# Slice sampler

**Exemple** : on veut simuler selon la loi $\mathcal{N}(0,1)$, de densité $f(x) \propto e^{-x^2/2}$

 &rarr; à l'itération $n$, on a $X_{n-1}=x_{n-1}$ :
  - on génère $U_n \sim \mathcal{U}([0,\exp(-x_{n-1}^2/2)])$
  - on détermine l'ensemble $\mathcal{D}_n = \{x : e^{-x^2/2} > u_n\}$
--

  - on simule $X_n \sim \mathcal{U}([-\sqrt{-2 \log(u_n)} ; \sqrt{-2 \log(u_n)}])$

--
```{r, echo=FALSE, fig.height=5,fig.width=7,fig.retina=2,fig.align='center'}
set.seed(0)
x <- seq(-3,3,0.01)
plot(x,exp(-x^2/2),type="l",xlab="",ylab="",axes=FALSE,ylim=c(0,sqrt(2*pi)*0.45))
axis(1, pos=0)
axis(2, pos=0, labels=FALSE,at=c(0,sqrt(2*pi)*0.5))
X1<-rnorm(1)
points(X1,0,pch=19)
text(X1,par("usr")[3]-0.01,labels="X(n-1)",xpd=NA,cex=0.75)
```

---
# Slice sampler

**Exemple** : on veut simuler selon la loi $\mathcal{N}(0,1)$, de densité $f(x) \propto e^{-x^2/2}$

 &rarr; à l'itération $n$, on a $X_{n-1}=x_{n-1}$ :
  - on génère $U_n \sim \mathcal{U}([0,\exp(-x_{n-1}^2/2)])$
  - on détermine l'ensemble $\mathcal{D}_n = \{x : e^{-x^2/2} > u_n\}$
  - on simule $X_n \sim \mathcal{U}([-\sqrt{-2 \log(u_n)} ; \sqrt{-2 \log(u_n)}])$


```{r, echo=FALSE, fig.height=5,fig.width=7,fig.retina=2,fig.align='center'}
set.seed(0)
x <- seq(-3,3,0.01)
plot(x,exp(-x^2/2),type="l",xlab="",ylab="",axes=FALSE,ylim=c(0,sqrt(2*pi)*0.45))
axis(1, pos=0)
axis(2, pos=0, labels=FALSE,at=c(0,sqrt(2*pi)*0.5))
X1<-rnorm(1)
points(X1,0,pch=19)
text(X1,par("usr")[3]-0.01,labels="X(n-1)",xpd=NA,cex=0.75)
segments(X1,0,X1,sqrt(2*pi)*dnorm(X1),lty=2)
U1 <- runif(1,0,exp(-X1^2/2))
points(X1,U1,pch=4,lwd=3)
text(X1+0.17,U1,labels="Un",xpd=NA,cex=0.75)
```


---
# Slice sampler

**Exemple** : on veut simuler selon la loi $\mathcal{N}(0,1)$, de densité $f(x) \propto e^{-x^2/2}$

 &rarr; à l'itération $n$, on a $X_{n-1}=x_{n-1}$ :
  - on génère $U_n \sim \mathcal{U}([0,\exp(-x_{n-1}^2/2)])$
  - on détermine l'ensemble $\mathcal{D}_n = \{x : e^{-x^2/2} > u_n\}$
  - on simule $X_n \sim \mathcal{U}([-\sqrt{-2 \log(u_n)} ; \sqrt{-2 \log(u_n)}])$


```{r, echo=FALSE, fig.height=5,fig.width=7,fig.retina=2,fig.align='center'}
set.seed(0)
x <- seq(-3,3,0.01)
plot(x,exp(-x^2/2),type="l",xlab="",ylab="",axes=FALSE,ylim=c(0,sqrt(2*pi)*0.45))
axis(1, pos=0)
axis(2, pos=0, labels=FALSE,at=c(0,sqrt(2*pi)*0.5))
X1<-rnorm(1)
points(X1,0,pch=19)
text(X1,par("usr")[3]-0.01,labels="X(n-1)",xpd=NA,cex=0.75)
segments(X1,0,X1,sqrt(2*pi)*dnorm(X1),lty=2)
U1 <- runif(1,0,exp(-X1^2/2))
points(X1,U1,pch=4,lwd=3)
text(X1+0.17,U1,labels="Un",xpd=NA,cex=0.75)
segments(-sqrt(-2*log(U1)),U1,sqrt(-2*log(U1)),U1,lty=3)
#segments(-sqrt(-2*log(U1)),0,-sqrt(-2*log(U1)),U1,lty=3)
#segments(sqrt(-2*log(U1)),0,sqrt(-2*log(U1)),U1,lty=3)
segments(-sqrt(-2*log(U1)),0,sqrt(-2*log(U1)),0,col="red",lwd=2)
```

---
# Slice sampler

**Exemple** : on veut simuler selon la loi $\mathcal{N}(0,1)$, de densité $f(x) \propto e^{-x^2/2}$

 &rarr; à l'itération $n$, on a $X_{n-1}=x_{n-1}$ :
  - on génère $U_n \sim \mathcal{U}([0,\exp(-x_{n-1}^2/2)])$
  - on détermine l'ensemble $\mathcal{D}_n = \{x : e^{-x^2/2} > u_n\}$
  - on simule $X_n \sim \mathcal{U}([-\sqrt{-2 \log(u_n)} ; \sqrt{-2 \log(u_n)}])$

```{r, echo=FALSE, fig.height=5,fig.width=7,fig.retina=2,fig.align='center'}
set.seed(0)
x <- seq(-3,3,0.01)
plot(x,exp(-x^2/2),type="l",xlab="",ylab="",axes=FALSE,ylim=c(0,sqrt(2*pi)*0.45))
axis(1, pos=0)
axis(2, pos=0, labels=FALSE,at=c(0,sqrt(2*pi)*0.5))
X1<-rnorm(1)
points(X1,0,pch=19)
text(X1,par("usr")[3]-0.01,labels="X(n-1)",xpd=NA,cex=0.75)
segments(X1,0,X1,sqrt(2*pi)*dnorm(X1),lty=2)
U1 <- runif(1,0,exp(-X1^2/2))
points(X1,U1,pch=4,lwd=3)
text(X1+0.17,U1,labels="Un",xpd=NA,cex=0.75)
segments(-sqrt(-2*log(U1)),U1,sqrt(-2*log(U1)),U1,lty=3)
segments(-sqrt(-2*log(U1)),0,sqrt(-2*log(U1)),0,col="red",lwd=2)
X2<-runif(1,-sqrt(-2*log(U1)),sqrt(-2*log(U1)))
points(X2,0,pch=19,lwd=3,col="red")
text(X2,par("usr")[3]-0.01,labels="X(n)",xpd=NA,cex=0.75)
```


---
# Slice sampler

```{r, out.width='500px', echo=FALSE, fig.align='center'}
magick::image_read("/home/charlotte/Documents/Enseignement/Lille/M1 ISN /Statistiques computationnelles/Poly/slice_sampler_in_action.png")
```
.center[*(source : Bayesian Statistics Group)*]

.pull-left[
.green[**Avantages**]
- tirages selon des lois uniformes
- pas d'hypothèses spécifiques sur $f$
- $f$ peut être connue à une constante près
]
.pull-right[
.red[**Inconvénients**]
- calcul de $\mathcal{D}_n$ peut être délicat
- extensions possibles dans ce cas qui peuvent être plus lourdes à implémenter
]

---
# Slice sampler

Exemple avec la loi $\text{Beta}(2,5)$, on a $\mathcal{D_n} = \{(x,u) : C x (1-x)^4 > u_n \}$.


```{r}
set.seed(0)
X <- runif(1)
U <- runif(1,0,dbeta(X,5,2))
Dn <- rootSolve::uniroot.all(function(x){dbeta(x,5,2)-U},lower=0,upper=1) #<<
Dn
```
```{r, echo=FALSE, fig.height=5,fig.width=7,fig.retina=2,fig.align='center'}
curve(dbeta(x,5,2),lwd=2)
segments(Dn[1],U,Dn[2],U,col="lightgray",lwd=8)
points(X,U,pch=19,cex=2)
```

---
# Slice sampler

```{r, warning=FALSE, eval=F}
n <- 1000
X <- runif(1)
for (i in 2:n){
 U <- runif(1,0,dbeta(X[i-1],5,2))
 Dn <- uniroot.all(function(x){dbeta(x,5,2)-U[i]},lower=0,upper=1) 
 X <- c(X,runif(1,Dn[1],Dn[2]))
}
```


```{r, echo=FALSE, fig.height=5,fig.width=7,fig.retina=2,fig.align='center'}
n <- 1000
X <- runif(1)
U <- 0
for (i in 2:n){
 U <- c(U,runif(1,0,dbeta(X[i-1],5,2)))
 Dn <- rootSolve::uniroot.all(function(x){dbeta(x,5,2)-U[i]},lower=0,upper=1) #<<
 X <- c(X,runif(1,Dn[1],Dn[2]))
}
plot(X,U,pch=19,col="red") 
lines(seq(0,1,0.01),dbeta(seq(0,1,0.01),a,b),col="red",lwd=4) 
```

---
# Echantillonneur de Gibbs

Idée générale : en dimension $p$ > 1, simuler chaque coordonnée l'une après l'autre

--

**Cas $p=2$**

$(X,Y) \in \mathbb{R}^2$
  1. **initialisation** : $(X_0,Y_0) = (x_0,y_0)$
  2. **pour** $n=1, \dots, N$ :
   - $X_n \sim f_{X|Y}(\cdot | y_{n-1})$
   - $Y_n \sim f_{Y|X}(\cdot | x_{n})$

--

**Exemple** : $Z=(X,Y) \sim \mathcal{N}\left(\begin{pmatrix}0 \\0 \end{pmatrix}, \begin{pmatrix}1 & \rho \\ \rho & 1 \end{pmatrix}\right)$

---
# Echantillonneur de Gibbs

```{r}
n <- 10000; rho <- 0.8; X <- Y <- rep(0,n);
for (i in 2:n){
  # mise à jour de la 1ère coordonnée
  X[i] <- rnorm(1,rho*Y[i-1],sqrt(1-rho^2))
  # mise à jour de la 2ème coordonnée
  Y[i] <- rnorm(1,rho*X[i],sqrt(1-rho^2)) }
```

.pull-left[
```{r, echo=FALSE, fig.height=5, fig.width=5, fig.retina=2}
dXY2=data.frame(X=X,Y=Y)
ggplot(dXY2,aes(x=X,y=Y)) + geom_point()
```
]
.pull-right[
```{r, echo=FALSE, fig.height=5.2, fig.width=6, fig.retina=2}
ggplot(dXY2,aes(x=X,y=Y)) + geom_density_2d_filled(alpha = 0.5) +
  geom_density_2d(linewidth = 0.25, colour = "black")
```
]

---
# Echantillonneur de Gibbs

```{r, eval=FALSE}
n <- 10000; rho <- 0.8; X <- Y <- rep(0,n);
for (i in 2:n){
  # mise à jour de la 1ère coordonnée
  X[i] <- rnorm(1,rho*Y[i-1],sqrt(1-rho^2))
  # mise à jour de la 2ème coordonnée
  Y[i] <- rnorm(1,rho*X[i],sqrt(1-rho^2)) }
```

.pull-left[
```{r, echo=FALSE, fig.height=4, fig.width=5, fig.retina=2, message=FALSE}
ggplot(dXY2,aes(x=X)) + geom_histogram(aes(y=after_stat(density)),fill="grey",col="darkgrey") +  stat_function(fun = dnorm) + ggtitle("Histogramme de la coordonnée X")
```
]
.pull-right[
```{r, echo=FALSE, fig.height=4, fig.width=5, fig.retina=2, message=FALSE}
ggplot(dXY2,aes(x=Y)) + geom_histogram(aes(y=after_stat(density)),fill="grey",col="darkgrey") +  stat_function(fun = dnorm) + ggtitle("Histogramme de la coordonnée Y")
```
]

---
# Echantillonneur de Gibbs

En pratique, comment se passe l'itération $n$ ?

<ol>
<li> on part du point $(X_{n-1},Y_{n-1})$
</ol>

```{r, echo=FALSE, fig.height=6,fig.width=8,fig.retina=2,fig.align='center'}
set.seed(220224)
n <- 1000
rho <- 0.2
XY <- matrix(0,nc=2,nr=2*n); XY[1,] <- runif(2)
for (i in seq(2,2*n-1,by=2)){
  # mise à jour de la 1ère coordonnée
  XY[i,1] <- rnorm(1,rho*XY[i-1,2],1-rho^2)
  XY[i,2] <- XY[i-1,2]
  # mise à jour de la 2ème coordonnée
  XY[i+1,2] <- rnorm(1,rho*XY[i,1],1-rho^2)
  XY[i+1,1] <- XY[i,1]
}

dXY=data.frame(XY);names(dXY)=c("X","Y")
dXY[1,] %>% ggplot(aes(x=X,y=Y)) + geom_point() + xlim(c(-2,2)) + ylim(c(-2,2)) + annotate(geom="text",x=dXY[1,1]-0.2,y=dXY[1,2]+0.175,label="X[n-1],Y[n-1]")
```

---
# Echantillonneur de Gibbs

En pratique, comment se passe l'itération $n$ ?

<ol start="2">
<li> on génère la 1ère coordonnée conditionnellement à $Y_{n-1}=y_{n-1}$
</ol>

```{r, echo=FALSE, fig.height=6,fig.width=8,fig.retina=2,fig.align='center'}
dXY[1:2,] %>% ggplot(aes(x=X,y=Y)) + geom_point() + xlim(c(-2,2)) + ylim(c(-2,2)) + geom_line(col="darkgrey") +  annotate(geom="text",x=dXY[1,1]-0.2,y=dXY[1,2]+0.175,label="X[n-1],Y[n-1]") +
  annotate(geom="text",x=dXY[2,1],y=dXY[2,2]+0.175,label="X[n]",col="red") +
  annotate(geom="text",x=dXY[2,1]+0.25,y=dXY[2,2]+0.175,label=",Y[n-1]")
```

---
# Echantillonneur de Gibbs

En pratique, comment se passe l'itération $n$ ?

<ol start="3">
<li> on génère la 2ème coordonnée conditionnellement à $X_n=x_n$
</ol>

```{r, echo=FALSE, fig.height=6,fig.width=8,fig.retina=2,fig.align='center'}
dXY[1:3,] %>% ggplot(aes(x=X,y=Y)) + geom_point() + xlim(c(-2,2)) + ylim(c(-2,2)) + geom_line(col="darkgrey") +  annotate(geom="text",x=dXY[1,1]-0.2,y=dXY[1,2]+0.175,label="X[n-1],Y[n-1]") +
  annotate(geom="text",x=dXY[2,1],y=dXY[2,2]+0.175,label="X[n]",col="red") +
  annotate(geom="text",x=dXY[2,1]+0.25,y=dXY[2,2]+0.175,label=",Y[n-1]") +
  annotate(geom="text",x=dXY[3,1]+0.25,y=dXY[3,2],label="X[n],Y[n]",col="red") +
  geom_line(data=dXY[c(1,3),],col="black")
```

---
# Echantillonneur de Gibbs

&rarr; on ne fait que des mouvements *parallèles aux axes*

```{r, echo=FALSE, fig.height=6,fig.width=8,fig.retina=2,fig.align='center'}
dXY %>% ggplot(aes(x=X,y=Y)) + geom_point() + geom_path()
```

---
# Echantillonneur de Gibbs

 - A un intérêt lorsque la loi jointe $f_{X,Y}$ ou les lois marginales $f_X$ ou $f_Y$ ne peuvent pas être calculées explicitement

--

 - La chaîne $(X_t,Y_t)_t$ est stationnaire de loi $f_{X,Y}$, de même que $(X_t)_t$ et $(Y_t)_t$ sont stationnaires de loi $f_X$ et $f_Y$ resp.

--

 - Simuler selon les lois conditionnelles **suffit** pour obtenir un échantillon selon la loi jointe (théorème de Hammersley-Clifford)
 - .blue[**Question :** Est-ce le cas si on remplace 'loi conditionnelle' par 'loi marginale' ?]

--

 - C'est un cas particulier de l'algorithme de Metropolis-Hastings

---
# Echantillonneur de Gibbs

**Cas général $p>2$**

$(X_1,\dots,X_p) \in \mathbb{R}^p$ de loi jointe $f$
  1. **initialisation** : $(X_{0,1},\dots,X_{0,p}) = (x_{0,1},\dots,x_{0,p})$
  2. **pour** $n=1, \dots, N$ :
   - $X_{n,1} \sim f_1(\cdot | x_{n-1,2},\dots,x_{n-1,p})$
   - $X_{n,2} \sim f_2(\cdot | x_{n,1},x_{n-1,3},\dots,x_{n-1,p})$
   - $\dots$
   - $X_{n,p} \sim f_p(\cdot | x_{n,1},\dots,x_{n,p-1})$

- Les densités $f_1,\dots,f_p$ sont appelées *lois conditionnelles complètes*.
- On ne fait que des simulations en dimension 1

---
# Echantillonneur de Gibbs

Pourquoi ça marche ?

> *Théorème de Hammersley-Clifford.* Si le support de la loi jointe est le produit cartésien des supports des lois marginales, alors :
$$f(x_1,\dots,x_p) \propto \prod_{j=1}^p \frac{f_j(x_j | x_1,\dots,x_{j-1},z_{j+1},\dots,z_p)}{f_j(z_j | x_1,\dots,x_{j-1},z_{j+1},\dots,z_p)}$$
pour tout vecteur $(z_1,\dots,z_p)$ dans le support de $f$.

</br>
Propriétés de convergence :
 - Mesure invariante
 - Irréductibilité et récurrence
 - Apériodicité

---
# Condition de positivité

**Exemple :** loi cible $f(x,y) = \frac{1}{2} \left(\mathbb{1}_{[-1,0]\times[-1,0]}(x,y) + \mathbb{1}_{[0,1]\times[0,1]}(x,y)\right)$

--

```{r, echo=FALSE, fig.align='center',fig.height=6,fig.width=8,fig.retina=2}
d <- data.frame(x=seq(-1.5,1.5,100),y=seq(-1.5,1.5,100))
p <- ggplot(d,aes(x,y)) + geom_rect(aes(xmin=-1,ymin=-1,xmax=0,ymax=0),fill="grey") + geom_hline(yintercept = 0) +
  geom_rect(aes(xmin=0,ymin=0,xmax=1,ymax=1),fill="grey") + xlim(c(-1.2,1.2)) + ylim(c(-1.2,1.2)) + geom_vline(xintercept = 0)
p
```

---
# Condition de positivité

**Exemple :** loi cible $f(x,y) = \frac{1}{2} \left(\mathbb{1}_{[-1,0]\times[-1,0]}(x,y) + \mathbb{1}_{[0,1]\times[0,1]}(x,y)\right)$

```{r, echo=FALSE, fig.align='center',fig.height=6,fig.width=8,fig.retina=2}
fjointe <- function(x,y){return(0.5*(-1 < x & x < 0 & -1 < y & y < 0) + 0.5*(0 < x & x < 1 & 0 < y & y < 1))}
fxcondy <- function(x,y){fjointe(x,y)/dunif(y,-1,1)}
fycondx <- function(x,y){fjointe(x,y)/dunif(x,-1,1)}

init <- runif(2,-1,1)
chaine <- c(init)
last <- init
for (n in 1:100){
  x <- ifelse(0 < last[2] & last[2] < 1, runif(1,0,1), runif(1,-1,0))
  y <- ifelse(0 < x & x < 1, runif(1,0,1), runif(1,-1,0))
  chaine <- rbind(chaine,c(x,last[2]))
  chaine <- rbind(chaine,c(x,y))
  last <- c(x,y)
}

dchaine <- as.data.frame(chaine)
names(dchaine) <- c("x","y")

p + geom_path(data=dchaine,aes(x,y),col=grey(0.4)) + geom_point(data=dchaine[2*(1:100),],aes(x,y)) +
  geom_point(data=dchaine[1,],col="red",size=2)
```

---
class: my-one-page-font
# Variantes

 - Gibbs avec mise à jour aléatoire : à chaque itération $n$, on choisit aléatoirement la coordonnée à mettre à jour
 - Metropolis-within-Gibbs : si on ne sait pas simuler selon $f_k$, on peut remplacer par une étape de Metropolis-Hastings :
 
  **pour** $n=1, \dots, N$ et la coordonnée $k$ :
   - $Y_{n,k} \sim q_k(\cdot | x_{n,1},\dots,x_{n,k-1},x_{n-1,k},x_{n-1,k+1},\dots,x_{n-1,p})$ 
   
   - on pose $X_{n,k} = Y_{n,k}$ avec probabilité $\alpha_k(X_{n-1,k},Y_{n,k})$ et $X_{n,k}=X_{n-1,k}$ avec probabilité $1- \alpha_k(X_{n-1,k},Y_{n,k})$, avec :
   
    $$\alpha_k(X_{n-1,k},Y_{n,k}) = \min\left[1,\frac{\frac{f_k(Y_{n,k} | X_{n,1},\dots,X_{n,k-1},X_{n-1,k+1},\dots,X_{n-1,p})}{q_k(Y_{nk,} | X_{n,1},\dots,X_{n,k-1},X_{n-1,k},X_{n-1,k+1},\dots,X_{n-1,p})}}{\frac{q_k(X_{n-1,k} | X_{n,1},\dots,X_{n,k-1},Y_{n,k},X_{n-1,k+1},\dots,X_{n-1,p})}{f_k(X_{n-1,k} | X_{n,1},\dots,X_{n,k-1},X_{n-1,k+1},\dots,X_{n-1,p})} } \right]$$
    

<span style="color:#16A085">**fin du cours 13 (04/03/2025)**</span>

---
name: c14
# Résumé des épisodes précédents

.bold[Objectif :] estimer $\mu = \int h(x)f(x)dx$ avec $f$ densité de probabilité

--

**Méthode :** <span style="color:orange"> **obtenir des réalisations $X_1,\dots,X_n$ selon la loi $f$**</span> et <span style="color:blue">**poser $\hat{\mu} = \frac{1}{n}\sum_{i=1}^n h(X_i)$**</span>

--

<span style="color:orange">**1. Obtenir des réalisations selon la loi $f$**</span>

- *méthode 1* : simuler directement selon $f$

</br>
</br>
</br>
</br>

- *méthode 2* : utiliser une loi instrumentale $g$


---
# Résumé des épisodes précédents

.bold[Objectif :] estimer $\mu = \int h(x)f(x)dx$ avec $f$ densité de probabilité

**Méthode :** <span style="color:orange"> **obtenir des réalisations $X_1,\dots,X_n$ selon la loi $f$**</span> et <span style="color:blue">**poser $\hat{\mu} = \frac{1}{n}\sum_{i=1}^n h(X_i)$**</span>

<span style="color:orange">**1. Obtenir des réalisations selon la loi $f$**</span>

- *méthode 1* : simuler directement selon $f$
 - méthode de la fonction inverse
 - lien avec d'autres lois (e.g. loi conditionnelle, ...)
 - slice sampler
 - échantillonneur de Gibbs (en passant par les lois conditionnelles)

- *méthode 2* : utiliser une loi instrumentale $g$
 - acceptation-rejet
 - échantillonnage préférentiel avec ré-échantillonnage
 - algorithme de Metropolis-Hastings
 - Metropolis-within-Gibbs 

---
name: simu_f
# Résumé des épisodes précédents

.bold[Objectif :] estimer $\mu = \int h(x)f(x)dx$ avec $f$ densité de probabilité

**Méthode :** <span style="color:orange"> **obtenir des réalisations $X_1,\dots,X_n$ selon la loi $f$**</span> et <span style="color:blue">**poser $\hat{\mu} = \frac{1}{n}\sum_{i=1}^n h(X_i)$**</span>

<span style="color:orange">**1. Obtenir des réalisations selon la loi $f$**</span>

- *méthode 1* : simuler directement selon $f$ <span style="color:red">&rarr; $f$ **connue exactement**</span>
 - méthode de la fonction inverse <span style="color:#16A085">&rarr; **éch. i.i.d.** </span>
 - lien avec d'autres lois (e.g. loi conditionnelle, ...) <span style="color:#16A085">&rarr; **éch. i.i.d.** </span>
 - slice sampler <span style="color:#16A085">&rarr; **éch. MC** </span>
 - échantillonneur de Gibbs <span style="color:#16A085">&rarr; **éch. MC** </span>

- *méthode 2* : utiliser une loi instrumentale $g$ <span style="color:red">&rarr; $f$ **connue à une constante près** </span>
 - acceptation-rejet <span style="color:#16A085">&rarr; **éch. i.i.d.** </span>
 - échantillonnage préférentiel avec ré-échantillonnage <span style="color:#16A085">&rarr; **éch. i.i.d.** </span>
 - algorithme de Metropolis-Hastings <span style="color:#16A085">&rarr; **éch. MC** </span>
 - Metropolis-within-Gibbs <span style="color:#16A085">&rarr; **éch. MC** </span>


---
# Résumé des épisodes précédents

.bold[Objectif :] estimer $\mu = \int h(x)f(x)dx$ avec $f$ densité de probabilité

**Méthode :** <span style="color:orange"> **obtenir des réalisations $X_1,\dots,X_n$ selon la loi $f$**</span> et <span style="color:blue">**poser $\hat{\mu} = \frac{1}{n}\sum_{i=1}^n h(X_i)$**</span>

<span style="color:blue">**2. Poser $\hat{\mu} = \frac{1}{n}\sum_{i=1}^n h(X_i)$ </span> **

- quelle est la qualité de cette approximation ?
 - convergence
 - biais
 - **variance**
 - coût computationnel

- pour les échantillons de type chaîne de Markov :
 - convergence de la chaîne ?
 - exploration de l'espace ?
 - sensibilité aux conditions initiales ?

---
# Diagnostics de convergence des algos MCMC

- Les algorithmes MCMC produisent des chaînes de Markov **ergodiques** dont la loi stationnaire est la loi cible $f$

- Comment être sûr(e)/vérifier que la chaîne est bien dans son régime stationnaire ? 
 - **quand peut-on arrêter la chaîne ?**
 - **quelle est la précision obtenue?**

- Trois types d'outils diagnostics pour étudier :
 - la convergence de la chaîne vers le régime stationnaire
 - la convergence des moyennes
 - la convergence vers un échantillonnage i.i.d.

- Ce sont en général des .red[**conditions "nécessaires" mais pas suffisantes**]
&rarr; permet de détecter un problème, mais ne garantit pas que tout s'est bien passé !



---
# Convergence vers la loi stationnaire (1)

- En pratique (et même en théorie !) il est difficile de tester si la chaîne produit des réalisations selon $f$
--

- Pour "tester" la convergence vers le régime stationnaire on peut utiliser des .red[**outils graphiques**] :
 - analyser le **trace plot** (évolution de la chaîne au cours des itérations) pour détecter les fortes non-stationnarités
```{r, echo=FALSE, fig.height=5, fig.width=10, fig.retina=2, fig.align='center'}
X1 <- X2 <- X3 <- rep(0,5000);
for (i in 2:5000){
  # chaine 1
  Y <- rnorm(1,X1[i-1],0.1)
  alpha <- dnorm(Y)/dnorm(X1[i-1])
  X1[i] <- ifelse(runif(1)<alpha,Y,X1[i-1])
  # chaine 2
  Y <- rnorm(1,X2[i-1],2)
  alpha <- dnorm(Y)/dnorm(X2[i-1])
  X2[i] <- ifelse(runif(1)<alpha,Y,X2[i-1])
}
par(mfrow=c(1,2))
plot(X1,type="l",xlab="Itération")
plot(X2,type="l",xlab="Itération")
``` 


---
# Convergence vers la loi stationnaire (1)

- En pratique (et même en théorie !) il est difficile de tester si la chaîne produit des réalisations selon $f$
- Pour "tester" la convergence vers le régime stationnaire on peut utiliser des .red[**outils graphiques**] :
 - analyser le **trace plot** (évolution de la chaîne au cours des itérations) pour détecter les fortes non-stationnarités
 - lancer **plusieurs chaînes** (en parallèle) et comparer les résultats
```{r, echo=FALSE, fig.height=5, fig.width=10, fig.retina=2, fig.align='center'}
set.seed(202503110)
X1 <- X2 <- X11 <- X22 <- rep(0,5000);
X1[1] <- rnorm(1); X11[1] <- rnorm(1); X2[1] <- rnorm(1); X22[1] <- rnorm(1)
for (i in 2:5000){
  # chaine 1
  Y <- rnorm(1,X1[i-1],0.1)
  Y1 <- rnorm(1,X11[i-1],0.1)
  alpha1 <- dnorm(Y)/dnorm(X1[i-1])
  alpha11 <- dnorm(Y1)/dnorm(X11[i-1])
  X1[i] <- ifelse(runif(1)<alpha1,Y,X1[i-1])
  X11[i] <- ifelse(runif(1)<alpha11,Y1,X11[i-1])
  # chaine 2
  Y <- rnorm(1,X2[i-1],2)
  Y2 <- rnorm(1,X22[i-1],2)
  alpha <- dnorm(Y)/dnorm(X2[i-1])
  alpha2 <- dnorm(Y2)/dnorm(X22[i-1])
  X2[i] <- ifelse(runif(1)<alpha,Y,X2[i-1])
  X22[i] <- ifelse(runif(1)<alpha2,Y2,X22[i-1])
}
par(mfrow=c(1,2))
plot(X1,type="l",xlab="Itération")
points(X11,type="l",col="red")
plot(X2,type="l",xlab="Itération") 
points(X22,type="l",col="red")
```

---
# Convergence vers la loi stationnaire (1)

.red[**Attention à l'effet "we only see where we've been"**]
&rarr; exemple avec loi cible $0.7\mathcal{N}(1,1) + 0.2\mathcal{N}(6,1)$ et MH à loi instrumentale $\mathcal{N}(0,1)$.
```{r, echo=FALSE}
fcible <- function(x){0.7*dnorm(x,1,1)+0.3*dnorm(x,6,1)}
n <- 10000
current <- -2
for (i in 2:n){
  candidate <- rnorm(1)
  alpha <- min(1,(fcible(candidate)*dnorm(current[i-1],0,1))/
                 (fcible(current[i-1])*dnorm(candidate,0,1)))
  new <- ifelse(runif(1) < alpha, candidate, current[i-1])
  current <- c(current,new)
}
current2 <- 2.5
for (i in 2:n){
  candidate <- rnorm(1) # + current2[i-1]
  alpha <- min(1,(fcible(candidate)*dnorm(current2[i-1],0,1))/(fcible(current2[i-1])*dnorm(candidate,0,1)))
  new <- ifelse(runif(1) < alpha, candidate, current2[i-1])
  current2 <- c(current2,new)
}
current3 <- 7
for (i in 2:n){
  candidate <- rnorm(1) # + current2[i-1]
  alpha <- min(1,(fcible(candidate)*dnorm(current3[i-1],0,1))/(fcible(current3[i-1])*dnorm(candidate,0,1)))
  new <- ifelse(runif(1) < alpha, candidate, current3[i-1])
  current3 <- c(current3,new)
}
```
.pull-left[
```{r, echo=FALSE, fig.align='center',fig.height=3,fig.width=4.55,fig.retina=2}
data <- data.frame(mcmc=c(current,current2),chaine=rep(1:2,each=n),iteration=rep(1:n,2))
ggplot(data=data,aes(x=iteration,y=mcmc,col=as.factor(chaine))) + geom_line() + theme(legend.position = "n") + xlab("Itération") + ylab("Valeur de la chaîne")
```
]
.pull-right[
```{r, echo=FALSE, fig.align='center',fig.height=3,fig.width=4.5,fig.retina=2, message=FALSE, warning=FALSE}
ggplot(data=data,aes(x=mcmc,fill=as.factor(chaine))) + geom_histogram(aes(y=..density..),position="identity",alpha=0.5) +
  stat_function(fun=fcible,col="red") + theme(legend.position = "n") + xlab("Valeur de la chaîne") + ylab("") + xlim(c(-2.5,10))
```
]
.pull-left[
```{r, echo=FALSE, fig.align='center',fig.height=3,fig.width=4.55,fig.retina=2}
data <- data.frame(mcmc=c(current,current2,current3),chaine=rep(1:3,each=n),iteration=rep(1:n,3))
ggplot(data=data,aes(x=iteration,y=mcmc,col=as.factor(chaine))) + geom_line() + theme(legend.position = "n") + xlab("Itération") + ylab("Valeur de la chaîne")
```
]
.pull-right[
```{r, echo=FALSE, fig.align='center',fig.height=3,fig.width=4.5,fig.retina=2, message=FALSE, warning=FALSE}
ggplot(data=data,aes(x=mcmc,fill=as.factor(chaine))) + geom_histogram(aes(y=..density..),position="identity",alpha=0.5) +
  stat_function(fun=fcible,col="red") + theme(legend.position = "n") + xlab("Valeur de la chaîne") + ylab("") + xlim(c(-2.5,10))
```
]

---
# Convergence vers la loi stationnaire (2)

On peut aussi utiliser des .red[**tests de stationnarité**] :
- test de Kolmogorov-Smirnov (fonction `ks.test()` sous R et `scipy.stats.kstest()` sous Python)
 - attention aux hypothèses du test : .red[**échantillons i.i.d.**]
 
- test de Heidelberg et Welch (fonction `heidel.diag()` du package `coda` sous R)

- test de Geweke (fonction `geweke.diag()` dans `coda`)

- exploration du support de $f$ &rarr; si $f$ connue exactement et si $d$ petit
 - approximation par sommes de Riemann
 - estimation de la densité

---
class: my-one-page-font
# Convergence vers la loi stationnaire (3)

- En pratique, on définit aussi une période de chauffe (ou *burn-in*) correspondant aux premières itérations de l'algorithme
- Ces itérations sont **supprimées** et on ne garde que les itérations suivantes
```{r, echo=FALSE, fig.height=4, fig.width=7, fig.retina=2, fig.align='center'}
set.seed(202503110)
X1 <- rep(0,3000);
X1[1] <- 10
for (i in 2:3000){
  # chaine 2
  if (i < 500){
    Y <- rnorm(1,X1[i-1],0.2)
  }else{
    Y <- rnorm(1,X1[i-1],1)
  }
  #Y2 <- rnorm(1,X22[i-1],2)
  alpha <- dnorm(Y)/dnorm(X1[i-1])
  #alpha2 <- dnorm(Y2)/dnorm(X22[i-1])
  X1[i] <- ifelse(runif(1)<alpha,Y,X1[i-1])
  #X[i] <- ifelse(runif(1)<alpha2,Y2,X22[i-1])
}
d <- data.frame(X=X1,type=(seq(1,3000)<500),it=1:3000)
ggplot(data=d,aes(x=it,y=X,col=as.factor(type))) + geom_line() + theme(legend.position = "none") + scale_color_manual(values=c("black","darkgrey")) + xlab("Itérations")
X <- X1
```
```{r}
library(coda)
heidel.diag(X)
```

---
# Convergence des moyennes empiriques (1)

- Si la chaîne a atteint son régime stationnaire, elle doit vérifier le *théorème ergodique*

- Autrement dit, toutes les expressions de la forme $\frac{1}{n}\sum_{i=1}^n h(X_n)$ doivent converger vers $\mathbb{E}(h(X))$ avec $X \sim f$.
- En pratique, on vérifie la stabilisation de ces quantités :
  - en traçant l'évolution de la moyenne cumulée 
  - idem pour les quantiles (fonction `cumuplot()` du package `coda`)
  - variance inter- et intra (plusieurs chaînes) &rarr; fonction `gelman.diag()` du package `coda`

.pull-left[
```{r, echo=FALSE, fig.height=5, fig.width=8, fig.retina=2, fig.align='center'}
cumuplot(X,main="Pas de stabilisation",auto.layout = F)
```
]
.pull-right[
```{r, echo=FALSE, fig.height=5, fig.width=8, fig.retina=2, fig.align='center'}
cumuplot(X[500:3000],main="Stabilisation (sans le burn-in)",auto.layout = FALSE)
```
]

---
# Convergence vers un échantillonnage i.i.d.

On peut "mesurer" la proximité avec un échantillon i.i.d. :

- en calculant l'effective sampling size &rarr; fonction `effectiveSamplingSize()` du package `coda`

- en traçant l'autocorrélation de la chaîne et en élagant

```{r, fig.height=4, fig.width=8, fig.retina=2, fig.align='center'}
par(mfrow=c(1,2))
acf(X)
acf(X[500:2000])
```


---
# Estimation de la variance

Sous certaines hypothèses d'ergodicité de la chaîne de Markov, elle vérifie un TCL :

$$\sqrt{n} (\hat{\mu}_n - \mu) \longrightarrow \mathcal{N}(0,\sigma^2_{c})$$

- expression de $\sigma^2_c$
- estimation de $\sigma_c^2$
 - par batch means
 - par méthodes spectrales


---
name: c15
class: inverse, middle, center

# III. Introduction aux statistiques bayésiennes

---
# Le point de vue Bayésien

Combiner notre information <span style="color:#ffe933">**a priori**</span> avec les <span style="color:#2a6099">**observations**</span> pour mettre à jour notre connaissance <span style="color:#77bc65">**a posteriori**</span>

```{r, out.width='800px', echo=FALSE, fig.align='center'}
magick::image_read_pdf("/home/charlotte/Documents/Enseignement/Lille/M1 ISN /Statistiques computationnelles/Poly/bayesian.pdf")
```

---
# Le point de vue Bayésien

Mise à jour **séquentielle** de la connaissance

</br>

```{r, out.width='800px', echo=FALSE, fig.align='center'}
magick::image_read_pdf("/home/charlotte/Documents/Enseignement/Lille/M1 ISN /Statistiques computationnelles/Poly/bayesian_seq.pdf")
```


---
# Le point de vue Bayésien

- Mixer l'information a priori avec les observations

- Donner plus ou moins de poids à l'a priori en fonction du contexte

- Interprétation différente des résultats de l'inférence
 - en bayésien, les probabilités reflètent la *plausabilité* d'un évènement
 - en fréquentiste, les probabilités s'interprètent comme la *fréquence limite* d'un évènement répété

- Interprétation des tests d'hypothèses beaucoup plus ''naturelle''

---
# Le théorème de Bayes

Thomas Bayes (1702-1761) et indépendamment Pierre-Simon de Laplace (1749-1827)
</br>

```{r, out.width='400px', echo=FALSE, fig.align='center'}
par(mfrow=c(1,2))
bayes <- magick::image_read("/home/charlotte/Documents/Enseignement/Lille/M1 ISN /Statistiques computationnelles/Poly/Thomas_Bayes.gif")
laplace <- magick::image_read("/home/charlotte/Documents/Enseignement/Lille/M1 ISN /Statistiques computationnelles/Poly/Pierre-Simon_Laplace.jpg")
all <- c(bayes,laplace)
magick::image_append(magick::image_scale(all,geometry="x200"))
```

</br>

> **Théorème de Bayes**
>
> Pour deux évènements $A$ et $B$ :
$$\mathbb{P}(A|B) = \frac{\mathbb{P}(B|A) \ \mathbb{P}(A)}{\mathbb{P}(B)} $$

---
# Le théorème de Bayes

- Version continue du théorème pour $X$ et $Y$ deux v.a. :

$$g(y|x) = \frac{f(x|y)h(y)}{\int f(x|y)h(y) dy} $$

--

- Bayes et Laplace appliquent ce résultat à la modélisation statistique :
 - en représentant l'*incertitude* entourant $\theta$ par une *distribution* $\pi$

$$p(\theta | x) = \frac{f(x|\theta) \pi(\theta)}{\int f(x|\theta) \pi(\theta) d\theta} $$
 - le paramètre du modèle statistique est donc considéré comme **aléatoire**

---
# Modélisation Bayésienne

$\mathcal{X} = (X_1,\dots,X_n)$ i.i.d de loi $f$ qui dépend d'une quantité $\theta$

- .orange[**Modèle statistique fréquentiste**]
  - $\theta$ est un paramètre déterministe
  - $X_i \sim f(\cdot ; \theta)$ &rarr; $f(\cdot;\theta)$ est la loi **marginale** des observations $X_i$
  - L'inférence se fait à l'aide de la **vraisemblance**

--

- .green[**Modèle statistique bayésien**]
  - $\theta$ est une variable aléatoire
  - $X_i \mid \theta \sim f(\cdot | \theta)$ &rarr; $f(\cdot |\theta)$ est la loi **conditionnelle** des observations $X_i$ sachant $\theta$
  - On ajoute une loi sur $\theta$, appelée **loi a priori** : $\theta  \sim \pi$
  - L'inférence se fait à l'aide de la **vraisemblance** .red[**et de la loi a priori**], combinées dans la .green[**distribution a posteriori**]
  - Loi a posteriori : loi conditionnelle de $\theta$ sachant $\{\mathcal{X}=x\}$, notée $p(\theta \mid x)$.


---
# Exemple historique (Laplace, 1786)

**Question** : est-ce que la probabilité $\theta$ d'avoir un garçon est plus grande que celle d'avoir une fille ?

--

- <span style="color:#2a6099">**données**</span> : naissances à Paris entre 1745 et 1770 : 251 527 garçons et 241 945 filles

 &rarr; loi des observations ? $\dots$

---
# Exemple historique (Laplace, 1786)

**Question** : est-ce que la probabilité $\theta$ d'avoir un garçon est plus grande que celle d'avoir une fille ?

- <span style="color:#2a6099">**données**</span> : naissances à Paris entre 1745 et 1770 : 251 527 garçons et 241 945 filles

 &rarr; loi des observations ? $X_i \mid \theta \sim \mathcal{B}(\theta) \Leftrightarrow X \mid \mathcal{B}(n,\theta)$ où $X_i =1$ si garçon et $0$ sinon et $X = \sum_{i=1}^n X_i$.


--

- <span style="color:#ffe933">**loi a priori**</span> : $\dots$ 


---
# Exemple historique (Laplace, 1786)

**Question** : est-ce que la probabilité $\theta$ d'avoir un garçon est plus grande que celle d'avoir une fille ?

- <span style="color:#2a6099">**données**</span> : naissances à Paris entre 1745 et 1770 : 251 527 garçons et 241 945 filles

 &rarr; loi des observations ? $X_i \mid \theta \sim \mathcal{B}(\theta) \Leftrightarrow X \mid \mathcal{B}(n,\theta)$ où $X_i =1$ si garçon et $0$ sinon et $X = \sum_{i=1}^n X_i$.

- <span style="color:#ffe933">**loi a priori**</span> : la probabilité d'avoir un garçon peut valoir n'importe quoi entre 0 et 1 &rarr; $\theta \sim \mathcal{U}([0,1])$

--

- <span style="color:#77bc65">**loi a posteriori**</span> : $\dots$ 


---
# Exemple historique (Laplace, 1786)

**Question** : est-ce que la probabilité $\theta$ d'avoir un garçon est plus grande que celle d'avoir une fille ?

- <span style="color:#2a6099">**données**</span> : naissances à Paris entre 1745 et 1770 : 251 527 garçons et 241 945 filles

 &rarr; loi des observations ? $X_i \mid \theta \sim \mathcal{B}(\theta) \Leftrightarrow X \mid \mathcal{B}(n,\theta)$ où $X_i =1$ si garçon et $0$ sinon et $X = \sum_{i=1}^n X_i$.

- <span style="color:#ffe933">**loi a priori**</span> : la probabilité d'avoir un garçon peut valoir n'importe quoi entre 0 et 1 &rarr; $\theta \sim \mathcal{U}([0,1])$

- <span style="color:#77bc65">**loi a posteriori**</span> : $\theta \mid x \sim \text{Beta}(x+1,n-x+1)$ 

--

- On obtient par le calcul $\mathbb{P}(\theta \leq 1/2 | (251 527, 241 945)) = 1.15 \cdot 10^{-42}$

--
```{r}
xg <- 251527
xf <- 241945
pbeta(0.5,251527+1,241945+1)
```


---
# Le modèle Beta-binomial

- Exemple : sondage électoral .small[(inspiré de *Bayes Rule! An Introduction to Applied Bayesian Modeling*)]

- **Contexte** : on souhaite évaluer les chances d'une candidate A face à une autre à l'aide d'un sondage.

--

- On note $X_1,\dots,X_n$ l'intention de vote de chaque électeur et $\theta$ la probabilité pour qu'un électeur choisisse A.

--

- Modèle fréquentiste : $X_i \sim \mathcal{B}(\theta) \ \Leftrightarrow \ \sum_{i=1}^n X_i =: X \sim \mathcal{B}(n,\theta)$

--

- Modèle bayésien :
\begin{align}
X | \theta & \sim \mathcal{B}(n,\theta) \\
\theta & \sim \pi
\end{align}


---
# Le modèle Beta-binomial

- Modèle bayésien : $X | \theta \sim \mathcal{B}(n,\theta), \quad \theta \sim \pi$
 &rarr; comment choisir $\pi$ ?

--

- On a déjà réalisé plusieurs sondages :

</br>
</br>
```{r, echo=FALSE, fig.width=5, fig.height=4, fig.align='center', fig.retina=2, message=FALSE, warning=FALSE}
set.seed(0)
d <- rbeta(25,12,17)
ggplot(data=data.frame(d=d),aes(x=d)) + geom_histogram(fill="lightgrey",col="darkgrey") + xlim(c(0,1)) + xlab("Pourcentage de votants pour A") + ylab("")
```
--

- On aimerait choisir une densité de probabilité sur $[0,1]$ qui reflète ces informations &rarr; on peut prendre la loi $\text{Beta}(a,b)$

---
class: my-one-page-font
# Le modèle Beta-binomial

- La loi Beta a pour densité : $f(x;a,b) = \frac{\Gamma(a+b)}{\Gamma(a)\Gamma(b)} x^{a-1} (1-x)^{b-1} \mathbb{1}_{[0,1]}(x)$

- $X \sim \text{Beta}(a,b)$ : $\mathbb{E}(X) = \frac{a}{a+b}$, $\text{Var}(X) = \frac{ab}{(a+b)^2(a+b+1)}$

- Si $a>1,b>1$ on peut aussi paramétrer la loi Beta par son mode $m = \frac{a-1}{a+b-2}$ et sa concentration $s=a+b$

--

```{r, echo=FALSE, fig.width=8, fig.height=5, fig.align='center', fig.retina=2}
x<-seq(0,1,0.001)
b1 <- dbeta(x,0.8,3)
b2 <- dbeta(x,1,5)
b3 <- dbeta(x,2,5)
b31 <- dbeta(x,3,9)
b4 <- dbeta(x,0.5,0.5)
b5 <- dbeta(x,1,1)
b6 <- dbeta(x,5,5)
b66 <- dbeta(x,20,20)
b70 <- dbeta(x,12,4)
b7 <- dbeta(x,5,2)
b8 <- dbeta(x,5,1)
b9 <- dbeta(x,3,0.8)
data <- data.frame(x=rep(x,12),beta=c(b1,b2,b3,b31,b4,b5,b6,b66,b70,b7,b8,b9),dist=rep(c("Beta(0.8,3)","Beta(1,5)","Beta(2,5)","Beta(3,9)","Beta(0.5,0.5)","Beta(1,1)","Beta(5,5)","Beta(20,20)","Beta(12,4)","Beta(5,2)","Beta(2,1)","Beta(5,1)"),each=length(x)))
data$dist <- factor(data$dist,levels=c("Beta(0.8,3)","Beta(1,5)","Beta(2,5)","Beta(3,9)","Beta(0.5,0.5)","Beta(1,1)","Beta(5,5)","Beta(20,20)","Beta(12,4)","Beta(5,2)","Beta(2,1)","Beta(5,1)"))
ggplot(data=data,aes(x=x,y=beta,group=dist)) + geom_line() + facet_wrap(~dist) + scale_x_continuous(breaks=c(0,0.25,0.50,0.75,1)) + ylim(c(0,5.02))
```

---
# Le modèle Beta-binomial

- Que choisir ici ?
- Mode observé dans les sondages : 0.375 &rarr; $m=0.375$

--

```{r, echo=FALSE, fig.width=12, fig.height=4, fig.align='center', fig.retina=2, message=FALSE, warning=FALSE}
x<-seq(0,1,0.001)
m <- 0.375
s <- c(10,20,30,50)
bs <- lapply(s,FUN=function(i){dbeta(x,m*(i-2)+1,i-m*(i-2)-1)})
bs <- do.call("c",bs)
data <- data.frame(x=rep(x,4),beta=bs,dist=rep(paste0("s=",s),each=length(x)))
data$dist <- factor(data$dist,levels=paste0("s=",s))
dd <- data.frame(d=rep(d,4),dist=rep(paste0("s=",s),each=length(d)))
dd$dist <- factor(dd$dist,levels=paste0("s=",s))
ggplot(data=dd,aes(x=d,group=dist)) + geom_histogram(fill="lightgrey",col="darkgrey") + facet_wrap(~dist,nc=4) + geom_line(data=data,aes(x=x,y=beta))
#ggplot(data=data,aes(x=x,y=beta,group=dist)) + geom_line() + facet_wrap(~dist,nc=4) + scale_x_continuous(breaks=c(0,0.25,0.50,0.75,1))
```

---
# Le modèle Beta-binomial

- Loi a priori : $\pi(\theta)$ &rarr; loi $\text{Beta}(a,b)$ avec $a=$ `r m*(20-2)+1` et $b=$ `r 20-m*(20-2)-1` (hyperparamètres)

- Vraisemblance : on fait un nouveau sondage sur $n=50$ personnes &rarr; $X | \theta \sim \mathcal{B}(50,\theta)$ et on observe $X(\omega) = 20$.

</br>

```{r , echo=FALSE, fig.width=5, fig.height=4, fig.align='center', fig.retina=2}
#theta <- seq(1,9,1)/10
#lik <- lapply(theta, FUN=function(p){dbinom(seq(0,50),50,p)})
#dlik <- do.call("c",lik)
#obs <- data.frame(x=rep(20,length(theta)),lik=dbinom(20,50,theta),dist=paste0("B(50,",theta,")"))
#ddlik <- data.frame(x=rep(seq(0,50,1),length(theta)),lik=dlik,dist=rep(paste0("B(50,",theta,")"),each=51))
#ggplot(data=ddlik,aes(x=x)) + geom_point(aes(y=lik),col="darkgrey",size=0.1) + facet_wrap(~dist) + geom_linerange(aes(ymax=lik,ymin=0),col="darkgrey") + geom_point(data=obs,aes(x=x,y=lik)) + geom_linerange(data=obs,aes(ymax=lik,ymin=0))

theta <- seq(0,1,0.001)
lik <- dbinom(20,50,theta)
ggplot(data=NULL,aes(x=theta,y=lik)) + geom_line()
```

---
# Le modèle Beta-binomial

Loi a posteriori $\text{Beta}(20+7.75,30+12.25)$

```{r, echo=FALSE, fig.width=7, fig.height=5, fig.align='center', fig.retina=2, message=FALSE}
cst <- integrate(function(x){dbinom(20,50,x)},lower=0,upper=1)
data_all <- data.frame(x=rep(seq(0,1,0.001),3),y=c(dbeta(seq(0,1,0.001),7.75,12.25),dbinom(20,50,x)/cst$value,dbeta(x,20+7.75,30+12.25)),dist=rep(c("Loi a priori","Vraisemblance (normalisée)","Loi a posteriori"),each=1001))
ggplot(data=data_all,aes(x=x,y=y,col=dist)) + geom_line(lwd=2) + scale_color_discrete("") + scale_color_manual("",values=c("#77bc65","#ffd700","#2a6099"))
```
<span style="color:red">**La vraisemblance est renormalisée de telle sorte à ce que son intégrale soit égale à 1, pour pouvoir la comparer plus facilement aux densités a priori et a posteriori qui sont des densités de probabilités pour $\theta$, contrairement à la vraisemblance.**</span>

---
# Le modèle Beta-binomial

- Que se passe t-il si on change de loi a priori ?

--

```{r, echo=FALSE, fig.width=12, fig.height=7, fig.align='center', fig.retina=2}
cst <- integrate(function(x){dbinom(20,50,x)},lower=0,upper=1)
priors <- c(dbeta(x,1,1),dbeta(x,1,5),dbeta(x,3,10),
            dbeta(x,10,10),dbeta(x,10,3),dbeta(x,5,1))
posteriors <- c(dbeta(x,20+1,30+1),dbeta(x,20+1,30+5),dbeta(x,20+3,30+10),
            dbeta(x,20+10,30+10),dbeta(x,20+10,30+3),dbeta(x,20+5,30+1))
data_all <- data.frame(x=rep(x,18),y=c(priors,rep(dbinom(20,50,x)/cst$value,6),posteriors),dist=rep(c("Loi a priori","Vraisemblance (normalisée)","Loi a posteriori"),each=6006),prior=rep(rep(c("Beta(1,1)","Beta(1,5)","Beta(3,10)","Beta(10,10)","Beta(10,3)","Beta(5,1)"),each=1001),3))
ggplot(data=data_all,aes(x=x,y=y,col=dist)) + geom_line(lwd=1) + facet_wrap(~prior) + scale_color_manual("",values=c("#77bc65","#ffd700","#2a6099")) + theme(legend.position = "bottom")
```

---
# Le modèle Beta-binomial

- Que se passe t-il si la taille de l'échantillon augmente ? ou diminue ?

```{r, echo=FALSE, fig.width=11, fig.height=6.5, fig.align='center', fig.retina=2}
nobs <- 200
xobs <- 0.4*nobs
cst2 <- integrate(function(x){dbinom(xobs,nobs,x)},lower=0,upper=1)
posteriors2 <- c(dbeta(x,xobs+1,nobs-xobs+1),dbeta(x,xobs+1,nobs-xobs+5),dbeta(x,xobs+3,nobs-xobs+10),
            dbeta(x,xobs+10,nobs-xobs+10),dbeta(x,xobs+10,nobs-xobs+3),dbeta(x,xobs+5,nobs-xobs+1))
data_all2 <- data.frame(x=rep(x,18),y=c(priors,rep(dbinom(xobs,nobs,x)/cst2$value,6),posteriors2),dist=rep(c("Loi a priori","Vraisemblance (normalisée)","Loi a posteriori"),each=6006),prior=rep(rep(c("Beta(1,1)","Beta(1,5)","Beta(3,10)","Beta(10,10)","Beta(10,3)","Beta(5,1)"),each=1001),3),n="n=200")
data_all$n <- "n=50"
nobs <- 10
xobs <- 0.4*nobs
cst3 <- integrate(function(x){dbinom(xobs,nobs,x)},lower=0,upper=1)
posteriors3 <- c(dbeta(x,xobs+1,nobs-xobs+1),dbeta(x,xobs+1,nobs-xobs+5),dbeta(x,xobs+3,nobs-xobs+10),
            dbeta(x,xobs+10,nobs-xobs+10),dbeta(x,xobs+10,nobs-xobs+3),dbeta(x,xobs+5,nobs-xobs+1))
data_all3 <- data.frame(x=rep(x,18),y=c(priors,rep(dbinom(xobs,nobs,x)/cst3$value,6),posteriors3),dist=rep(c("Loi a priori","Vraisemblance (normalisée)","Loi a posteriori"),each=6006),prior=rep(rep(c("Beta(1,1)","Beta(1,5)","Beta(3,10)","Beta(10,10)","Beta(10,3)","Beta(5,1)"),each=1001),3),n="n=10")
data_all123 <- rbind(data_all,data_all2,data_all3)
data_all123$n <- factor(data_all123$n,levels=c("n=10","n=50","n=200"))
ggplot(data=data_all123,aes(x=x,y=y,col=dist)) + geom_line(lwd=1) + facet_grid(n~prior) + scale_color_manual("",values=c("#77bc65","#ffd700","#2a6099")) + theme(legend.position = "bottom")
```

<span style="color:#16A085">**fin du cours 15 (11/03/2025)**</span>
