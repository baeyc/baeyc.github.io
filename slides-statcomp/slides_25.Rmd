---
title: "Statistiques computationnelles"
author: <font size="5"> Charlotte Baey </font>
date: <font size="5"> M1 MA - 2025/2026 </font>
output:
  xaringan::moon_reader:
    css: [default, metropolis, metropolis-fonts, hygge]
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      slideNumberFormat: "%current%"
---

<style>

.remark-slide-content {
  background-color: #FFFFFF;
  border-top: 80px solid #16A085;
  font-size: 20px;
  line-height: 1.5;
  padding: 1em 2em 1em 2em
}

.my-one-page-font {
  font-size: 20px;
}

.remark-slide-content > h1 {
  font-size: 38px;
  margin-top: -85px;
}

.inverse {
  background-color: #16A085;
  border-top: 80px solid #16A085;
  text-shadow: none;
	background-position: 50% 75%;
  background-size: 150px;
  font-size: 40px
}

.title-slide {
  background-color: #16A085;
  border-top: 80px solid #16A085;
  background-image: none;
}

.remark-slide-number {
  position: absolute;
}

.remark-slide-number .progress-bar-container {
  position: absolute;
  bottom: 0;
  height: 4px;
  display: block;
  left: 0;
  right: 0;
}

.remark-slide-number .progress-bar {
  height: 100%;
  background-color: grey;
}

.left-column {
  width: 20%;
  height: 92%;
  float: left;
}
.left-column h2:last-of-type, .left-column h3:last-child {
  color: #000;
}
.right-column {
  width: 75%;
  float: right;
  padding-top: 1em;
}


.left-column2 {
  width: 60%;
  height: 92%;
  float: left;
}
.right-column2 {
  width: 35%;
  height: 92%;
  float: left;
}

</style>

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: {
    Macros: {
      yellow: ["{\\color{yellow}{#1}}", 1],
      orange: ["{\\color{orange}{#1}}", 1],
      green: ["{\\color{green}{#1}}", 1]
    },
    loader: {load: ['[tex]/color']},
    tex: {packages: {'[+]': ['color']}}
  }
});
</script>

```{css, echo=FALSE}
.left-code {
  width: 40%;
  height: 92%;
  float: left;
}
.right-plot {
  width: 59%;
  float: right;
  padding-left: 1%;
}
```

```{css, echo=FALSE}
.left-plot {
  width: 59%;
  float: left;
}
.right-code {
  width: 40%;
  float: right;
  padding-left: 1%;
}
```


```{r setup, include = FALSE}

# R markdown options
#knitr::opts_chunk$set(echo = TRUE, 
                      #cache = TRUE, 
                      #fig.width = 10,
                      #fig.height = 5,
                      #fig.align = "center", 
                      #message = FALSE)

# Load packages
library(gapminder)
library(gganimate)
library(ggplot2)
library(tidyverse)
library(xaringancolor)
library(flair)
yellow <- "#FFCC29"
orange <- "#F58634"
green <- "#007965"
cprior <- "#ffe933"
cobs <- "#2a6099"
cpost <- "#77bc65"
```

```{r, echo=FALSE}
library(reticulate)
Sys.setenv('RETICULATE_PYTHON'='/usr/bin/python3')
```

# Quelques informations pratiques

### Plan du cours
1. Méthodes de ré-échantillonnage
2. Méthodes de Monte-Carlo
3. Introduction aux statistiques bayésiennes
4. Algorithme EM (s'il reste du temps)

### Organisation

- 2 séances de cours d'1h30 par semaine ($\times$ 11 semaines)
- 2 séances de TD/TP de 2h par semaine ($\times$ 12 semaines)

### Evaluation

- 1 DS intermédiaire d'une durée de 2h
- 1 Projet **à effectuer en binôme**
- 1 DS final d'une durée de 3h

.red[**Aucun document autorisé lors des examens.**]

---
# Sommaire

<!-- .pull-left[ -->
**1. Méthodes de ré-échantillonnage**
  - [Cours 1](#c1) (12/01/2026)
  - [Cours 2](#c2) (12/01/2026)
  - [Cours 3](#c3) (19/01/2026)
  - [Cours 4](#c4) (20/01/2026)

---
name: c1
class: inverse, middle, center

# Introduction


---
class: my-one-page-font 
# C'est quoi les statistiques computationnelles ?

<br>

<br>

 - C'est le recours (plus ou moins) intensif à l'ordinateur pour répondre à des questions statistiques que l'on ne sait pas (ou difficilement) résoudre autrement.
 
 - On utilise/développe/étudie des algorithmes, des astuces numériques/statistiques/computationnelles 
 - L'objectif est de faire de l'inférence, d'étudier la robustesse de méthodes statistiques, de traiter de grands jeux de données, ...
 
 

---

class: inverse, middle, center

# I. Méthodes de ré-échantillonnage

---

# Notion d'échantillon

Qu'est-ce qu'un échantillon ?

- une suite de variables aléatoires $\mathcal{X} = (X_1, \dots, X_n)$
- dont on observe une réalisation $\mathcal{X}(\omega) = (X_1(\omega), \dots, X_n(\omega))$

---
# Notion d'échantillon

Qu'est-ce qu'un échantillon ?

- une suite de variables aléatoires $\mathcal{X} = (X_1, \dots, X_n)$
- dont on observe une réalisation $\mathcal{X}(\omega) = (X_1(\omega), \dots, X_n(\omega))$

Que représente $\omega$ ?
- l'aléa autour de l'expérience (ex. : $n$ lancers d'une pièce de monnaie)
- cet aléa $\omega \in \Omega$ est transporté dans $\mathbb{R}$ via $X_i$
<!-- - on a seulement accès à la variabilité sur $\mathbb{R}$ -->
- en général, on ne dispose que d'une seule réalisation, pour un $\omega$ donné

---
# Statistique paramétrique

```{r, out.width='2000px', echo = FALSE}
magick::image_read_pdf("/home/charlotte/Documents/Enseignement/Lille/M1 ISN /Statistiques computationnelles/Poly/slides.pdf",pages=34)
```

---
# Statistique paramétrique

```{r, out.width='2000px', echo = FALSE}
magick::image_read_pdf("/home/charlotte/Documents/Enseignement/Lille/M1 ISN /Statistiques computationnelles/Poly/slides.pdf",pages=35)
```

---
# Statistique paramétrique

```{r, out.width='2000px', echo = FALSE}
magick::image_read_pdf("/home/charlotte/Documents/Enseignement/Lille/M1 ISN /Statistiques computationnelles/Poly/slides.pdf",pages=36)
```

---
# Statistique paramétrique

```{r, out.width='2000px', echo = FALSE}
magick::image_read_pdf("/home/charlotte/Documents/Enseignement/Lille/M1 ISN /Statistiques computationnelles/Poly/slides.pdf",pages=37)
```

---
# Statistique paramétrique

```{r, out.width='2000px', echo = FALSE}
magick::image_read_pdf("/home/charlotte/Documents/Enseignement/Lille/M1 ISN /Statistiques computationnelles/Poly/slides.pdf",pages=38)
```

---
# Statistique paramétrique

```{r, out.width='2000px', echo = FALSE}
magick::image_read_pdf("/home/charlotte/Documents/Enseignement/Lille/M1 ISN /Statistiques computationnelles/Poly/slides.pdf",pages=39)
```


---
# Statistique paramétrique

```{r, out.width='2000px', echo = FALSE}
magick::image_read_pdf("/home/charlotte/Documents/Enseignement/Lille/M1 ISN /Statistiques computationnelles/Poly/slides.pdf",pages=40)
```

---
# Statistique paramétrique

```{r, out.width='2000px', echo = FALSE}
magick::image_read_pdf("/home/charlotte/Documents/Enseignement/Lille/M1 ISN /Statistiques computationnelles/Poly/slides.pdf",pages=41)
```

---
# Statistique paramétrique

```{r, out.width='2000px', echo = FALSE}
magick::image_read_pdf("/home/charlotte/Documents/Enseignement/Lille/M1 ISN /Statistiques computationnelles/Poly/slides.pdf",pages=42)
```

---
# Statistique paramétrique

```{r, out.width='2000px', echo = FALSE}
magick::image_read_pdf("/home/charlotte/Documents/Enseignement/Lille/M1 ISN /Statistiques computationnelles/Poly/slides.pdf",pages=43)
```


---
# Un exemple simple 

Soit $(X_1,\dots,X_n)$ un échantillon gaussien i.i.d. de loi $\mathcal{N}(\theta,1)$, et $\hat{\theta}$ l'EMV de $\theta$. Quelle est la loi de $\hat{\theta}$ ?

--

.pull-left[
```{r, eval=F}
theta_vrai <- 2; n <- 100; N <- 500
hat_theta <- rep(0,N)
for (i in 1:N){
  ech_i <- rnorm(n,theta_vrai,1)
  hat_theta[i] <- mean(ech_i)
}
hist(hat_theta,freq=F)
curve(dnorm(x,theta_vrai,1/sqrt(n)))
```
]
.pull-right[
```{r, fig.height=5,fig.width=6,fig.retina=2,results='hold',echo=F}
theta_vrai <- 2; n <- 100; N <- 500
hat_theta <- rep(0,N)
for (i in 1:N){
  ech_i <- rnorm(n,theta_vrai,1)
  hat_theta[i] <- mean(ech_i)
}
hist(hat_theta,freq=F,main="Histogramme des estimations",ylim=c(0,4))
curve(dnorm(x,theta_vrai,1/sqrt(n)),
      lwd=2,col="red",add=T)
```
]
---
class: my-one-page-font 
# Jackknife

Comment construire de nouveaux échantillons ?

```{r, fig.height=6,fig.width=10,fig.retina=2, echo=F}
library(RColorBrewer)
pal <- brewer.pal(n = 8, name = "Dark2")
n <- 8
set.seed(2)
ech <- sort(rnorm(n,0,1))
ech_j <- matrix(0,nrow=n,ncol=7)
for (i in 1:n){
  ech_j[i,] <- ech[-i]
}

mean_ech <- mean(ech)
mean_j <- apply(ech_j,1,mean)
par(oma=c(0,4,0,0))
plot(ech,rep(1.9,n),col=pal,pch=19,cex=4,ylim=c(0.1,2),yaxt="none",xlab="Observations",ylab="")
points(mean_ech,1.9,col="red",pch=15,cex=2)
mtext("éch. initial",side=2,at=1.9,las=1,adj=1.1)
```

---
class: my-one-page-font 
# Jackknife

Comment construire de nouveaux échantillons ?

```{r, fig.height=6,fig.width=10,fig.retina=2, echo=F}
par(oma=c(0,4,0,0))
plot(ech,rep(1.9,n),col=pal,pch=19,cex=4,ylim=c(0.1,2),yaxt="none",xlab="Observations",ylab="")
points(mean_ech,1.9,col="red",pch=15,cex=2)
mtext("éch. initial",side=2,at=1.9,las=1,adj=1.1)
for (i in 1:n){
  abline(h=1.8-0.2*i,col="grey")
  points(ech_j[i,],rep(1.8-0.2*i,n-1),col=pal[-i],cex=3,pch=19)
  points(mean_j[i],1.8-0.2*i,col="red",pch=15,cex=2)
  mtext(paste("éch. jackknife",i),side=2,at=1.8-0.2*i,las=1,adj=1.1)
}
```


---
# Jackknife 

#### 1. Réduction du biais

 - Estimation du biais : 

$$\hat{b}_{jack} = (n-1)(\hat{\theta}_{jack} - \hat{\theta}),$$ avec $\hat{\theta}_{jack} = \frac{1}{n}\sum_{i=1}^n \hat{\theta}_{(i)}$.

 - Pseudo-valeurs :

$$\tilde{\theta}_{(i)} = n\hat{\theta} - (n-1)\hat{\theta}_{(i)}$$
<!-- Sur l'exemple précédent, cela donne : -->
 <!-- \tilde{\theta}_{(1)} = \tilde{\theta}_{(2)} = \tilde{\theta}_{(3)} = \tilde{\theta}_{(7)} = \tilde{\theta}_{(8)} = \tilde{\theta}_{(10)} = \tilde{\theta}_{(12)} = 13\times0.5384 - 12\times0.50 = 0.9992
 \tilde{\theta}_{(4)} = \tilde{\theta}_{(5)} = \tilde{\theta}_{(6)} = \tilde{\theta}_{(9)} = \tilde{\theta}_{(11)} = \tilde{\theta}_{(13)} = 13\times0.5384 - 12\times0.5833 = -0.0004 -->


 - Estimateur jackknife corrigé du biais :

$$\hat{\theta}^*_{jack} = \hat{\theta} - \hat{b}_{jack} = \frac{1}{n}\sum_{i=1}^n \tilde{\theta}_{(i)}$$
.red[**Réduire le biais n'implique pas nécessairement une amélioration de l'estimateur (au sens du risque quadratique)**]


---
# Jackknife

#### 2. Estimation de la variance

 $$\hat{s}^2_{jack} = \frac{n-1}{n} \sum_{i=1}^n \big(\hat{\theta}_{(i)} - \hat{\theta}^*_{jack}\big)^2$$
 
avec les pseudo-valeurs : $\hat{s}^2_{jack} = \frac{1}{n(n-1)} \sum_{i=1}^n \big(\tilde{\theta}_{(i)} - \tilde{\theta}\big)^2$

--

#### 3. Construction d'intervalles de confiance

Si existence d'un TCL :
- en utilisant l'estimateur jackknife de la variance : 
 $$\hat{I}_{jack} = \left[\hat{\theta} - q^{\mathcal{N(0,1)}}_{1-\alpha/2} \hat{s}_{jack} ; \hat{\theta} + q^{\mathcal{N(0,1)}}_{1-\alpha/2} \hat{s}_{jack} \right]$$

- en utilisant les deux estimateurs :
  $$\hat{I}_{jack} = \left[\hat{\theta}_{jack} - q^{\mathcal{N(0,1)}}_{1-\alpha/2} \hat{s}_{jack} ; \hat{\theta}_{jack} + q^{\mathcal{N(0,1)}}_{1-\alpha/2} \hat{s}_{jack} \right]$$


---
name: c2
# Résumé et remarques

<br>
- le jackknife est une méthode **non-paramétrique** permettant d'estimer le biais et la variance d'un estimateur à l'aide de simulations 

- la consistance est garantie pour un grand nombre d'estimateurs **suffisamment réguliers**

- les hypothèses de régularité sont toutefois plus strictes que pour un TCL par exemple :
 - la delta-méthode requiert la différentiabilité de $g$ en $\mu=\mathbb{E}(X_1)$
 - la consistance de $\hat{s}_{jack}$ requiert que $g'$ soit continue en $\mu$
 - ex. d'estimateur pour lequel $\hat{s}_{jack}$ n'est pas consistant : la médiane empirique (malgré existence TCL)
 
- extensions possibles reposant sur des hypothèses moins fortes : 
 - le *delete-d* jackknife
 - le jackknife infinitésimal
 <!-- - peut-être utilisé pour la médiane empirique -->
 <!-- - peut permettre d'estimer la distribution de $\hat{\theta}$ -->


<span style="color:#16A085">**fin du cours 1**</span>-

---
# Du jackknife au bootstrap

 - $\hat{\theta} = T(\underbrace{X_1,\dots,X_n}_{\text{observations}},\underbrace{\omega_1,\dots,\omega_n}_{\text{poids des obs.}}) = T(X_1,\dots,X_n,\frac{1}{n},\dots,\frac{1}{n})$
 
 - réplication jackknife : $\hat{\theta}_{(i)} = T(X_1,\dots,X_{i-1},X_i,X_{i+1},X_n,\frac{1}{n-1},\dots,\frac{1}{n-1},0,\frac{1}{n-1},\dots,\frac{1}{n-1})$

--

 - Idée du bootstrap : mettre des poids **aléatoires**

--

 - Procédure de ré-échantillonnage : tirer uniformément et **avec remise** parmi les observations de $\mathcal{X}$, pour construire un échantillon *bootstrap* de taille $n$ noté $\mathcal{X}^*$ :
 
 - Puis sur chaque échantillon bootstrap, on construit la statistique bootstrapée $\hat{\theta}^* = T(\mathcal{X}^*)$

---
# Bootstrap

```{r, out.width='2000px', echo = FALSE}
magick::image_read_pdf("/home/charlotte/Documents/Enseignement/Lille/M1 ISN /Statistiques computationnelles/Poly/slides.pdf",pages=25)
```

---
# Bootstrap

```{r, out.width='2000px', echo = FALSE}
magick::image_read_pdf("/home/charlotte/Documents/Enseignement/Lille/M1 ISN /Statistiques computationnelles/Poly/slides.pdf",pages=26)
```

---
# Bootstrap

```{r, out.width='2000px', echo = FALSE}
magick::image_read_pdf("/home/charlotte/Documents/Enseignement/Lille/M1 ISN /Statistiques computationnelles/Poly/slides.pdf",pages=27)
```


---
# Bootstrap

```{r, out.width='2000px', echo = FALSE}
magick::image_read_pdf("/home/charlotte/Documents/Enseignement/Lille/M1 ISN /Statistiques computationnelles/Poly/slides.pdf",pages=28)
```

---
# Bootstrap

```{r, out.width='2000px', echo = FALSE}
magick::image_read_pdf("/home/charlotte/Documents/Enseignement/Lille/M1 ISN /Statistiques computationnelles/Poly/slides.pdf",pages=29)
```

---
# Bootstrap

```{r, out.width='2000px', echo = FALSE}
magick::image_read_pdf("/home/charlotte/Documents/Enseignement/Lille/M1 ISN /Statistiques computationnelles/Poly/slides.pdf",pages=30)
```

---
# Bootstrap

```{r, out.width='2000px', echo = FALSE}
magick::image_read_pdf("/home/charlotte/Documents/Enseignement/Lille/M1 ISN /Statistiques computationnelles/Poly/slides.pdf",pages=31)
```

---
# Bootstrap

```{r, out.width='2000px', echo = FALSE}
magick::image_read_pdf("/home/charlotte/Documents/Enseignement/Lille/M1 ISN /Statistiques computationnelles/Poly/Figures/boot_samples.pdf")
```


---
# Bootstrap

.pull-left[
### Monde réel
.left[
<span style="color:white">**on raisonne conditionnellement à $F_n$**</span>
- échantillon $\mathcal{X} = (X_1,\dots,X_n)$
- $X_i$ de loi inconnue $F$
- paramètre $\theta(F)$
- estimateur $\hat{\theta} = T(\mathcal{X})$
- loi de $\hat{\theta}$ : $G$ inconnue
]
]
.pull-right[
### Monde Bootstrap 
.left[
<span style="color:red">**on raisonne conditionnellement à $F_n$**</span>
- échantillon bootstrap $\mathcal{X}^* = (X_{1}^*,\dots,X_{n}^*)$
- $X_{i}^*$ de loi connue $F_n$
- paramètre $\theta(F_n) = \hat{\theta}$
- statistique bootstrapée $\hat{\theta}^* = T(\mathcal{X}^*)$
- loi de $\hat{\theta}^*$ : $G^*$ connue
]
]

</br>
.center[
$G$ inconnue $\longrightarrow$ $G^*$ connue $\longrightarrow$ $\hat{G}^*_B$ approximation bootstrap
]

</br>
<span style="color:#16A085">**fin du cours 2**</span>

---
name: c3
# Fonction de répartition empirique

 
```{r, echo=FALSE}
set.seed(10101)
xi <- rnorm(10)
print(round(xi,2))
```

--

```{r, echo=FALSE, warning=FALSE, fig.height=7, fig.width=10,fig.retina=2}
plot(ecdf(xi),xaxt='n',main="",col="white")
mtext(round(xi,2),at=xi,side=1,line=1,cex=0.8)
mtext(paste0("X(",1:10,")"),at=xi,side=1,line=2,cex=0.75)
axis(1,at=sort(xi),labels=rep("",10),cex.axis=0.75)
```

---
# Fonction de répartition empirique

 
```{r, echo=FALSE}
set.seed(10101)
xi <- rnorm(10)
print(round(xi,2))
```


```{r, echo=FALSE, warning=FALSE, fig.height=7, fig.width=10,fig.retina=2}
plot(ecdf(xi),xaxt='n',main="")
mtext(round(xi,2),at=xi,side=1,line=1,cex=0.8)
mtext(paste0("X(",1:10,")"),at=xi,side=1,line=2,cex=0.75)
axis(1,at=sort(xi),labels=rep("",10),cex.axis=0.75)
```

---

# Fonction de répartition empirique

 - Echantillon exponentiel
 
```{r, echo=FALSE, warning=FALSE, fig.height=4, fig.width=16,fig.retina=2}
n <- c(10,20,50,100,200,500)
d <- lapply(n,FUN = function(i){return(data.frame(x=rexp(i*100,1),rep=rep(1:100,each=i),n=rep(i,i*100)))})
d <- do.call(rbind,d)

d %>% ggplot(aes(x,group=rep)) + stat_ecdf(geom="step",col="darkgrey") + stat_function(fun=pexp) + facet_wrap(~n,nrow = 1) 
```

 - Echantillon uniforme
 
```{r, echo=FALSE, warning=FALSE, fig.width=16, fig.height=4,fig.retina=2}
d <- lapply(n,FUN = function(i){return(data.frame(x=runif(i*100),rep=rep(1:100,each=i),n=rep(i,i*100)))})
d <- do.call(rbind,d)

d %>% filter(n<1000) %>% ggplot(aes(x,group=rep)) + stat_ecdf(geom="step",col="darkgrey") + stat_function(fun=punif) + facet_wrap(~n,nrow=1) + xlim(c(-0.2,1.2))
```
<span style="color:#16A085">**fin du cours 3**</span>


---
name: c4
# Bootstrap : principe du plug-in

 - on remplace $F$ par sa version empirique $F_n$ 


```{r, echo=FALSE, fig.width=12, fig.height=6,fig.retina=2}
set.seed(1999)
x <- rnorm(15)
par(mfrow=c(1,2))
curve(pnorm(x),from=-2.5,to=2.5,xlab="",ylab="",main=expression(Loi~des~X[i]))
plot(ecdf(x),xaxt='n',xlim=c(-2.5,2.5),main=expression(Loi~des~X[bi]^"*"))
text(sort(x),y = par("usr")[3]-0.075,labels=paste0("X(",1:15,")"),xpd=NA,cex=0.75,srt=45)
axis(1,at=sort(x),labels=rep("",15),cex.axis=0.75)
lines(seq(-3,3,0.1),pnorm(seq(-3,3,0.1)),xlab="",ylab="",col="grey")
```
<!-- .pull-left[ -->
<!-- .center[ -->
<!-- espérance : $\theta = \int x dF(x)$ ]]-->
<!-- .pull-right[ -->
<!-- espérance : $\int x dF_n(x) = \frac{1}{n} \sum_{i=1}^n X_i$] -->

---
# Bootstrap : principe du plug-in

#### Exemple  
$\theta = \mathbb{E}(X_1) = \int x dF(x)$ 

--

Méthode du plug-in : on estime $\theta$ par $\hat{\theta} = \int x dF_n(x) = \frac{1}{n}\sum_{i=1}^n X_i$.

--

Dans l'échantillon bootstrap, les $X_{b,i}^*$ sont distribuées selon la loi $F_n$, et ont pour espérance

$\theta^* = \int x dF_n(x) = \hat{\theta}$

--

La statistique bootstrapée $\hat{\theta}^*_b = \frac{1}{n}\sum_{i=1}^n X_{b,i}^*$ est donc à $\theta^*$ ce que $\hat{\theta}$ est à $\theta$.

---
# Bootstrap 

#### Exemple 2 : estimation du biais de $\hat{\theta}$

Paramètre $\theta(F)$ estimé par $\hat{\theta} = \theta(F_n)$

--

Biais : $b(\hat{\theta}) = \mathbb{E}_F(\hat{\theta}) - \theta = \int xdG(x) - \theta(F)$

--

Dans le monde bootstrap : 

$$b^*(\hat{\theta}) = \int x dG^*(x) - \theta(F_n) = \int x dG^*(x) - \hat{\theta}$$

--

Estimateur du bootstrap :

$$\hat{b}^*(\hat{\theta}) = \int x d\hat{G}^*_B(x) - \hat{\theta} = \frac{1}{B} \sum_{i=1}^B \hat{\theta}^*_b - \hat{\theta}$$

---
# Bootstrap

#### Exemple 3 : estimation de la variance de $\hat{\theta}$

Variance : $\text{Var} (\hat{\theta}) = \mathbb{E}_F\big[(\hat{\theta} - \mathbb{E}(\hat{\theta}))^2\big] = \int (x - \int x dG(x))^2dG(x)$

--

Dans le monde bootstrap :

$$\text{Var}^*(\hat{\theta}) = \int (x - \int x dG^*(x))^2 dG^*(x)$$

--

Estimateur du bootstrap :

$$\hat{\text{Var}}^*(\hat{\theta}) = \int (x - \int x d\hat{G}_B^*(x))^2 d\hat{G}_B^*(x) = \frac{1}{B} \sum_{b=1}^B \big(\hat{\theta}^*_b - \frac{1}{B} \sum_{b=1}^B \hat{\theta}^*_b \big)^2$$

$\rightarrow$ c'est la variance empirique des statistiques bootstrapées $\hat{\theta}^*_1,\dots,\hat{\theta}^*_B$.

</br>

---
# Construction d'intervalles de confiance

#### Méthode du bootstrap classique
Point de départ : l'existence d'une quantité pivotale, par exemple $\hat{\theta} - \theta$, de loi $H$.

--
 
```{r echo=FALSE, fig.width=5,fig.height=5,warning=FALSE,fig.retina=2} 
library(ggplot2)
x <- seq(0,13,0.01)
y <- dgamma(x,2.5,1)
d <- data.frame(x=rep(x,2),y=rep(y,2),type=rep(c("HDI","Quantiles"),each=length(x)),
                xmin=c(qgamma(0.025,2.5,1)),xmax=c(qgamma(0.975,2.5,1)))
ggplot(data=d,aes(x,y)) + geom_line() + stat_function(fun = dgamma,args = list(shape=2.5, rate=1),geom = "area",fill = "red", alpha = 0.5, xlim = c(0,qgamma(0.025,2.5,1))) + stat_function(fun = dgamma,args = list(shape=2.5, rate=1),geom = "area",fill = "red", alpha = 0.5, xlim = c(qgamma(0.975,2.5,1),10)) + ggtitle(expression(paste("Loi (densité) de ",hat(theta)-theta))) + xlim(c(-0.75,10)) + annotate(geom="text",x=qgamma(0.025,2.5,1)+0.15,y=-0.015,label="H^-1~(alpha/2)",parse=TRUE) + geom_vline(xintercept = 0) + geom_segment(x=qgamma(0.025,2.5,1),xend=qgamma(0.025,2.5,1),y=-0.005,yend=0.005) + geom_hline(yintercept = 0) + 
  annotate(geom="text",x=qgamma(0.975,2.5,1),y=-0.01,label="H^-1~(1-alpha/2)",parse=TRUE) + 
  geom_segment(x=qgamma(0.975,2.5,1),xend=qgamma(0.975,2.5,1),y=-0.005,yend=0.005) +
  geom_segment(x=7,xend=8,y=0.005,yend=0.07,lwd=0.1) + geom_segment(x=0.25,xend=-0.5,y=0.0075,yend=0.07,lwd=0.1) +
  annotate(geom="text",x=-0.55,y=0.08,label="alpha/2",parse=TRUE) + annotate(geom="text",x=8.1,y=0.08,label="alpha/2",parse=TRUE)
```
 
--

IC : $I(1-\alpha) =  \big[\hat{\theta} - H^{-1}(1-\alpha/2); \hat{\theta} - H^{-1}(\alpha/2)\big]$
 
---
# Construction d'intervalles de confiance
 
Version bootstrap empirique : $\hat{I}^*(1-\alpha) =  \big[\hat{\theta} - (\hat{H}^*_B)^{-1}(1-\alpha/2); \hat{\theta} - (\hat{H}^*_B)^{-1}(\alpha/2)\big]$
 
#### Calcul des quantiles de $\hat{H}^*_B$
 
 - $H^*(x) = G^*(x + \hat{\theta})$
 
--
 
```{r echo=FALSE, fig.width=7,fig.height=5, warning=FALSE,fig.retina=2} 
d <- data.frame(x=rep(x,2),y=c(pgamma(x,2.5,1),pgamma(x-2.5,2.5,1)),type=rep(c("H","G"),each=length(x)),
                xmin=c(qgamma(0.025,2.5,1)),xmax=c(qgamma(0.975,2.5,1)))
ggplot(data=d,aes(x,y,col=type)) + geom_line(lwd=1.5) + scale_color_discrete("Fonction") + xlim(c(0,12)) +
   annotate(geom="text",x=1.4,y=-0.028,label="x",parse=TRUE) + geom_segment(x=1.45,y=0,xend=1.45,yend=0.31,linetype=2,col="black") + geom_segment(x=4,y=0,xend=4,yend=0.31,linetype=2,col="black") +
  annotate(geom="text",x=4,y=-0.01,label="x+hat(theta)",parse=TRUE) + geom_segment(x = 1.45, y = 0.31, xend = 4, yend = 0.31, arrow = arrow(length = unit(0.015, "npc"), ends = "both"), lwd=0.1, col="black") +   annotate(geom="text",x=2.75,y=0.32,label="hat(theta)",parse=TRUE)
```

--
 
 - $(H^*)^{-1}(y) = (G^*)^{-1}(y) - \hat{\theta}$
 
---
# Construction d'intervalles de confiance

#### Méthode du bootstrap classique

$$\hat{I}_{boot}^*(1-\alpha) =  \big[2\hat{\theta} - \hat{\theta}^*_{(\lceil B(1-\alpha/2)\rceil )} ; 2\hat{\theta} - \hat{\theta}^*_{(\lceil B \alpha/2\rceil )}  \big]$$

---
# Construction d'intervalles de confiance

#### Méthode percentile

<span style="color:red">Hypothèse : il existe une transformation $h$ **croissante** telle que la loi de $h(\hat{\theta})$ soit symétrique autour de $\eta = h(\theta)$</span>


IC pour $\eta$ : 
$$IC_\eta(1-\alpha) = \left[U - H^{-1}_U\left( 1 - \frac{\alpha}{2}\right) ; U - H^{-1}_U \left(\frac{\alpha}{2}\right)\right]. $$

 - $h$ inconnue ?
 - $H_U$ inconnue ?

<span style="color:#16A085">**fin du cours 4**</span>

