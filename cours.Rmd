---
title: "Analyse de données 3"
author: <font size="5"> Charlotte Baey </font>
date: <font size="5"> M2 MIASHS - 2025/2026 </font>
output:
  xaringan::moon_reader:
    css: [default, metropolis, metropolis-fonts, hygge]
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      slideNumberFormat: "%current%"
  format: 
    revealjs:
      html-math-method: mathjax
---

<!-- <div class="progress-bar-container">
          <div class="progress-bar" style="width: calc(%current% / %total% * 100%);">
          </div>
        </div>  
-->

<style>

.remark-slide-content {
  background-color: #FFFFFF;
  border-top: 80px solid #16A085;
  font-size: 20px;
  line-height: 1.5;
  padding: 1em 2em 1em 2em
}

.my-one-page-font {
  font-size: 20px;
}

.remark-slide-content > h1 {
  font-size: 38px;
  margin-top: -85px;
}

.inverse {
  background-color: #16A085;
  border-top: 80px solid #16A085;
  text-shadow: none;
	background-position: 50% 75%;
  background-size: 150px;
  font-size: 40px
}

.title-slide {
  background-color: #16A085;
  border-top: 80px solid #16A085;
  background-image: none;
}

.remark-slide-number {
  position: absolute;
}

.remark-slide-number .progress-bar-container {
  position: absolute;
  bottom: 0;
  height: 4px;
  display: block;
  left: 0;
  right: 0;
}

.remark-slide-number .progress-bar {
  height: 100%;
  background-color: grey;
}

.left-col {
  width: 50%;
  float: left;
}
.right-col {
  width: 50%;
  float: right;
}

</style>

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: {
    Macros: {
      yellow: ["{\\color{yellow}{#1}}", 1],
      orange: ["{\\color{orange}{#1}}", 1],
      green: ["{\\color{green}{#1}}", 1]
    },
    loader: {load: ['[tex]/color']},
    tex: {packages: {'[+]': ['color']}}
  }
});
</script>


```{css, echo=FALSE}
.left-code {
  width: 40%;
  height: 92%;
  float: left;
}
.right-plot {
  width: 59%;
  float: right;
  padding-left: 1%;
}

```

```{css, echo=FALSE}
.left-plot {
  width: 59%;
  float: left;
}
.right-code {
  width: 40%;
  float: right;
  padding-left: 1%;
}
```


```{r setup, include = FALSE}

# R markdown options
#knitr::opts_chunk$set(echo = TRUE, 
                      #cache = TRUE, 
                      #fig.width = 10,
                      #fig.height = 5,
                      #fig.align = "center", 
                      #message = FALSE)

# Load packages
library(gapminder)
library(gganimate)
library(ggplot2)
library(tidyverse)
library(xaringancolor)
library(flair)
yellow <- "#FFCC29"
orange <- "#F58634"
green <- "#007965"
```

# Quelques informations pratiques

### Plan du cours
1. Simulation de variables aléatoires
2. Méthodes de Monte-Carlo
3. Modèles de mélange et algorithme EM 

### Organisation

- 1 séance de 2h15 par semaine
- Répartition type : entre 1h et 1h30 de cours, et entre 45 minutes et 1h15 de TP

### Evaluation

- 1 DS intermédiaire d'une durée de 2h (sur une séance de cours)
- 1 projet à rendre (en binôme)
- 1 DS final d'une durée de 3h

.red[**Aucun document autorisé lors des examens.**]



---
# Sommaire

**1. Simulations de variables aléatoires**
  - [Cours 1](#c1) (01/09/2025)
  - [Cours 2](#c2) (08/09/2025)
  - [Cours 3](#c3) (15/09/2025)
  
**2. Méthodes de Monte-Carlo**
  - [Cours 4](#c4) (29/09/2025)
  - [Cours 5](#c5) (06/10/2025)
  - [Cours 6](#c6) (20/10/2025)
  
**3. Modèles de mélange**
  - [Cours 7](#c7) (09/11/2025)
  - [Cours 8](#c8) (01/12/2025)
  - [Cours 9](#c9) (05/12/2025)
  

---
name: c1
class: inverse, middle, center

# Simulation de variables aléatoires
---

# Introduction

- Pour quoi faire ? &rarr; pour simuler une expérience aléatoire et étudier le comportement d'une suite de v.a. de loi donnée

--

- Pour les lois usuelles, la plupart des logiciels contiennent des fonctions permettant de simuler des v.a. suivant une telle loi

--

- Pour les autres lois, il faut avoir recours à d'autres méthodes :

 - méthode de la fonction inverse

 - méthode par transformation

 - méthode d'acceptation-rejet

---
# Lois usuelles
Lois usuelles &rarr; disponibles sous la plupart des langages ou logiciels 

- Sous R

Fonctions commençant par 'r': `runif`, `rnorm`, `rexp`, `rchisq`, `rbinom`, `rt`, ...
 
```{r, echo=FALSE}
library(reticulate)
Sys.setenv('RETICULATE_PYTHON'='/usr/bin/python3')
```

```{r}
rexp(n=10, rate=1/5)
```

- Sous Python

Module `scipy.stats` ou `numpy.random`

```{python, eval=TRUE}
from scipy.stats import expon
expon.rvs(scale=5, size=10)
```

.red[**attention aux conventions utilisées dans chaque langage !**]


---
# Reproductibilité des résultats 

- Les valeurs générées sont *pseudo-aléatoires*, elles dépendent d'une séquence déterministe de nombres
- Un paramètre nommé *seed* permet d'obtenir des suites identiques

--

```{r, results='hold'}
rnorm(5)
set.seed(0);rnorm(5)
set.seed(0);rnorm(5)
```
--

```{python, eval=TRUE, results='hold'}
import numpy as np
from scipy.stats import norm
np.random.seed(0)
norm.rvs(loc=0, size=5)
np.random.seed(0)
norm.rvs(loc=0, size=5)
```

---
#  Méthode de la fonction inverse

Repose sur le résultat suivant :
> Soit $X$ une variable aléatoire admettant une densité $f$ telle que $f>0$ p.s.. Alors sa *fonction de répartition* $F$ admet une fonction réciproque $F^{-1}$. 
>De plus, si $U \sim \mathcal{U}(]0,1[)$, alors $X = F^{-1}(U)$ suit la loi $F$.

--

```{r, echo=FALSE, fig.align='center', fig.height=6, fig.width=7, fig.retina=2}
set.seed(10101)
U <- runif(15)
X <- qnorm(U,sd=0.5)

plot(seq(-2,2,0.001),pnorm(seq(-2,2,0.001),sd=0.5),type="l",xlab="",ylab="",yaxt="n")
```


---
#  Méthode de la fonction inverse

Repose sur le résultat suivant :
> Soit $X$ une variable aléatoire admettant une densité $f$ telle que $f>0$ p.s.. Alors sa *fonction de répartition* $F$ admet une fonction réciproque $F^{-1}$. 
>De plus, si $U \sim \mathcal{U}(]0,1[)$, alors $X = F^{-1}(U)$ suit la loi $F$.


```{r, echo=FALSE, fig.align='center', fig.height=6, fig.width=7, fig.retina=2}
plot(seq(-2,2,0.001),pnorm(seq(-2,2,0.001),sd=0.5),type="l",xlab="",ylab="",yaxt="n")
mtext(paste0("U[",1:15,"]"),at=U,side=2,line=2,cex=0.75,las=2)
axis(2,at=sort(U),labels=rep("",15),cex.axis=0.75)
```

---
#  Méthode de la fonction inverse

Repose sur le résultat suivant :
> Soit $X$ une variable aléatoire admettant une densité $f$ telle que $f>0$ p.s.. Alors sa *fonction de répartition* $F$ admet une fonction réciproque $F^{-1}$. 
>De plus, si $U \sim \mathcal{U}(]0,1[)$, alors $X = F^{-1}(U)$ suit la loi $F$.


```{r, echo=FALSE, fig.align='center', fig.height=6, fig.width=7, fig.retina=2}
plot(seq(-2,2,0.001),pnorm(seq(-2,2,0.001),sd=0.5),type="l",xlab="",ylab="",yaxt="n")
mtext(paste0("U[",1:15,"]"),at=U,side=2,line=2,cex=0.75,las=2)
axis(2,at=sort(U),labels=rep("",15),cex.axis=0.75)
mtext(paste0("X[",1:15,"]"),at=X,side=1,line=2,cex=0.75,las=2)
axis(1,at=sort(X),labels=rep("",15),cex.axis=0.75)
segments(-2,U,X,U,lty=2,col="grey")
segments(X,U,X,0,lty=2,col="grey")
```


---
#  Méthode de la fonction inverse

Repose sur le résultat suivant :
> Soit $X$ une variable aléatoire admettant une densité $f$ telle que $f>0$ p.s.. Alors sa *fonction de répartition* $F$ admet une fonction réciproque $F^{-1}$. 
>De plus, si $U \sim \mathcal{U}(]0,1[)$, alors $X = F^{-1}(U)$ suit la loi $F$.


```{r, echo=FALSE, fig.align='center', fig.height=6, fig.width=7, fig.retina=2}
plot(ecdf(X),xlim=c(-2,2),main="",ylab="",yaxt="n")
points(seq(-2,2,0.001),pnorm(seq(-2,2,0.001),sd=0.5),type="l",xlab="",ylab="",yaxt="n")
mtext(paste0("U[",1:15,"]"),at=U,side=2,line=2,cex=0.75,las=2)
axis(2,at=sort(U),labels=rep("",15),cex.axis=0.75)
mtext(paste0("X[",1:15,"]"),at=X,side=1,line=2,cex=0.75,las=2)
axis(1,at=sort(X),labels=rep("",15),cex.axis=0.75)
segments(-2,U,X,U,lty=2,col="grey")
segments(X,U,X,0,lty=2,col="grey")
```


---
# Méthode de la fonction inverse

Dans le cas général, i.e. si $X$ n'admet pas de densité, on définit l'.orange[**inverse généralisée**] $F^-$:

$$\forall u \in ]0,1[, \quad F^-(u) =  \inf \ \{x \in \mathbb{R}, F(x) \geq u  \}$$

--

</br>

On a alors le résultat suivant :

> Soit $U \sim \mathcal{U}(]0,1[)$, alors $X = F^{-}(U)$ suit la loi $F$.


</br>*Preuve*

--

</br>
</br>
**RQ :** La fonction inverse (généralisée) de la f.d.r. s'appelle la .orange[**fonction quantile**].

---
# Méthode de la fonction inverse

Le résultat précédent conduit à l'algorithme suivant. 

Soit $F$ la fonction de répartition de la loi selon laquelle on souhaite générer des réalisations, et $F^{-}$ son inverse (généralisée). Pour générer un échantillon de taille $n$ selon la loi $F$, on réalise les étapes suivantes :

1. On génère $n$ réalisations $u_1,\dots,u_n$ selon la loi $\mathcal{U}(]0,1[)$
2. On pose $x_i = F^{-}(u_i)$ pour $i=1,\dots,n$
3. On renvoie l'échantillon $x_1,\dots,x_n$

---
# Exemple 1 : loi à densité

Soit $X$ une v.a. de loi de Cauchy, de densité $f(x) = \frac{1}{\pi(1+x^2)}$.

1. Quelle est sa fonction de répartition ? et son inverse ?
2. En déduire un algorithme pour générer un échantillon selon la loi de Cauchy.

--

```{r}
u <- runif(10000)
Finv <- function(u){tan(pi*(u-0.5))}
x <- Finv(u)
```

--

```{r, echo=FALSE, fig.width=5, fig.height=4.5, fig.retina=2, fig.align='center', warning=FALSE}
d <- data.frame(x=x)
ggplot(d, aes(x)) +
  geom_histogram(aes(y = after_stat(density)), binwidth = 0.35, fill = "#999999") +
  stat_function(
    fun = dcauchy,
    lwd = 1
  ) + xlim(c(-10,10))
```

---
# Exemple 2 : loi discrète

Soit $X$ une v.a. discrète sur $\{1,2,3,4,5\}$ dont la loi est donnée par $p_k=\mathbb{P}(X=k)$, avec:

$$p_1=0.15, \quad p_2=0.25, \quad p_3=0.30, \quad p_4=0.20, \quad p_5=0.10$$

1. Quelle est la fonction de répartition $F$ de $X$ ?
2. Identifier l'inverse généralisée de $F$.

--

```{r, echo=FALSE, fig.align='center', fig.height=5.5, fig.width=11, fig.retina=2}
x <- c(1,2,3,4,5)
px <- c(0.15,0.25,0.30,0.20,0.10)
Fx <- cumsum(px)
par(mfrow=c(1,2))
plot(x,Fx,pch=19,cex=1.75,ylim=c(0,1),xlim=c(0,6),col="darkorange",main="Fonction de répartition",xlab="x",ylab="F(x)")
segments(0:5,c(0,Fx),1:6,c(0,Fx),lty=1,lwd=2,col="darkblue")
points(x,Fx,col="darkorange",pch=19,cex=1.75)
plot(Fx,x,pch=19,cex=1.75,col="darkorange",ylim=c(1,5),xlim=c(0,1.1),main="Fonction quantile",xlab="u",ylab=expression(paste("F"^"-","(u)")))
segments(c(0,Fx),1:5,c(Fx,1),1:5,lty=1,lwd=2,col="darkblue")
points(Fx,x,pch=19,cex=1.75,col="darkorange")
```

---
# <img src="laptop-code-solid.svg" width="45" height="45" align="center"> Exercices

1. > Utiliser la méthode de la fonction inverse pour générer un échantillon de taille $n=1000$ de loi exponentielle de paramètre 5.
2. > Soit $U$ une variable aléatoire uniforme sur $[0,1]$, et $0 < p < 1$. Quelle est la loi de la variable aléatoire $X = \unicode{x1D7D9}_{U \leq p}$ ? En déduire une méthode pour simuler une variable aléatoire de loi binomiale de paramètres $(n,p)$, avec $n>1$.
3. > Soit $X$ une variable aléatoire à valeurs dans $\{1,\dots,K\}$. On note $p_k = \mathbb{P}(X=k).$ Utiliser la méthode de la fonction inverse pour proposer une méthode de simulation selon la loi de $X$ à partir d'une loi uniforme sur $[0,1].$

</br>
</br>
**(Exercice 1 de la fiche 1)**

<span style="color:#16A085">**fin du cours 1**</span>


---
name: c2
# Méthode par transformation

- Repose sur les liens entre lois de probabilité.

- Si une v.a. $X$ de loi $f$ peut s'écrire en fonction d'une v.a. $Y$ dont la loi est facile à simuler, on peut utiliser la relation entre $X$ et $Y$ pour générer un échantillon de loi $f$ à partir d'un échantillon de loi $g$.

- La méthode de la fonction inverse est un cas particulier.

- Exemples :
 - transformation affines
 - sommes de v.a. i.i.d.
 - loi conditionnelle
 - loi mélange
 
---
# Transformations affines

$X$ de loi $f$ est une transformation affine de $Y$ de loi $g$, s'il existe $a\neq 0$ et $b$ dans $\mathbb{R}$ tq :
$$ X = aY + b $$
La loi de $X$ est alors :
$$  f(x) = \frac{1}{a}g\left(\frac{x-b}{a}\right) $$
</br>

Algorithme de simulation selon $f$:
> 1. Générer $y_1,\dots,y_n$ selon la loi $g$. 

> 2. Poser $x_i=ay_i+b$ pour $i=1,\dots,n$.

--

</br>
Quelles lois sont concernées ?

---
# Paramètres de localisation et d'échelle

On appelle .orange[**location-scale family**] une famille de distributions telles que, si $Y$ a une loi appartenant à cette famille, alors la loi de $X=aX+b$ appartient également à cette même famille.

Les paramètres $a$ et $b$ s'appellent respectivement .orange[**paramètre d'échelle**] et .orange[**paramètre de localisation**].

Quelques exemples :
 - les lois normales
 - les lois de Student
 - les lois de Fréchet
 - les lois de Cauchy
 - les lois uniformes
 - les lois de Laplace
 - ...

---
# Exemple : les lois normales

Proposer un algorithme pour générer un échantillon selon la loi normale de moyenne $\mu$ et de variance $\sigma^2$ à partir d'un échantillon de loi normale centrée réduite.

--

```{r norm, echo=TRUE, fig.show='hold'}
mu <- 5
sigma <- 2
x <- rnorm(2000)
y <- sigma*x + mu
```

--

```{r, echo=FALSE, fig.width=7, fig.height=4.5, fig.retina=2, fig.align='center', warning=FALSE}
d <- data.frame(y=y)
ggplot(d, aes(x=y)) +
  geom_histogram(aes(y = after_stat(density)), binwidth = 0.35, fill = "#999999") +
  stat_function(
    fun = dnorm,
    mapping=aes(colour="Densité N(5,4)"),
    args = list(mean=5,sd=2),
    lwd = 1
  ) + xlim(c(-5,15)) + scale_colour_manual("", values = c("Densité N(5,4)" = "black"))
```

---
# <img src="laptop-code-solid.svg" width="45" height="45" align="center"> Exercices

1. > Montrer que la famille des lois exponentielles $\{\mathcal{E}(\lambda),\lambda\in\mathbb{R}^{+*}\}$ forme une famille de lois paramétrées par un paramètre de localisation et d'échelle. Donner la valeur de ces deux paramètres pour la distribution de paramètre $\lambda$. 

 > Quelle transformation affine permet d'obtenir une variable aléatoire $X$ de loi $\mathcal{E}(\lambda)$ à partir d'une variable aléatoire $Y$ de loi $\mathcal{E}(1)$ ? 

2. > Proposer un algorithme pour tirer $X\sim\mathcal{N}(\mu,\Sigma)$, où $X \in \mathbb{R}^d$, $\mu$ est le vecteur d'espérances et $\Sigma$ la matrice de variance-covariance. 

</br>
**(Exercice 2 de la fiche 1)**

---
# Lois dérivants d'autres lois

Compléter le tableau suivant donnant le lien entre $X_1,X_2,\dots$ i.i.d. de loi $g$ et $Y$ de loi $f$:

|f | g |
|-|-|
|$$\chi^2_n$$ | $$\mathcal{N}(0,1)$$ |  
|$$t_n $$ | $$\mathcal{N}(0,1)$$  | 
|$$\mathcal{F}_{n,p} $$ | $$\mathcal{N}(0,1)$$ | 
|$$\chi^2_{2n}$$ | $$\mathcal{E}(1)$$ | 
|$$\Gamma(a,\beta)$$ | $$\mathcal{E}(1)$$ |
|$$\mathcal{B}e(a,b)$$ | $$\mathcal{E}(1)$$ | 

---
# Lois dérivants d'autres lois

Compléter le tableau suivant donnant le lien entre  $X_1,X_2,\dots$ i.i.d. de loi $g$ et $Y$ de loi $f$:

|f | g | lien|
|--|
|$$\chi^2_n$$ | $$\mathcal{N}(0,1)$$ | $Y = \sum_{i=1}^nX_i^2 \quad n \in \mathbb{N}^*$|
|$$t_n $$ | $$\mathcal{N}(0,1)$$  | $Y = \frac{X_{n+1}}{\sqrt{\sum_{i=1}^nX_i^2 /n}}\quad n \in \mathbb{N}^*$|
|$$\mathcal{F}_{n,p} $$ | $$\mathcal{N}(0,1)$$ | $Y = \frac{\sum_{i=1}^nX_i^2/n}{\sum_{i=n+1}^{n+p}X_i^2/p}$|
|$$\chi^2_{2n}$$ | $$\mathcal{E}(1)$$ | $Y = 2 \sum_{i=1}^n X_i \quad n \in \mathbb{N}^*$|
|$$\Gamma(a,\beta)$$ | $$\mathcal{E}(1)$$ | $Y = \beta \sum_{i=1}^aX_i^2 \quad a \in \mathbb{N}^*$|
|$$\mathcal{B}e(a,b)$$ | $$\mathcal{E}(1)$$ | $Y = \frac{\sum_{j=1}^aX_j}{\sum_{j=1}^{a+b}X_j} \quad a,b \in \mathbb{N}^*$|

---
# Méthode par transformation

On peut également utiliser les relations entre lois jointes, lois conditionnelles et lois marginales.

- **Exemple 1 :**
La loi de Student à $\nu$ degrés de liberté peut s'obtenir comme la loi marginale d'un couple de variables aléatoires $(X,Y)$ telles que :
	$$X \ | \ Y=y \sim \mathcal{N}(0, \nu/y), \quad Y \sim \chi^2_\nu $$ 

--
.pull-left[
```{r, eval=FALSE}
v <- 15; n <- 10000
Y <- rchisq(n,v)
XcondY <- rnorm(n,0,v/Y)
XY <- cbind(XcondY,Y) 
plot(XY,pch=19) 
```
]
.pull-right[
```{r, echo=FALSE, fig.width=6,fig.height=4, fig.retina=2}
v <- 15; n <- 10000
Y <- apply(matrix(rnorm(n*v),nc=v),1,FUN = function(x){sum(x^2)}) 
XcondY <- rnorm(n,0,v/Y)
XY <- cbind(XcondY,Y) # loi jointe (X,Y)
 #Visualisation de la densité
dXY=data.frame(XY);names(dXY) <- c("X","Y")
ggplot(dXY,aes(x=X,y=Y)) + geom_bin2d(bins = 70) +
  scale_fill_continuous(type = "viridis") 
# ou encore :
#ggplot(dXY,aes(x=X, y=Y)) + stat_density_2d(aes(fill = ..level..), geom = "polygon") 

```
]

---
# Méthode par transformation

On peut également utiliser les relations entre lois jointes, lois conditionnelles et lois marginales.

- **Exemple 2 :**
On appelle *loi mélange* (à densité) une loi qui s'écrit comme une combinaison convexe de densités : 
	$$f(x) = \sum_{k=1}^K \alpha_k f_k(x), \quad \text{avec } \ \sum_{k=1}^K \alpha_k = 1$$
La loi de mélange s'obtient comme la loi marginale d'un couple de variables aléatoires $(X,Z)$ telles que :
$$X \ | \ Z = k \sim f_k, \quad \mathbb{P}(Z=k) = \alpha_k,$$

---
# Méthode par transformation

- **Exemple 2 (suite)** : générer un échantillon selon la loi $f(x) =  0.4 \mathcal{N}(0,1) + 0.6 \mathcal{N}(2,0.5)$

--

```{r, fig.width=6,fig.height=5,fig.align='center'}
Z <- rbinom(n,1,0.6) # on génère une v.a. donnant le numéro de groupe
Xcond <- ifelse(Z==0,rnorm(n),rnorm(n,2,sqrt(0.5))) # loi de X sachant Z
XZ <- cbind(Xcond,Z) # on colle ensemble X et Z pour avoir la loi jointe
hist(XZ[,1],freq=F,breaks=50,main="") 
lines(seq(-4,4,0.01),0.4*dnorm(seq(-4,4,0.01)),col="red",lwd=2)
lines(seq(-4,4,0.01),0.6*dnorm(seq(-4,4,0.01),2,sqrt(0.5)),col="blue",lwd=2)
```

<span style="color:#16A085">**fin du cours 2**</span>


---
name: c3
# Méthode par transformation : Box-Muller

Méthode due à George Box et Mervin Muller (1958), permettant de générer deux échantillons indépendants selon la loi $\mathcal{N}(0,1)$ uniquement à l'aide de tirages selon la loi $\mathcal{U}(]0,1])$.

Elle est utilisée dans certaines bibliothèques de calculs (notamment en C++).

--

Algorithme de Box-Muller :
> 1. Générer $U_1$ et $U_2$ indépendamment selon une loi $\mathcal{U}(]0,1])$
2. Poser 
$$\begin{cases}
X_1 & = \sqrt{-2 \log U_1} \cos (2 \pi U_2) \\
X_2  & = \sqrt{-2 \log U_1} \sin (2 \pi U_2) \\
\end{cases}$$
3. Renvoyer $(X_1,X_2)$, couple de v.a. indépendantes de loi $\mathcal{N}(0,1)$

---
# Méthode par transformation : Box-Muller

*Démonstration :*

Soient $U_1$ et $U_2$ deux v.a. indépendantes de loi $\mathcal{U}(]0,1])$.

1. Quelle est la loi de $R= \sqrt{-2 \log U_1}$ ? et de $\theta = 2\pi U_2$ ?

2. Exprimer $X_1$ et $X_2$ en fonction de $R$ et $\theta$. 

3. Quelle est la loi jointe de $(X_1,X_2)$ ? 


---
# <img src="laptop-code-solid.svg" width="45" height="45" align="center"> Exercices

</br>

> Implémenter l'algorithme de Box-Muller


</br>
**(Exercice 2 (fin) de la fiche 1)**


---
# Méthode d'acceptation-rejet

Proposition :
> Soit $X$ une v.a. de densité $f$ et soit $g$ une densité de probabilité et une constante $M \geq 1$ t.q. $\forall x, f(x) \leq M g(x)$. Alors pour simuler selon la loi $f$ il suffit de :
1. simuler $Y \sim g$
2. simuler $U | Y=y \sim \mathcal{U}([0,Mg(y)])$
3. si $0 < U < f(Y)$, poser $X=Y$, sinon reprendre l'étape 1.

*Preuve*  

<!--<div class="horizontalgap" style="width:10px"></div> <span style="color:#16A085">**fin du cours 5 (05/02/2024)**</span>-->

--

Remarques :
- on a seulement besoin de connaître $f$ à une constante multiplicative près
- $\forall x, f(x) \leq M g(x) \Rightarrow \text{supp}(f) \subset \text{supp}(g)$
- probabilité d'accepter un candidat : $\frac{1}{M}$ (influence du choix de $g$)



---
# Méthode d'acceptation-rejet

Illustration

```{r, fig.retina=2, fig.height=7, fig.width=10, echo=FALSE, fig.align='center', warning=FALSE, message=FALSE}
fx <- function(x){0.65*dnorm(x,0,1.1)+0.35*dnorm(x,4,1)}
gx <- function(x){1.83*dnorm(x,1.4,2.5)}
x <- seq(-8,10,length.out=1000)
dx <- data.frame(x=c(x,x),y=c(fx(x),gx(x)),Fonction=c(rep("f",length(x)),rep("Mg",length(x))))
ggplot(data=dx,aes(x=x,y=y,col=Fonction)) + geom_line(size=1) #+ geom_point(x=xg,y=0,cex=4) + 
  #annotate(geom="text",x=xg,y=0.01,label="Y",cex=5) 
```

---
# Méthode d'acceptation-rejet

- Tirage de $Y$ selon la loi $g$

```{r, fig.retina=2, fig.height=7, fig.width=10, echo=FALSE, fig.align='center'}
set.seed(10)
xg <- rnorm(1,1.4,2.5)
ggplot(data=dx,aes(x=x,y=y,col=Fonction)) + geom_line(size=1) + geom_point(x=xg,y=0,cex=4) + 
  annotate(geom="text",x=xg,y=0.01,label="Y1",cex=5) 
```


---
# Méthode d'acceptation-rejet

- Tirage de $U$ conditionnellement à $Y=y$ selon la loi $\mathcal{U}([0,Mg(y)])$ &rarr; on rejette

```{r, fig.retina=2, fig.height=7, fig.width=10, echo=FALSE, fig.align='center'}
set.seed(110)
yg <- runif(1,0,gx(xg))
ggplot(data=dx,aes(x=x,y=y,col=Fonction)) + geom_line(size=1) + geom_point(x=xg,y=yg,cex=4) + geom_segment(x=xg,y=0,xend=xg,yend=gx(xg)) + geom_point(x=xg,y=0,cex=4) + 
  annotate(geom="text",x=xg,y=0.01,label="Y1",cex=5) 
```

---
# Méthode d'acceptation-rejet

- Tirage de $U$ conditionnellement à $Y=y$ selon la loi $\mathcal{U}([0,Mg(y)])$ &rarr; on accepte 

```{r, fig.retina=2, fig.height=7, fig.width=10, echo=FALSE, fig.align='center'}
set.seed(1000)
xg <- rnorm(1,1.4,2.5)
yg <- runif(1,0,gx(xg))
ggplot(data=dx,aes(x=x,y=y,col=Fonction)) + geom_line(size=1) + geom_point(x=xg,y=yg,cex=4) +  geom_segment(x=xg,y=0,xend=xg,yend=gx(xg)) + geom_point(x=xg,y=0,cex=4) + 
  annotate(geom="text",x=xg,y=0.01,label="Y2",cex=5) 
```


---
# Méthode d'acceptation-rejet

A la fin :

```{r, fig.align='center', fig.retina=2, fig.height=7, fig.width=10, echo=FALSE, warning=FALSE, message=FALSE}
library(gganimate)
x <- seq(-8,10,0.01)
fxs <- fx(x)
gxs <- gx(x)
set.seed(0)
xg <- rnorm(5000,1.4,2.5)
yg <- runif(5000,0,gx(xg))
u <- ifelse(yg<fx(xg),1,0)
ddens <- data.frame(x=x,y=fxs,accept=TRUE)
ddens$accept <- factor(ddens$accept,labels=c("Accepté"))
ddens1 <- data.frame(x=x,y=gxs,accept=FALSE)
ddens1$accept <- factor(ddens1$accept,labels=c("Rejeté"))
dpoints <- data.frame(x=xg,y=yg,accept=u)
dpoints$accept <- factor(dpoints$accept,labels=c("Rejeté","Accepté"))
dpoints$n <- rep(seq(100,5000,by=100),each=100)
ggplot(data = dpoints, aes(x=x,y=y,col=accept))  + geom_point(size=0.9) + geom_line(data=ddens, linewidth=1.1) +  geom_line(data=ddens1, linewidth=1.1) + scale_color_hue(direction=-1) #+ scale_color_brewer(name="",palette="Dark2")# + transition_states(n) + shadow_mark() 
```


---
# Un cas particulier

Un cas particulier intéressant : le cas $f$ bornée à support compact.

--

- on prend pour $g$ la loi uniforme sur le support de $f$
- et on simule $U$ uniforme sur $[0,m]$ où $m=\max_x f(x)$

--

ex. : $\displaystyle f(x) = \frac{1}{8} |x^4 - 5x^2 + 4| \ \mathbb{1}_{[-2,2]}(x)$
```{r, echo=FALSE, fig.height=6,fig.width=8,fig.align='center',fig.retina=2}
fdens <- function(x){
  ifelse(abs(x)<2,abs(x^4-5*x^2+4)/8,0)
}

curve(fdens,from=-2.5,2.5,lwd=2)
```

---
# Un cas particulier

Un cas particulier intéressant : le cas $f$ bornée à support compact.

- on prend pour $g$ la loi uniforme sur le support de $f$
- et on simule $U$ uniforme sur $[0,m]$ où $m=\max_x f(x)$

ex. : $\displaystyle f(x) = \frac{1}{8} |x^4 - 5x^2 + 4| \ \mathbb{1}_{[-2,2]}(x)$
```{r, echo=FALSE, fig.height=6,fig.width=8,fig.align='center',fig.retina=2}
curve(fdens,from=-2.5,2.5,lwd=2)
curve(2*dunif(x,-2,2),from=-2.5,to=2.5,col="red",lwd=2,add=T)
```

---
# Un cas particulier

$f(x) = \frac{1}{8} |x^4 - 5x^2 + 4| \ \mathbb{1}_{[-2,2]}(x)$

.pull-left[
```{r paged.print=FALSE, eval=FALSE}
Y <- runif(10000,-2,2)
U <- runif(10000,0,0.5)

accept <- (U<fdens(Y))
X <- Y[accept]

hist(X,breaks=50,freq=F)
points(x,fx,type="l",col="red")
```

```{r, echo=FALSE}
Y <- runif(10000,-2,2)
U <- runif(10000,0,0.5)
accept <- (U<fdens(Y))
```

```{r}
mean(accept)
```

&rarr; la moitié des points simulés est 'perdue'
]

.pull-right[
```{r, echo=FALSE}
Y <- runif(10000,-2,2)
U <- runif(10000,0,0.5)

accept <- (U<fdens(Y))
X <- Y[accept]

hist(X,breaks=50,,freq=F,xlab="x",ylab="",main="")
points(seq(-2,2,0.01),fdens(seq(-2,2,0.01)),type="l",col="red",lwd=2)
```
]


---
# Un cas particulier

Visuellement : on peut représenter les points acceptés dans le rectangle $[-2,2] \times [0,1/2]$

</br>

```{r, fig.align='center',fig.height=6,fig.width=8, fig.retina=1,echo=FALSE, warning=FALSE, message=FALSE}
library(gganimate)
x <- seq(-2,2,0.01)
fx <- fdens(x)
ddens <- data.frame(x=x,y=fx,accept=TRUE)
ddens$accept <- factor(ddens$accept,labels=c("Accepté"))
dpoints <- data.frame(x=Y,y=U,accept=accept)
dpoints$accept <- factor(dpoints$accept,labels=c("Rejeté","Accepté"))
dpoints$n <- rep(seq(100,10000,by=100),each=100)
ggplot(data = dpoints, aes(x=x,y=y,col=accept))  + geom_point(size=0.9) + geom_line(data=ddens, linewidth=1.1) + scale_color_discrete(name="") + scale_color_brewer(name="",palette="Dark2") #+ transition_states(n) + shadow_mark() 
```


---
# Retour sur l'acceptation-rejet

Rappel de l'algorithme de simulation :
> Soit $X$ une v.a. de densité $f$ et soit $g$ une densité de probabilité et une constante $M \geq 1$ t.q. $\forall x, f(x) \leq M g(x)$. Alors pour simuler selon la loi $f$ il suffit de :
1. simuler $Y \sim g$
2. simuler $U | Y=y \sim \mathcal{U}([0,Mg(y)])$
3. si $0 < U < f(Y)$, poser $X=Y$, sinon reprendre l'étape 1.

Autre formulation (plus générale) :
> Soit $X$ une v.a. de densité $f$ connue à une constante près (i.e. on connaît $\tilde{f} = c f$),  $g$ une densité de probabilité et $\tilde{M}$ t.q. $\forall x, \tilde{f}(x) \leq \tilde{M} g(x)$. Alors pour simuler selon la loi $f$ il suffit de :
1. simuler $Y \sim g$
2. simuler $U \sim \mathcal{U}([0,1])$
3. si $0 < U < \tilde{f}(Y)/\tilde{M}g(Y)$, poser $X=Y$, sinon reprendre l'étape 1.


---
# <img src="laptop-code-solid.svg" width="45" height="45" align="center"> Exercices

Soit $f$ la densité de la loi normale centrée réduite, et $g$ la densité de la loi de Cauchy.

1.  > Trouver une constante $M$ telle que $f(x) \leq M g(x)$ pour tout $x \in \mathbb{R}$.
	 
2. >  Proposer un algorithme d'acceptation-rejet pour simuler une loi normale à partir d'une loi de Cauchy, puis un algorithme permettant de générer une loi normale à partir d'une loi uniforme sur $[0,1]$.

3. > En moyenne, combien faut-il générer de variables aléatoires de loi uniforme pour obtenir une réalisation selon la loi normale ?

**(Exercice 3 de la fiche 1)**


On cherche à simuler un échantillon i.i.d. de loi $\mathrm{Beta}(a,b)$, avec $a > 1$ et $b > 1$.
1. > Pourquoi a-t-on besoin des conditions $a > 1$ et $b > 1$ ? Calculer $M_{a,b}$, le maximum de $f_{a,b}$. En déduire une densité $g$ telle que $f_{a,b}(x) \leq M_{a,b} g(x)$.

3. > Proposer et implémenter un algorithme de type acceptation-rejet pour simuler une variable aléatoire de loi Beta de paramètres $a$ et $b$.

**(Exercice 4 de la fiche 1)**

<span style="color:#16A085">**fin du cours 3**</span>

---
name: c4
# Programme

- Finir la fiche de TD 1
- Résumer le chapitre sur la simulation de v.a.
- Commencer le chapitre sur les méthodes de Monte-Carlo

---
# Résumé du chapitre

- On a vu différentes méthodes pour simuler des v.a. selon une loi cible $f$

  - méthode de la transformation inverse (en utilisant l'inverse de la f.d.r.)
  - méthode par transformation
  - méthode d'acceptation-rejet

--

- Pourquoi a-t-on besoin de savoir simuler des v.a. selon une certaine loi $f$ ? Quelles sont les applications possibles ?

---

class: inverse, middle, center

# Les méthodes de Monte Carlo


---
# Un peu d'histoire ...

- première expérience de type Monte Carlo dûe à Buffon au XVIIIème siècle &rarr; l'aiguile de Buffon

.center[
<img src="https://upload.wikimedia.org/wikipedia/commons/thumb/5/58/Buffon_needle.svg/1920px-Buffon_needle.svg.png" width="200" align="center">]

--

- les méthodes actuelles sont nées pendant la seconde guerre mondiale au laboratoire américain de Los Alamos &rarr; nom de code *Monte-Carlo*

.center[
<img src="https://frenchriviera.travel/wp-content/uploads/2018/03/Monte-Carlo-Casino1.jpg" width="300" align="center">
]


---
# Objectif

- Question principale : **l'approximation d'intégrales**

$$\int h(x) dx $$

- Outil théorique à la base des méthodes de Monte-Carlo : **la loi forte des grands nombres**

- Outil pratique nécessaire : **la simulation de variables aléatoires**



---
# Monte Carlo classique

**Objectif** : calculer une intégrale de la forme $\mu=\int h(x) f(x) dx$ où $f$ est une densité de probabilité.

--

> *Définition.* Soit $X_1,\dots,X_n$ un échantillon i.i.d. de loi $f$. L'estimateur de Monte Carlo de $\mu$ est :
$$\hat{\mu}_n = \frac{1}{n} \sum_{i=1}^n h(X_i)$$

--

Propriétés :
- estimateur sans biais
- fortement consistant (par la loi forte des grands nombres)
- construction d'intervalles de confiance par le TCL

Remarques :
- vitesse de convergence en $\sqrt{n}$, indépendante de la dimension
- ne dépend pas de la régularité de $h$

---
# Exemples d'application

Plus généralement, les méthodes de Monte Carlo s'utilisent pour :

- approcher le niveau ou la puissance d'un test

- approcher une intégrale en grande dimension

- faire de l'optimisation (e.g. algorithme du recuit simulé, descente de gradient stochastique, ...)

- ...

---
# Monte Carlo classique

Algorithme d'estimation par Monte-Carlo:
> 1. Générer $X_1,...,X_n$ iid selon la loi de densité $f$
2. Poser $\hat{\mu}_n = \frac{1}{n}\sum_{i=1}^n h(X_i)$

 - variance $\text{Var} (\hat{\mu}_n) = \frac{1}{n} \left(\int h^2(x)f(x)dx - \mu^2\right) = \frac{\sigma_f^2}{n}$ (finie si $\sigma_f^2$ est finie)
 
--

 - On peut estimer $\sigma_f^2$ par : $$v_n = \frac{1}{n} \sum_{i=1}^n h^2(X_i) - \hat{\mu}_n^2$$
 
--

 - IC de niveau $1-\alpha$ en utilisant le TCL appliqué à la suite de v.a. $(Y_i)$ où $Y_i=h(X_i)$. Les $(Y_i)$ sont bien i.i.d., et si $\sigma_f^2 < +\infty$ on a :
 $$\hat{I}_\mu =\left[\hat{\mu}_n - q_{1-\alpha/2}^{\mathcal{N}(0,1)} \sqrt{v_n/n} ; \hat{\mu}_n + q_{1-\alpha/2}^{\mathcal{N}(0,1)} \sqrt{v_n/n} \right]$$

 &rarr; pour diminuer la taille de l'IC par 10, il faut multiplier $n$ par 100.


---
# <img src="laptop-code-solid.svg" width="45" height="45" align="center"> Exercices

Approcher les intégrales suivantes par Monte Carlo :

1. > $$I = \int_0^1(\cos(20x) + \sin(30x))^2 dx$$

2. > $$J = \int_0^1 \sin(x^2) e^{-x} dx$$
	
3. > $$K = \int_0^{+\infty} \sin(x^3) e^{-x} e^{-\frac{x^2}{2}} dx $$


Pour 2. et 3., utiliser deux algorithmes de Monte Carlo différents.


<span style="color:#16A085">**fin du cours 4**</span>


---
name: c5
# Choix de la loi de simulation
**Exemple** : on veut calculer $\int_0^1 e^{-x}x^{-a} dx$ pour $0 < a < 1$.

--

Monte Carlo "naïf" &rarr; $f$ densité de la loi uniforme sur $[0,1]$ et $h(x) = e^{-x}x^{-a}$.

--

```{r} 
set.seed(09102024)
a <- 1/2
n <- 1000
X <- runif(n)
hX <- exp(-X) * X^(-a)
print(paste0("Estimation : ",mean(hX),", variance : ",var(hX)/n))
```

--

Peut-on faire mieux ? 

---
# Choix de la loi de simulation

A quoi ressemble la fonction $h(x) = e^{-x}x^{-a}$ ?

```{r,fig.align='center',fig.height=4,fig.width=5, fig.retina=2,echo=FALSE}
xabs <- seq(0,1,0.001)
yord <- exp(-xabs) * xabs^(-a)
dd <- data.frame(x=xabs,y=yord)
drug <- data.frame(x=X)
ggplot(data=NULL,aes(x=xabs,y=yord)) + geom_line() + ggtitle("Fonction h") + xlab("x") + ylab("h")
```


---
# Choix de la loi de simulation

A quoi ressemble la fonction $h(x) = e^{-x}x^{-a}$ ? 

```{r,fig.align='center',fig.height=4,fig.width=7, fig.retina=2,echo=FALSE}
xabs <- seq(0,1,0.001)
yord <- exp(-xabs) * xabs^(-a)
dd <- data.frame(x=xabs,y=yord)
drug <- data.frame(x=X,Loi="Uniforme [0,1]")
ggplot(data=NULL,aes(x=xabs,y=yord)) + geom_line() + geom_rug(data=drug,aes(x=x,col=Loi), inherit.aes=FALSE, linewidth=0.1) + ggtitle("Fonction h") + xlab("x") + ylab("h")

#xpred <- rbinom(10000,10,ech_finv)
#ggplot(data=NULL,aes(x=as.factor(xpred))) + geom_bar(fill="grey",col="darkgrey") + #geom_rug(aes(x=as.factor(X)),col="red",lwd=2,length=unit(0.05, "npc")) + xlab("Prédictions")
```
&rarr; en échantillonnant selon la loi uniforme, on obtient des points répartis de façon **uniforme** sur $[0,1]$.

&rarr; Peut-on trouver une loi d'échantillonnage qui produise plus de points proches de 0 ?


---
class: my-one-page-font
# Choix de la loi de simulation

Autre choix : $g(x) = (1-a) \ x^{-a} \mathbb{1}_{[0,1]}(x)$ et $\tilde{h}(x) = e^{-x}/ (1-a).$

--

```{r, fig.align='center',fig.height=4,fig.width=5, fig.retina=2,echo=FALSE}
yord2 <- (1-a)*xabs^(-a)
ggplot(data=NULL,aes(x=xabs,y=yord2)) + geom_line() + ggtitle("Densité g") + xlab("x") + ylab("g")
```
--

```{r} 
Z <- X^(1/(1-a))
hZ <- exp(-Z) / (1-a)
print(paste0("Estimation : ",mean(hZ),", variance : ",var(hZ)/n))
```

On a diminué la variance par 100 pour une même taille d'échantillon.

--
Vraie valeur : 1.49365 (intégration numérique)


---
# Choix de la loi de simulation

Avant, on intégrait $h(x) = e^{-x}x^{-a}$ en utilisant la loi uniforme sur $[0,1]$.

Maintenant, on intègre $\tilde{h}(x) = e^{-x}/(1-a)$ en utilisant la loi de densité $(1-a) \ x^{-a} \mathbb{1}_{[0,1]}(x)$.

```{r,fig.align='center',fig.height=4,fig.width=12, fig.retina=2,echo=FALSE, message=F, warning=F}
xabs2 <- seq(0,1,0.001)
yord2 <- exp(-xabs2) / (1-a)
drug2 <- data.frame(x=Z,Loi="Puissance")
p1 <- ggplot(data=NULL,aes(x=xabs,y=yord)) + geom_line() + geom_rug(data=drug,aes(x=x,col=Loi), inherit.aes=FALSE, linewidth=0.1) + ggtitle("Fonction h") + xlab("x") + ylab("h")
p2 <- ggplot(data=NULL,aes(x=xabs2,y=yord2)) + geom_line() + geom_rug(data=drug2,aes(x=x,col=Loi), inherit.aes=FALSE, linewidth=0.1) + ggtitle(str2expression(paste("Fonction","tilde(h)",sep="~"))) + xlab("x") + ylab(expression(tilde(h)))
library(gridExtra)
grid.arrange(p1,p2,ncol=2)
```

---
# Echantillonnage préférentiel 

**Objectif** : toujours d'évaluer $\mu = \int h(x) f(x) dx$


On va **ré-écrire l'intégrale** sous la forme :

$$\mu = \int h(x) f(x) dx = \int h(x) \frac{f(x)}{g(x)} g(x) dx,$$
avec $g$ densité de probabilité, telle que $(g(x) = 0) \Rightarrow (h(x)f(x) = 0)$.

--

Pourquoi ?

 - pas toujours possible de simuler selon la loi $f$, ou d'utiliser la méthode d'acceptation-rejet
 - ou on s'intéresse à un évènement de probabilité trop faible
 - dans certains cas cela permet de réduire la variance de l'estimateur

---
# Echantillonnage préférentiel

> *Définition.* Soit $Z_1,\dots,Z_n$ un échantillon i.i.d. de loi $g$. L'estimateur par échantillonnage préférentiel de $\mu$ est :
$$\tilde{\mu} = \frac{1}{n} \sum_{i=1}^n h(Z_i) \frac{f(Z_i)}{g(Z_i)} = \frac{1}{n} \sum_{i=1}^n w_i h(Z_i)$$

Propriétés : (si $\text{supp}(f) \subset \text{supp}(g)$)

- estimateur sans biais 
- fortement consistant

Remarques :

- On appelle *poids d'importance* les quantités $w_i = \frac{f(Z_i)}{g(Z_i)}$
- La loi $g$ est appelée *loi instrumentale*

---
class: my-one-page-font
# Echantillonnage préférentiel

**Exemple** : calculer $\mathbb{P}(X>10)$ pour $X \sim \mathcal{E}(1)$.
--

```{r, results='hold'}
1 - pexp(10) # vraie valeur calculée à l'aide de la fonction de répartition
mean(rexp(1000)>10) # estimation par Monte Carlo naïf
```
--

L'estimateur est nul ... on n'a peut-être pas eu de chance sur ce tirage de 1000 ? Et si on ré-essayait ?


---
# Echantillonnage préférentiel

On tire 100 fois un échantillon de taille $n=1000$ et on regarde ce que vaut l'estimateur par Monte Carlo sur chacun de ces 100 tirages.

```{r}
repet_1000 <- sapply(1:100,FUN = function(i){mean(rexp(1000)>10)})
```

--

```{r echo=FALSE, fig.align='center',fig.height=4,fig.width=4, fig.retina=2}
hist(repet_1000,xlab="",main="",ylim=c(0,100))
```
&rarr; l'estimateur vaut presque tout le temps 0 (96 fois sur 100), le reste du temps il vaut 0.001 (dans ces 4 cas, cela veut dire qu'une seule observation sur les 1000 du tirage était plus grande que 10).

---
# Echantillonnage préférentiel


Que se passe t-il si on augmente la taille des échantillons ?

```{r}
repet_1000 <- sapply(1:100,FUN = function(i){mean(rexp(1000)>10)})
repet_10000 <- sapply(1:100,FUN = function(i){mean(rexp(10000)>10)})
repet_100000 <- sapply(1:100,FUN = function(i){mean(rexp(100000)>10)})
```

```{r echo=FALSE, fig.height=4,fig.width=12, fig.retina=2}
par(mfrow=c(1,3))
hist(repet_1000,main="n=1000",xlab="",freq=F)
hist(repet_10000,main="n=10000",xlab="",freq=F)
hist(repet_100000,main="n=100000",xlab="",freq=F)
```

&rarr; on obtient des résultats "raisonnables" quand $n=100 000$, mais la variance reste très élevée.

---
# Echantillonnage préférentiel

En prenant comme loi instrumentale $\mathcal{E}(1/10)$, on peut ré-écrire :

$$
\begin{eqnarray}
\mathbb{P}(X>10) & = & \mathbb{E}(\mathbf{1}_{X>10}) \quad \text{où } X \sim \mathcal{E}(1)\\
                 & = & \int \mathbf{1}_{x>10} f(x)dx \quad \text{où } \ f(x) = e^{-x}\mathbf{1}_{x>0}\\
                 & = & \int \mathbf{1}_{x>10} \frac{f(x)}{g(x)} g(x)dx \quad \text{où} \ g(x) = \frac{1}{10}e^{-x/10}\mathbf{1}_{x>0} \\
                 & = & \int \mathbf{1}_{x>10} \frac{10 \ e^{-x}}{e^{-x/10}} g(x)dx \quad \text{où} \ g(x) = \frac{1}{10}e^{-x/10}\mathbf{1}_{x>0} \\
                 & = & \mathbb{E}\left(\mathbf{1}_{X>10} \ 10 \ e^{-9X/10}\right) \quad \text{où } X \sim \mathcal{E}(1/10)
\end{eqnarray}
$$
En utilisant Monte-Carlo, on approche cette espérance à l'aide de la moyenne empirique d'un échantillon $Z_1,\dots,Z_n$ de taille $n$ de v.a. iid de loi exponentielle de paramètre 1/10.

$$\hat{p} = \frac{1}{n} \sum_{i=1}^n 10 \ e^{-0.9 Z_i} \mathbf{1}_{Z_i > 10}$$

---
# Echantillonnage préférentiel

Code correspondant (pour un tirage, donc une estimation) :

```{r}
Z <- rexp(n,1/10)
w <- 10*exp(-9*Z/10)  # ou plus généralement : dexp(Z,1)/dexp(Z,1/10)
mean(w*(Z>10))
```

--

On fait plusieurs tirages (plusieurs estimations de $p$) :

```{r, echo=F}
repet_1000 <- sapply(1:100,FUN = function(i){Z<-rexp(1000,1/10);mean(10*exp(-0.9*Z)*(Z>10))})
```

```{r, echo=F, fig.retina=2, fig.height=5, fig.width=5, fig.align='center'}
hist(repet_1000,main="n=1000",xlab="",freq=F)
```

C'est beaucoup mieux qu'avec la loi "naïve" !

---
# Echantillonnage préférentiel

Avec la loi $\mathcal{E}(1)$ :
```{r, echo=FALSE}
repet_1000 <- sapply(1:100,FUN = function(i){mean(rexp(1000)>10)})
repet_10000 <- sapply(1:100,FUN = function(i){mean(rexp(10000)>10)})
repet_100000 <- sapply(1:100,FUN = function(i){mean(rexp(100000)>10)})
```

```{r echo=FALSE, fig.height=3,fig.width=12, fig.retina=2}
ptrue <- 1-pexp(10)
par(mfrow=c(1,3))
hist(repet_1000,main="n=1000",xlab="",freq=F)
abline(v=ptrue,col="red",lwd=2)
hist(repet_10000,main="n=10000",xlab="",freq=F)
abline(v=ptrue,col="red",lwd=2)
hist(repet_100000,main="n=100000",xlab="",freq=F)
abline(v=ptrue,col="red",lwd=2)
```
Avec la loi $\mathcal{E}(1/10)$ :

```{r, echo=FALSE}
repet_1000 <- sapply(1:100,FUN = function(i){Z<-rexp(1000,1/10);mean(10*exp(-0.9*Z)*(Z>10))})
repet_10000 <- sapply(1:100,FUN = function(i){Z<-rexp(10000,1/10);mean(10*exp(-0.9*Z)*(Z>10))})
repet_100000 <- sapply(1:100,FUN = function(i){Z<-rexp(100000,1/10);mean(10*exp(-0.9*Z)*(Z>10))})
```

```{r, echo=FALSE, fig.height=3,fig.width=12, fig.retina=2}
par(mfrow=c(1,3))
hist(repet_1000,main="n=1000",xlab="",freq=F)
abline(v=ptrue,col="red",lwd=2)
hist(repet_10000,main="n=10000",xlab="",freq=F,xlim=c(3e-5,6e-5))
abline(v=ptrue,col="red",lwd=2)
hist(repet_100000,main="n=100000",xlab="",freq=F,xlim=c(3e-5,6e-5))
abline(v=ptrue,col="red",lwd=2)
```

<span style="color:#16A085">**fin du cours 5**</span>

---
name: c6
# Echantillonnage préférentiel

**Rappel :**

On cherche à estimer $\mu = \int h(x)f(x)dx$ par :
$$\tilde{\mu} = \frac{1}{n} \sum_{i=1}^n h(Z_i) \frac{f(Z_i)}{g(Z_i)} = \frac{1}{n} \sum_{i=1}^n w_i h(Z_i),$$
avec $Z_1,\dots,Z_n$ un échantillon i.i.d. de loi $g$.

Propriétés : (si $\text{supp}(f) \subset \text{supp}(g)$)

- estimateur sans biais 
- fortement consistant

Remarques :

- On appelle *poids d'importance* les quantités $w_i = \frac{f(Z_i)}{g(Z_i)}$
- La loi $g$ est appelée *loi instrumentale*

---
# Choix de la loi instrumentale

- Calcul de la variance de $\tilde{\mu}$

--

- Variance finie si le ratio $f/g$ est borné (i.e. queues de distribution de $g$ plus lourdes)

--

- Choix optimal pour $g$ &rarr; celui qui minimise cette variance.

--

- En pratique : choisir $g$ telle que $|h|f/g$ soit (presque) constant, et de variance finie.



---
class: my-one-page-font
# Choix de la loi instrumentale

 **Exemple** : approcher $\int x^2 e^{-x^2/2} dx = \sqrt{2\pi} \ \mathbb{E}(X^2)$ avec $X \sim \mathcal{N}(0,1)$ (vraie valeur : $\sqrt{2 \pi} =$ `r sqrt(2*pi)`)
 
- avec la loi instrumentale $\mathcal{N}(0,0.5^2)$. 
 
--

```{r, echo=F, warning=F, message=F}
set.seed(123)
```

```{r}
n <- 10000
Z1 <- rnorm(n,0,sd=0.5)
w1 <- dnorm(Z1,0,1)/dnorm(Z1,0,0.5)
print(paste0("Estimation : ",sqrt(2*pi)*mean(Z1^2*w1),", variance : ",var(Z1^2*w1)/n))
```
--

 - avec la loi instrumentale $\mathcal{N}(0,1.2^2)$ :
```{r}
n <- 10000
Z2 <- rnorm(n,0,sd=1.2)
w2 <- dnorm(Z2,0,1)/dnorm(Z2,0,1.2)
print(paste0("Estimation : ",sqrt(2*pi)*mean(Z2^2*w2),", variance : ",var(Z2^2*w2)/n))
```


---
class: my-one-page-font
# Choix de la loi instrumentale

Si le ratio $f/g$ n'est pas borné, certains poids d'importance peuvent être *trop élevés* ce qui donne trop de poids à certaines observations extrêmes.

```{r, echo=F, fig.retina=2, fig.width=8,fig.height=3.4, fig.align='center'}
dw <- data.frame(w=c(w1,w2),type=rep(c("cas N(0,0.5^2)","cas N(0,1.2^2)"),each=n))
ggplot(data=dw,aes(x=type,y=w)) + geom_boxplot(fill="lightgrey",width=0.5) + xlab("") + facet_wrap(~type,scales = "free")
```

```{r, echo=F, fig.retina=2, fig.height=3.4,fig.width=8,fig.env='figure',strip.white=T,fig.align='center'}
dw <- data.frame(w=c(w1,w2),Z=c(Z1,Z2),type=rep(c("cas N(0,0.5^2)","cas N(0,1.2^2)"),each=n))
ggplot(data=dw,aes(x=Z,y=w)) + geom_point() + xlab("") + facet_wrap(~type,scales = "free") + xlab("valeurs échantillonnées") + ylab("poids d'importance")
```


---
# Version auto-normalisée

On peut définir une version *auto-normalisée* de l'estimateur :

$$\bar{\mu} = \frac{\sum_{i=1}^n w_i h(Z_i)}{\sum_{i=1}^n w_i}$$
--
 
```{r, fig.width=9,fig.height=4,fig.retina=2, fig.align='center', echo=FALSE}
dw <- data.frame(w=c(w1,w2,w1/sum(w1)),type=rep(c("cas N(0,0.5^2)","cas N(0,1.2^2)","cas N(0,0.5^2) auto-normalisé"),each=n))
ggplot(data=dw,aes(x=type,y=w)) + geom_boxplot(fill="lightgrey",width=0.5) + xlab("") + facet_wrap(~type,scales = "free")
```

--

 - estimateur **biaisé**
 - fortement consistant
 - variance plus faible dans certains cas



---
# Version auto-normalisée
  
  - Peut s'utiliser lorsque $f/g$ est connue à une constante multiplicative près
 
--
 
 - Permet également de *simuler* selon la loi $f$ : il suffit d'échantillonner selon une loi discrète à support sur l'ensemble des $\{Z_1,\dots,Z_n\}$, où la probabilité de tirer la valeur $Z_i$ est $w_i /\sum_j w_j$.

```{r}
Z <- rnorm(10000,0,sd=1.5)
w2 <- dnorm(Z,0,1)/dnorm(Z,0,1.5)
X <- sample(Z, size = 2000, prob = w2/sum(w2))
```

```{r, echo=FALSE, fig.width=10, fig.height=4, fig.retina=2, fig.align='center'}
par(mfrow=c(1,2))
hist(Z, freq=FALSE,main="",ylim=c(0,0.4))
lines(seq(-3,3,0.01),dnorm(seq(-3,3,0.01),0,1),col="red",lwd=2)
hist(X, freq=FALSE,main="",ylim=c(0,0.4))
lines(seq(-3,3,0.01),dnorm(seq(-3,3,0.01),0,1),col="red",lwd=2)
```

---
# Résumé du chapitre

Dans ce chapitre, on a vu comment approcher des intégrales par des méthodes stochastiques :

- l'approche par Monte Carlo "simple"
- l'échantillonnage préférentiel

Ces méthodes s'utilisent pour approcher une intégrale qui peut s'écrire comme une espérance :
  - soit parce qu'elle est déjà sous la forme $\int h(x) f(x) dx$ avec $f$ densité de probabilité
  - soit en faisant apparaître une densité $f$ 

Le choix de la loi d'échantillonnage a un impact sur la **variance** de l'estimation


<span style="color:#16A085">**fin du cours 6**</span>


---
name: c7
class: inverse, middle, center

# Modèles de mélange

<!-- ---
# Introduction

Qu'est-ce qu'un .green[**modèle de mélange**] ?

- c'est un modèle statistique, i.e. un modèle que l'on peut appliquer à un échantillon d'observations

- qui suppose que les données observées suivent une *loi mélange*, c'est-à-dire :
 - l'échantillon est constitué de $K$ sous-populations
 - chaque sous-population suit sa propre distribution
 
 
Les modèles de mélange permettent :
 - de faire de la **classification**
 - de modéliser des jeux de données plus complexes qu'avec une approche "classique" (distribution multi-modale, asymétrique, ...)
-->

---
# Introduction -- exemple 1

- Longueur des ailes de 381 passereaux
```{r, echo=F, fig.retina=2, fig.width=6,fig.height=5, fig.align='center'}
d1 <- data.frame(Longueur=c(seq(82,96,1),98),Freq=c(5,3,12,36,55,45,21,13,15,34,59,48,16,12,6,1))
d1v <- as.vector(rep(d1$Longueur, d1$Freq))
hist(d1v,breaks=16,main="",freq=F,xlab="Longueur des ailes")
```

--

- On distingue deux sous-populations

--

- En fait mâles et femelles sont mélangés, ils n'ont pas été distingués lors des mesures

--

- Si on savait quels étaient les individus mâles et les individus femelles, on pourrait faire l'analyse séparément par sexe. 

---
# Introduction -- exemple 2

- Taux de chlorure dans le sang de 543 patients.

```{r, echo=F, fig.retina=2, fig.width=10,fig.height=5, fig.align='center'}
d2 <- data.frame(Taux=c(seq(88,109,1),111,113,115),Freq=c(2,3,4,5,7,5,13,13,28,36,40,72,68,80,47,43,33,19,6,6,4,5,2,1,1))
d2v <- as.vector(rep(d2$Taux, d2$Freq))
par(mfrow=c(1,2))
hist(d2v,breaks=16,main="",freq=F,xlab="Taux de chlorure")
mu2 <- sum(d2$Taux*d2$Freq)/sum(d2$Freq)
sd2 <- sum((d2$Taux^2)*d2$Freq)/sum(d2$Freq) - mu2^2
curve(dnorm(x,mu2,sqrt(sd2)),add=T)
qqnorm((d2v-mu2)/sqrt(sd2),pch=16)
abline(a=0,b=1)
```

- L'ajustement à une loi normale semble correct sur l'histogramme, mais le QQ-plot indique la présence d'outliers mal pris en compte par un modèle gaussien

- Population constituée d'individus sains et d'individus malades, en proportions inconnues 

---
# Loi mélange

On dit qu'un échantillon i.i.d. $(X_1,\dots,X_n)$ suit un **modèle de mélange** si :
 1. la population est structurée en $K$ sous-groupes
 2. au sein du sous-groupe $C_k$, $k=1,\dots,K$, les individus suivent une loi paramétrique $f_k$ dont les paramètres $\theta_k$ dépendent du groupe
 
<!--
```{r, echo=F, fig.retina=2, fig.width=7, fig.height=5, fig.align='center'}
#ggplot(data=iris,aes(x=Sepal.Length,y=Petal.Length,col=Species)) + geom_point() + scale_color_discrete("Espèce") + xlab("Longueur des sépales") + ylab("Longueur des pétales")
```
-->

</br>
--
Pour modéliser une loi mélange :
 - on introduit $Z_i$, $i=1,\dots,n$ telle que $Z_i = k$ ssi l'individu $i$ appartient au groupe $k$
 - sachant que l'individu $i$ est dans le groupe $k$, on peut définir sa loi $f_k(\cdot \mid \theta_k)$


</br>
--
Autrement dit, on a le modèle suivant :

$$ 
\begin{cases}
\mathbb{P}(Z_i = k) & = & p_k\\
X_i \mid Z_i = k & \sim & f_k(\cdot \mid \theta_k)
\end{cases}
$$

---
# Loi mélange

> La loi *marginale* de $X_i$ est 
 $$X_i \sim p_1f_1(\cdot \mid \theta_1)  + \dots + p_K f_K(\cdot \mid \theta_K) $$

On note $p = (p_1,\dots,p_K)$, $\theta=(\theta_1,\dots,\theta_K)$ et $\phi = (p,\theta)$.

--

- Si les lois $f_k$ sont discrètes, la loi de $X_i$ l'est aussi
- Si les lois $f_k$ sont à densité, la loi de $X_i$ l'est aussi

- Souvent, les lois $f_1,\dots,f_K$ appartiennent à la même famille de lois, on note alors $f$ pour simplifier.

- Le nombre de composantes est appelé **ordre du mélange**
- La variable $Z$ est appelé **variable latente** ou **variable cachée**



---
# Loi mélange

Dans un modèle de mélange, on ne connaît pas les groupes :
 - on ne sait pas comment sont réparties les observations dans les groupes
 - parfois, on ne sait même pas combien il y a de groupes

L'objectif d'un tel modèle peut-être :
 - de prendre en compte une structuration en sous-populations dans l'analyse des données
 - d'identifier les sous-populations (&rarr; clustering)

---
# Lois mélange : une grande diversité

```{r, echo=F, fig.retina=5, fig.height=9, fig.width=14}
densmel <- function(x,p,mu,sigma){
  s <- sapply(1:(length(p)),FUN=function(i){p[i]*dnorm(x,mu[i],sigma[i])})
  return(rowSums(s))
}

xx <- seq(-5,5,0.01)
y1 <- densmel(xx,c(1,0,0),c(0,0,0),c(1,1,1))
y2 <- densmel(xx,c(1/5,1/5,3/5),c(0,1/2,13/15),c(1,(2/3),5/9))
y3 <- densmel(xx,rep(1/8,8),3*((2/3)^(1:8)-1),(2/3)^(1:8))
y4 <- densmel(xx,c(2/3,1/3),c(0,0),c(1,1/10))
y5 <- densmel(xx,c(1/10,9/10),c(0,0),c(1,1/10))
y6 <- densmel(xx,c(1/2,1/2),c(-1,1),c(2/3,2/3))
y7 <- densmel(xx,c(3/4,1/4),c(-3/2,3/2),c(1/2,1/2))
y8 <- densmel(xx,c(3/4,1/4),c(0,3/2),c(1,1/3))
y9 <- densmel(xx,c(9/20,9/20,1/10),c(-6/5,6/5,0),c(3/5,3/5,1/4))
y10 <- densmel(xx,c(49/100,49/100,rep(1/350,7)),c(-1,1,((1:7)-3)/2),c(2/3,2/3,rep(1/100,7)))
y11 <- densmel(xx,c(1/2,2^(1-seq(-2,2,1))/31),c(0,seq(-2,2,1)+1/2),c(1,2^seq(2,-2,-1)/10))
y12 <- densmel(xx,(2^(5-seq(0,5,1))/63),(65-96*(1/2)^seq(0,5,1))/21,(32/63)/2^seq(0,5,1))


gr <- c("a) N(0,1) standard","b) Unimodale asymétrique","c) Fortement asymétrique",
        "d) Unimodale leptocurtique","e) Avec outlier","f) Bimodale","g) Bimodale séparée",
        "h) Bimodale asymétrique","i) Trimodale","j) Griffe","k) Griffe asymétrique","l) Peigne")
dmel <- data.frame(x=rep(xx,12),y=c(y1,y2,y3,y4,y5,y6,y7,y8,y9,y10,y11,y12),
                   group=rep(gr,each=length(xx)))

ggplot(data=dmel,aes(x=x,y=y)) + geom_line() + facet_wrap(~group,scales = "free")

```

(*source : T. Rebafka*)

---
# <img src="laptop-code-solid.svg" width="45" height="45" align="center"> Exercices

1. > Illustration d'un mélange de:
 - lois Gaussiennes
 - lois de Poisson
 - lois exponentielles


2. > En notant $\mu_k$ (resp. $\sigma_k^2$) l'espérance (resp. la variance) de la loi de la $k$-ème composante (i.e. de la loi $f_k$), calculer l'espérance et la variance de la loi mélange.


**(Exercices 1 et 2 de la fiche 3)**

<!-- </br>
</br>
</br>
<span style="color:#16A085">**fin du cours 7**</span> -->


---
# Loi mélange
  
Revenons à nos observations :
  
```{r, echo=F, fig.retina=2, fig.width=9, fig.height=5, fig.align='center'}
#dlong <- iris %>% select(Petal.Length,Sepal.Length,Species) %>% pivot_longer(c(Sepal.Length,Petal.Length))
#dlong$name <- factor(dlong$name,levels=c("Sepal.Length","Petal.Length"),labels=c("Longueur des sépales","Longueur des pétales"))
#ggplot(data=dlong,aes(x=value)) + geom_histogram(bins=16,fill="grey") + facet_wrap(~name,scales = "free_x")
ggplot(data=NULL,aes(x=d1v)) + geom_histogram(bins=16,fill="grey",col="black")
```


**.center[.orange[Peut-on identifier les différentes composantes du mélange ? ]]**
  
---
# Problématiques
  
Plusieurs questions à aborder :
  
1. Identifiabilité du modèle de mélange

2. Estimation des paramètres

3. Choix du nombre de classes


---
# Identifiabilité
  
Qu'est-ce que l'identifiabilité ?
  
--
  
> **Définition :**
> Une famille de distributions, paramétrée par $\theta \in \Theta$, et de densité $f(\cdot ; \theta)$ est dite .orange[**identifiable**] ssi :
  
> $$f(x; \theta) = f(x; \theta') \ \text{presque sûrement} \Rightarrow \theta = \theta'$$

</br>
Autrement dit, s'il existe deux jeux de paramètres $\theta$ et $\theta'$ tels que les densités $f(x; \theta)$ et $f(x; \theta')$ sont identiques presque sûrement, alors la famille *n'est pas identifiable*.

--

&rarr; .red[**Est-ce grave ?**]

- si deux jeux de paramètres différents donnent la même loi, alors il est **impossible** d'estimer sans ambiguité les paramètres du modèle.
- cela augmente l'incertitude sur l'estimation, voire cela rend l'estimation caduque.

---
# Identifiabilité

Quelques exemples :
- de lois identifiables :
  - lois Gaussiennes,
  - lois exponentielles,
  - lois de Poisson,
  - la plupart des lois usuelles !

- de non identifiabilité :
  - $X = \lambda Y$, avec $Y \sim \mathcal{N}(0,1)$ et $\lambda \in \mathbb{R}$
  - les modèles de mélange !

---
# Identifiabilité

</br>
Il existe différentes situations/raisons pouvant mener à la non identifiabilité d'un modèle de mélange :

1. la loi mélange est invariante par permutation des groupes (.orange[**problème de ré-étiquetage** ou *label switching*])

2. la loi mélange est inchangée si un groupe est divisé en sous-groupes de même distribution (.orange[**problème de surdimensionnement**])

3. la loi mélange peut hériter des problèmes d'identifiabilité des lois la composant, ou de leur mélange 

---
# Invariance par permutation des groupes

- La numérotation des groupes est **arbitraire**
- On peut donc permuter les étiquettes de groupes, et obtenir la même loi mélange
- Exemple : les lois mélange $0.2 \ \mathcal{N}(0,1) + 0.8\ \mathcal{N}(1,2)$ et $0.8\ \mathcal{N}(1,2) + 0.2 \ \mathcal{N}(0,1)$ sont identiques.

- En fait dans l'écriture :
$$ 
\begin{cases}
\mathbb{P}(Z_i = k) & = & p_k\\
X_i \mid Z_i = k & \sim & f_k(\cdot \mid \theta_k)
\end{cases}
$$
le choix du numéro de groupe n'a pas d'importance.

- La seule chose qui importe est d'associer le "bon" $p_k$ avec le "bon" $\theta_k$. 

- **La loi mélange de paramètres $(p_1,\dots,p_K,\theta_1,\dots,\theta_K)$ est identique à la loi mélange $(p_{\sigma(1)},\dots,p_{\sigma(K)},\theta_{\sigma(1)},\dots,\theta_{\sigma(K)})$, où $\sigma(\cdot)$ est une permutation de $\{1,\dots,K\}$.**

*(Pour vous en convaincre, vous pouvez utiliser le code de l'exercice 1 de la fiche 3 en permutant les groupes)*

---
# Invariance par permutation des groupes

Que faire ?

&rarr; on peut ajouter **des contraintes** sur les paramètres :

- imposer par exemple $\theta_1 \leq \theta_2 \leq \dots \leq \theta_K$

  - .green[**Avantage**] : si $\theta \in \mathbf{R}$, cela garantit l'identifiabilité
  - .red[**Inconvénients**] : 
      - si $\theta_k \in \mathbf{R}^q$, comment définir un ordre ?
      - l'ajout de ces contraintes complique la tâche d'estimation (optimisation sous contraintes)
  
--

- en pratique, on se contente de la notion d'*identifiabilité à permutation des paramètres près*

<span style="color:#16A085">**fin du cours 7**</span>





---
name: c8
# Sur-dimensionnement

- Le modèle de mélange tel qu'il a été défini n'est pas identifiable :
  - il suffit d'ajouter une $K+1$-ème composante de poids nul 
  - ou de "couper" une des composantes en deux nouvelles composantes, avec les mêmes paramètres $\theta$
--

- Ex. : la loi mélange  $0.3 \mathcal{N}(0,1) + 0.7 \mathcal{N}(1,2)$
 est identique à la loi mélange
 $$0.3 \mathcal{N}(0,1) + 0.7 \mathcal{N}(1,2) + 0 \ \mathcal{N}(5,10)$$ 
 et à la loi mélange
 $$0.3 \mathcal{N}(0,1) + 0.4 \mathcal{N}(1,2) + 0.3 \mathcal{N}(1,2)$$

- On aimerait obtenir $K$ populations **distinctes**.

&rarr; On peut alors ajouter les contraintes :

$$p_k > 0, \forall \ k, \quad \theta_k \neq \theta_{k'},  \forall \ k \neq k'$$

---
# Identifiabilité : en résumé

1. Sous les contraintes :
$$p_k > 0, \forall \ k, \quad \theta_k \neq \theta_{k'},  \forall \ k \neq k'$$
 &rarr; on peut obtenir l'identifiabilité *à permutation des paramètres près*
  - pour la plupart des modèles de mélange 
  - **condition suffisante** : la famille de lois composant le mélange est .orange[linéairement indépendante]
  - c'est le cas de la plupart des lois usuelles (Gaussiennes, Exponentielles, Gamma, Poisson, ...)
  
2. Sous les contraintes supplémentaires (lorsqu'elles sont possibles) :
  $$\theta_1 < \dots < \theta_K$$
  &rarr; on peut obtenir l'identifiabilité "forte"
 
.red[**Même sous ces deux types de contraintes, il existe des lois mélange non identifiables, par ex. les mélanges de Bernoulli ou certains mélanges de lois uniformes**]

---
# <img src="pen-to-square-regular.svg" width="45" height="45" align="center"> Exercices

1. > Montrer qu'un mélange de lois de Bernoulli n'est pas identifiable.
2. > Quelle est la densité de probabilité des deux lois mélanges suivantes :
	$$\frac{1}{2} \mathcal{U}([-2,1]) + \frac{1}{2} \mathcal{U}([-1,2]),$$
	$$\frac{1}{3} \mathcal{U}([-1,1]) + \frac{2}{3} \mathcal{U}([-2,2]).$$
	Qu'en conclut-on ?
3. > Montrer qu'une loi uniforme sur un segment $[a,b]$ peut toujours s'écrire comme un mélange de lois uniformes à supports disjoints.	


**(Exercice 3 de la fiche 3)**


---
# Estimation

Soit $X_1, \dots, X_n$  i.i.d. de loi marginale :
$$ f(x;\theta) = \sum_{k=1}^K p_k f_k(x;\theta_k).$$
**Objectif** : estimer $\theta = (\theta_1,\dots,\theta_K)$, les paramètres des lois du mélange, et également $p=(p_1,\dots,p_K)^t$, les proportions du mélange.

On note $\mathbf{X} = (X_1,\dots,X_n)$. La log-vraisemblance est :

$$\begin{align}
\ell(\phi;\mathbf{X}) & = \sum_{i=1}^n \ln f(X_i;\phi) \\
     									& = \sum_{i=1}^n \ln \left( \sum_{k=1}^K p_k f_k(X_i;\phi) \right).
\end{align}$$

**Problème** : il n'existe pas de solution analytique au problème de maximisation de $\ell(\phi;\mathbf{X})$ car les $p_k$ ne sont pas estimables facilement

---
# Estimation

- Que se passerait-il si on observait aussi $(Z_1,\dots,Z_n)$ ?
- Notre échantillon serait alors constitué des couples $(X_i,Z_i), i=1,\dots,n$
- On parle de **données complètes**
- La loi jointe de $(X_i,Z_i)$ est :

--

$$f(x,z;\phi) = \prod_{k=1}^K \left(p_k f_k(x;\phi)\right)^{\mathbf{1}_{z=k}}$$

--

La log-vraisemblance serait alors :
$$\begin{align}
\ \ell(\phi;\mathbf{X}, \mathbf{Z})&  =\sum_{i=1}^n \ln f(X_i,Z_i;\phi) \\
& = \sum_{i=1}^n  \sum_{k=1}^K \mathbb{1}_{Z_{i}=k} \ln \left(p_k f_k(X_i;\phi) \right)
\end{align}$$

---
# Estimation

- Les deux quantités précédentes vont jouer un rôle particulier dans l'estimation
- Notations :
  - la log-vraisemblance des **données observées** est notée $\ell_{\text{obs}}$ :
$$\ell_{\text{obs}}(\phi;\mathbf{X}) = \sum_{i=1}^n \ln \left( \sum_{k=1}^K p_k f_k(X_i;\phi) \right)$$
  - la log-vraisemblance des **données complètes** est notée $\ell_{\text{comp}}$ :

$$\ell_{\text{comp}}(\phi;\mathbf{X},\mathbf{Z}) = \sum_{i=1}^n  \sum_{k=1}^K \mathbf{1}_{Z_i=k} \ln \left(p_k f_k(X_i;\phi) \right)$$
- La log-vraisemblance complète est plus facile à manipuler car on a "fait sortir" la somme du log.

- On va exploiter i) le fait que $\ell_{\text{comp}}$ est plus facile à maximiser et ii) le lien entre ces deux fonctions, pour maximiser $\ell_{\text{obs}}$.

.center[.orange[&rarr; **c'est le principe de l'algorithme EM**]]

---
# Algorithme EM

- Permet d'obtenir un maximum .red[**LOCAL**] de la log-vraisemblance en présence de variables latentes
	
- Il a été introduit par Dempster, Laird et Rubin en 1977.

- Particulièrement adapté aux cas où la vraisemblance complète s'écrit plus simplement que la vraisemblance observée

- Repose sur l'idée suivante : en présence de données manquantes, une première intuition est **d'estimer ou de remplacer** ces données manquantes, puis d'estimer les paramètres du modèle à l'aide des **données "augmentées"**

---
# Algorithme EM

On a : $\quad \quad \quad \quad \quad \quad \quad \quad \quad f(x,z;\phi) = f(x \mid z;\phi) f(z;\phi)$

mais aussi : $\quad \quad \ \ \ \quad \quad \quad \quad f(x,z;\phi) = f(x;\phi) f(z \mid x;\phi)$

et donc $\quad \quad \quad \ \quad \quad \quad \quad \ln f(x,z;\phi) = \ln f(x;\phi) + \ln f(z \mid x;\phi)$

D'où :

$$\newcommand{\green}[1]{\color{green}{#1}}
\newcommand{\orange}[1]{\color{orange}{#1}}
\newcommand{\red}[1]{\color{red}{#1}}
\newcommand{\blue}[1]{\color{blue}{#1}}$$

$$\begin{align}
\sum_{i=1}^n \ln f(X_i,Z_i;\phi) & = \sum_{i=1}^n \ln f(X_i;\phi) + \sum_{i=1}^n \ln f(Z_i \mid X_i;\phi) \\
\underbrace{\orange{\sum_{i=1}^n \ln f(X_i,Z_i;\phi)}}_{\text{vrais.} \orange{\text{ complète}}} & = \underbrace{\green{\sum_{i=1}^n \ln f(X_i;\phi)}}_{\text{vrais.} \green{\text{ observée}}} + \sum_{i=1}^n \ln f(Z_i \mid X_i;\phi) \\
\Rightarrow \ell_{\text{obs}} (\phi;\mathbf{X}) & = \ell_{\text{comp}} (\phi;\mathbf{X},\mathbf{Z}) - \ln f(\mathbf{Z} \mid \mathbf{X}; \phi)
\end{align}$$

---
# Algorithme EM

- Mais $\mathbf{Z}$ est inconnu ...

- L'idée est .orange[**d'intégrer**] l'expression précédente par rapport à la loi conditionnelle de $\mathbf{Z}$ sachant $\mathbf{X}$, afin d'obtenir une expression qui ne dépende plus de $\mathbf{Z}$ (c'est-à-dire, calculer l'.red[**espérance conditionnelle**] sachant $\mathbf{X}$.)

- On se place à l'itération $m+1$ de l'algorithme, avec l'estimation courante $\phi^{(m)}$.

- On va intégrer l'expression : $$\ell_{\text{obs}} (\phi;\mathbf{X}) = \ell_{\text{comp}} (\phi;\mathbf{X},\mathbf{Z}) - \ln f(\mathbf{Z} \mid \mathbf{X}; \phi)$$
par rapport à la loi $f(z \mid x; \phi^{(m)})$, i.e. par rapport à la loi conditionnelle de $\mathbf{Z}$ sachant $\mathbf{X}$ et la valeur actuelle du paramètre $\phi^{(m)}$.

- On note $\mathbb{E}_{\phi^{(m)}}\left[ \cdot \mid \mathbf{X} \right]$ l'espérance par rapport à cette loi. Autrement dit, si on considère une fonction $h(\mathbf{X},\mathbf{Z})$, on a :
$$\mathbb{E}_{\phi^{(m)}}[h(\mathbf{X},\mathbf{Z})\mid \mathbf{X} = x]= \int h(x,z) f(z \mid x; \phi^{(m)}) dz$$

---
# Algorithme EM

- Comme $\ell_{\text{obs}} (\phi;\mathbf{X})$ est $\sigma(\mathbf{X})$-mesurable, on a : $\mathbb{E}_{\phi^{(m)}}[\ell_{\text{obs}} (\phi;\mathbf{X})\mid \mathbf{X}] = \ell_{\text{obs}} (\phi;\mathbf{X}).$ En effet :

$$\begin{align}
\mathbb{E}_{\phi^{(m)}}[\ell_{\text{obs}} (\phi;\mathbf{X})\mid \mathbf{X} = x] & = \int \ell_{\text{obs}} (\phi;x) f(z \mid x; \phi^{(m)}) dz \\
 & = \ell_{\text{obs}} (\phi;x) \int f(z \mid x; \phi^{(m)}) dz \\
 & = \ell_{\text{obs}} (\phi;x)
\end{align}$$

- D'où :
$$\begin{align}
\ell_{\text{obs}} (\phi;\mathbf{X}) &  = 	\orange{\mathbb{E}_{\phi^{(m)}}\left[\ell_{\text{comp}} (\phi;\mathbf{X},\mathbf{Z}) \mid \mathbf{X} \right]}  \ \blue{-  \ \mathbb{E}_{\phi^{(m)}}\left[ \ln f(\mathbf{Z} \mid \mathbf{X}; \phi) \ \middle\vert \ \mathbf{X}\right]} \\
 & = \orange{Q(\phi ; \phi^{(m)})} \ \red{+} \ \blue{H(\phi ; \phi^{(m)})}.
\end{align}$$

- Que peut-on dire de $Q$ et $H$ ?

  1. on montre que $H(\phi ; \phi^{(m)}) -  H(\phi^{(m)} ; \phi^{(m)}) \geq 0$ .red[**&rarr; au tableau**]
  2. on peut alors se contenter d'étudier $Q$
  
---
# Algorithme EM

Idée générale :

- A l'itération $m+1$ de l'algorithme EM, la valeur de $H$ augmente $\rightarrow$ 
il suffit de s'intéresser à la fonction $Q$

- Chaque itération de l'algorithme EM se divise en deux étapes :
  - l'étape d'.orange[**espérance**] (étape E) qui consiste à calculer la quantité $Q(\phi ; \phi^{m})$ en fonction de la valeur courante de l'estimation $\phi^{m}$
	- l'étape de .green[**maximisation**] (étape M), qui consiste à maximiser la fonction $Q$
	
- L'objectif est de maximiser la log-vraisemblance observée en maximisant $Q$. 


---
# Algorithme EM

Description de l'algorithme

1. **initialisation** :  valeur initiale $\phi^{0}$, nombre total d'itérations $M$\;

2. **pour** $m=1,\dots,M$ :
	- .orange[**Etape E (Espérance)**] :	calcul de $Q$
	$$Q(\phi ; \phi^{(m-1)}) =	\mathbb{E}_{\phi^{(m-1)}}\left(\ell_{\text{comp}} (\phi;\mathbf{X},\mathbf{Z}) \mid \mathbf{X} \right)$$
	- .green[**Etape M (Maximisation)**] : maximisation de $Q$
		$$\phi^{(m)} = \arg \max_\phi Q(\phi ; \phi^{(m-1)})$$


<!--
---
# Illustration

</br>
```{r, out.width='1000px', echo = FALSE, fig.align='center'}
magick::image_read_pdf("/home/charlotte/Documents/Enseignement/Lille/M2 MIASHS/fig_EM.pdf",pages=1)
```

---
# Illustration

</br>
```{r, out.width='1500px', echo = FALSE}
magick::image_read_pdf("/home/charlotte/Documents/Enseignement/Lille/M2 MIASHS/fig_EM.pdf",pages=2)
```

---
# Illustration

</br>
```{r, out.width='1500px', echo = FALSE}
magick::image_read_pdf("/home/charlotte/Documents/Enseignement/Lille/M2 MIASHS/fig_EM.pdf",pages=3)
```

---
# Illustration

</br>
```{r, out.width='1500px', echo = FALSE}
magick::image_read_pdf("/home/charlotte/Documents/Enseignement/Lille/M2 MIASHS/fig_EM.pdf",pages=4)
```

---
# Illustration

</br>
```{r, out.width='1500px', echo = FALSE}
magick::image_read_pdf("/home/charlotte/Documents/Enseignement/Lille/M2 MIASHS/fig_EM.pdf",pages=5)
```

---
# Illustration

</br>
```{r, out.width='1500px', echo = FALSE}
magick::image_read_pdf("/home/charlotte/Documents/Enseignement/Lille/M2 MIASHS/fig_EM.pdf",pages=6)
```

---
# Illustration

</br>
```{r, out.width='1500px', echo = FALSE}
magick::image_read_pdf("/home/charlotte/Documents/Enseignement/Lille/M2 MIASHS/fig_EM.pdf",pages=7)
```

---
# Illustration

</br>
```{r, out.width='1500px', echo = FALSE}
magick::image_read_pdf("/home/charlotte/Documents/Enseignement/Lille/M2 MIASHS/fig_EM.pdf",pages=8)
```
 
-->
---
# Illustration

</br>
```{r, out.width='1500px', echo = FALSE}
magick::image_read_pdf("/home/charlotte/Documents/Enseignement/Lille/M2 MIASHS/fig_EM.pdf",pages=9)
```

<span style="color:#16A085">**fin du cours 8**</span>


---
name: c9
# Algorithme EM

Description de l'algorithme

1. **initialisation** :  valeur initiale $\phi^{0}$, nombre total d'itérations $M$\;

2. **pour** $m=1,\dots,M$ :
	- .orange[**Etape E (Espérance)**] :	calcul de $Q$
	$$Q(\phi ; \phi^{(m-1)}) =	\mathbb{E}_{\phi^{(m-1)}}\left(\ell_{\text{comp}} (\phi;\mathbf{X},\mathbf{Z}) \mid \mathbf{X} \right)$$
	- .green[**Etape M (Maximisation)**] : maximisation de $Q$
		$$\phi^{(m)} = \arg \max_\phi Q(\phi ; \phi^{(m-1)})$$


---
# Algorithme EM pour les modèles de mélange

.large[.orange[**Etape E**]] : calcul de $Q$

- Dans le cas du modèle de mélange, les $Z_i$ sont des variables aléatoires discrètes

- On montre (.red[**au tableau**]) que :
 $$Q(\phi ; \phi^{(m-1)})	= \sum_{i=1}^n \sum_{k=1}^K t_k(X_i;\phi^{(m-1)}) \ln p_k  + \sum_{i=1}^n \sum_{k=1}^K t_k(X_i;\phi^{(m-1)}) \ln f_k(X_i;\theta_k)$$
 avec
$$t_k(X_i;\phi^{(m-1)})  = \frac{p_k^{(m-1)} f_k(X_i;\theta_k^{(m-1)})}{\sum_{k=1}^K p_k^{(m-1)} f_k(X_i;\theta_k^{(m-1)})}$$


---
# Algorithme EM pour les modèles de mélange

.large[.green[**Etape M**]] : maximisation de $Q$

- La fonction $Q$ peut se décomposer en deux parties : .blue[une qui dépend de] $\blue{p}$ et .red[une autre qui dépend de] $\red{\theta}$

$$Q(\phi ; \phi^{(m-1)}) = \blue{\sum_{i=1}^n \sum_{k=1}^K t_k(X_i;\phi^{(m-1)}) \ln p_k } + \red{\sum_{i=1}^n \sum_{k=1}^K t_k(X_i;\phi^{(m-1)}) \ln f_k(X_i;\theta_k)}$$

&rarr; on maximise indépendamment pour $\theta$ et pour $p$.

--

Si $f_k$ est la densité Gaussienne, on obtient :
$$\begin{align}
p_k^m & = \frac{1}{n} \sum_{i=1}^n t_k(X_i; \phi^{(m-1)})\\
\mu_k^m  & = \dfrac{\sum_{i=1}^n t_k(X_i; \phi^{m-1})\  X_i}{\sum_{i=1}^n t_k(X_i; \phi^{(m-1)})}\\
\Sigma_k^m  & = \frac{\sum_{i=1}^n t_k(X_i; \phi^{m-1})\  (X_i - \mu_k^m) (X_i - \mu_k^m)^t}{\sum_{i=1}^n t_k(X_i; \phi^{(m-1)})}
\end{align}$$


---
# Quelques remarques

- On répète les étapes E et M jusqu'à convergence de l'algorithme, i.e. jusqu'à stabilisation de la fonction $Q$

--

- $K$ doit être fixé en amont  $\rightarrow$ lancer l'algorithme pour différentes valeurs de $K$ et comparer AIC, BIC, ...

--

- Il existe des variantes .orange[**stochastiques**] de l'algorithme, basés sur une approximation de l'étape E par des méthodes de type Monte Carlo

--

- Chaque itération de l'algorithme EM permet d'augmenter la valeur de la log-vraisemblance
	- sous certaines conditions, l'algorithme .red[converge vers un point stationnaire de la vraisemblance]
	- Ce point stationnaire peut être un maximum local, un maximum global, ou un point selle
	- Dans la pratique on lance plusieurs fois l'algorithme avec des initialisations différentes



---
# Résumé 

- **Objectif** : maximiser la log-vraisemblance *observée* $\ell_{obs}(\phi;\mathbf{X}) = \sum_{i=1}^n \ln f(X_i;\phi)$

--

- **Problème** : cette fonction n'est pas maximisable *explicitement*

--

- **Solution** : utiliser l'algorithme EM :
 - on maximise une *fonction auxiliaire* plus simple à manipuler : (à l'itération $m$)
 $Q(\phi ; \phi^{(m-1)}) =	\mathbb{E}_{\phi^{(m-1)}}\left(\ell_{\text{comp}} (\phi;\mathbf{X},\mathbf{Z}) \mid \mathbf{X} \right)\quad$ (à l'itération $m$)
 - cette fonction est une borne inférieure de $\ell_{obs}(\phi;\mathbf{X})$ &rarr; en augmentant $Q$ on augmente $\ell_{obs}(\phi;\mathbf{X})$

--

- .green[**Avantages**] :
 - sous certaines conditions, permet d'obtenir l'EMV
 - extensions stochastiques pour augmenter son implémentabilité

- .red[**Inconvénients**] :
 - dans le cas général, convergence vers un point stationnaire de la log-vraisemblance (pas forcément le max)
 - nécessite d'expliciter les étapes E et M pour chaque modèle de mélange

---
# Estimation par algorithme EM

Dans le cas Gaussien (conclusions similaires dans le cas général) :

.pull-left[

Modèle de mélange et algorithme EM :
$$\small\begin{align*}
\hat{p}_k 		 & = \frac{\sum_{i=1}^n \red{t_k(X_i; \phi^{M})}}{n}\\
\hat{\mu}_k  & = \frac{\sum_{i=1}^n \red{t_k(X_i; \phi^{M})} X_i}{\sum_{i=1}^n \red{t_k(X_i; \phi^{M})}}\\
\hat{\Sigma}_k  & = \frac{\sum_{i=1}^n \red{t_k(X_i; \phi^{M})} (X_i - \mu_k)^t (X_i - \mu_k)^t}{\sum_{i=1}^n \red{t_k(X_i; \phi^{M})}}
\end{align*}$$
]
.pull-right[
Si les $Z_i$ étaient connues, EMV exact :
$$\small\begin{align*}
\hat{p}_k 		 & = \frac{\sum_{i=1}^n \blue{\mathbf{1}_{Z_i=k}}}{n}\\
\hat{\mu}_k  & = \frac{\sum_{i=1}^n \blue{\mathbf{1}_{Z_i=k}} X_i}{\sum_{i=1}^n \blue{\mathbf{1}_{Z_i=k}}}\\
\hat{\Sigma}_k  & = \frac{\sum_{i=1}^n \blue{\mathbf{1}_{Z_i=k}} (X_i - \mu_k)^t (X_i - \mu_k)^t}{\sum_{i=1}^n \blue{\mathbf{1}_{Z_i=k}}}
\end{align*}$$
]

où $t_k(X_i; \phi^{M}) = \mathbb{P}(Z_i = k \mid X_i; \phi^M)$

--

&rarr; si les $Z_i$ étaient connues, on saurait à quelles classes appartiennent les observations, et on utiliserait, pour estimer les paramètres de la classe $k$, les observations de cette classe

&rarr; dans le cas des modèles de mélange, les $Z_i$ ne sont pas observées : pour le calcul des paramètres associés à la classe $k$, on **pondère** alors chaque observation par la probabilité, calculée à l'itération $m-1$, pour que l'individu correspondant appartienne au groupe $k$, sachant ses observations.




---
# Estimation de la partition

A partir des estimations obtenues en sortie de l'algorithme EM :
- on peut déduire une règle d'affectation des individus dans les différentes classes en utilisant le principe du .blue[Maximum A Posteriori (MAP)] :

$$i \in C_\ell \Leftrightarrow \ell = \arg \max_{k =1,\dots,K} t_k(X_i ; \hat{\phi})$$
où $\hat{\phi}$ est l'estimateur obtenu lorsque l'algorithme a convergé. On affecte donc l'individu $i$ à la classe dont la probabilité a posteriori est la plus élevée.

- On utilise ensuite les estimations obtenues pour $\theta_k$ pour interpréter les classes.


---
# Exemple

Données sur les éruptions du geyser 'Old faithful' du parc Yellowstone (disponibles sour `R` dans la libraire `MASS`):

.pull-left[
```{r, echo=FALSE, fig.retina=2, fig.width=6, fig.height=5, fig.align='center', warning=F, message=F}
library(MASS)
library(mclust)
classif <- Mclust(geyser$waiting,G=2,verbose = F)
geyser$classe <- classif$classification
geyser$tx1 <- classif$z[,1]
geyser$tx2 <- classif$z[,2]
#hist(geyser$waiting,main="Temps entre 2 éruptions",freq=F)
ggplot(data=geyser,aes(x=waiting)) + geom_histogram(bins=20,fill="grey",col="darkgrey") + ylab("") + xlab("Temps entre 2 éruptions") + ggtitle("Histogramme des données")
```
]

---
# Exemple

Données sur les éruptions du geyser 'Old faithful' du parc Yellowstone (disponibles sour `R` dans la libraire `MASS`):

```{r, echo=FALSE, fig.retina=2, fig.width=12, fig.height=5, fig.align='center', warning=F, message=F}
library(MASS)
library(mclust)
classif <- Mclust(geyser$waiting,G=2,verbose = F)
geyser$classe <- classif$classification
geyser$tx1 <- classif$z[,1]
geyser$tx2 <- classif$z[,2]
#hist(geyser$waiting,main="Temps entre 2 éruptions",freq=F)
p1 <- ggplot(data=geyser,aes(x=waiting)) + geom_histogram(bins=20,fill="grey",col="darkgrey") + ylab("") + xlab("Temps entre 2 éruptions") + ggtitle("Histogramme des données")
#plot(geyser$waiting,classif$z[,1],pch=16,main="Probabilité d'être dans la classe 1",col=classif$classification,ylab="")
#abline(h=0.5)
p2 <- ggplot(data=geyser) + geom_point(aes(x=waiting,y=tx1,col=tx1,shape="Classe 1"),size=2) + geom_point(aes(x=waiting,y=tx2,col=tx2,shape="Classe 2"),size=2) + scale_color_gradient("Proba",high="#32CD32",low="red") +  geom_hline(yintercept = 0.5,col="darkgrey") + ylab("") + xlab("Temps entre 2 éruptions") + ggtitle("Probabilité (a posteriori) d'appartenir à chaque classe") + scale_shape_manual("",values=c(19,2))
library(gridExtra)
grid.arrange(p1,p2,nrow=1)
paste0("Poids de chaque composante : ",paste(round(classif$parameters$pro,4),collapse = ", "))
paste0("Moyenne dans chaque groupe : ",paste(round(classif$parameters$mean,2),collapse=", "))
paste0("Variance dans chaque groupe : ",paste(round(classif$parameters$variance$sigmasq,2),collapse=", "))
```

---
# Classification par l'algorithme CEM

- L'algorithme EM a pour objectif principal d'estimer les paramètres du modèle, à savoir $p_k$ et $\theta_k$

- On ne tient pas compte explicitement de l'objectif de classification

- On peut obtenir en pratique des classes qui ne soient pas suffisamment séparées ce qui rend plus difficile l'interprétation des résultats

- Autre approche aussi basée sur un algorithme de type EM a été proposée par Celeux et Govaert (1992), intégrant de façon explicite cet objectif de classification

$\rightarrow$ algorithme .orange[**CEM**] (Classification - EM)

---
# Algorithme CEM

On a :
$$\ell_{\text{obs}} (\phi;\mathbf{X}) = \ell_{\text{comp}} (\phi;\mathbf{X},\mathbf{Z}) - \sum_{i=1}^n \sum_{k=1}^K \mathbf{1}_{Z_i=k} \ln t_k(\mathbf{X}_i; \phi)$$

--

- Si on a une partition idéale, avec des classes parfaitement séparées, alors les $t_k(\mathbf{X}_i; \phi)$ sont binaires (il n'y a pas d'ambiguïté quant au choix de la classe)

- Le deuxième terme ci-dessus est alors .red[**nul**]

- Il s'agit en fait d'un terme d'entropie (fonction $H$) : mesure l'écart entre la partition définie par les $Z_{i}$ et les probabilités a posteriori $t_k(\mathbf{X}_i; \phi)$.

- &rarr; la log-vraisemblance observée et la log-vraisemblance complète sont égales en cas de partition idéale

- L'algorithme CEM cherche alors à maximiser .red[**directement la vraisemblance complète**], au lieu de chercher à maximiser la log-vraisemblance observée à l'aide de la fonction $Q$.

---
# Algorithme CEM

Algorithme CEM

- **initialisation** : valeur initiale $\phi^0$, nombre total d'itérations $M$

- **pour $m=1,\dots,M$** :

  - .orange[**Etape E**] : identique à l'étape E de l'algorithme EM. On obtient
		$$t_k(X_i;\phi^{m-1}) = \frac{p_k^{m-1} f_k(X_i;\theta_k^{m-1})}{\sum_{k=1}^K p_k^{m-1} f_k(X_i;\theta_k^{m-1})}$$

  - .blue[**Etape C (Classification)**] : création de la partition.
		$$Z^m_{i} = \arg \max_{\ell = 1, \dots, K} t_\ell(X_i;\phi^{m-1})$$

  - .green[**Etape M**] : maximisation de la vraisemblance complète
		$$\ell_{\text{comp}} (\phi;\mathbf{X},\mathbf{Z}^m) = \sum_{i=1}^n  \sum_{k=1}^K \mathbf{1}_{Z^m_i=k} \ln  \left(p_k f_k(X_i;\theta_k) \right)$$


---
# L'étape de maximisation

- L'expression de la vraisemblance complète est très proche de celle de la fonction $Q$, avec les $\mathbf{1}_{Z_{i}^{m-1}=k}$ qui remplacent les $t_k(X_i;\phi^{m-1})$:

$$\begin{align*}
Q(\phi ; \phi^{m-1}) & = \sum_{i=1}^n \sum_{k=1}^K t_k(X_i;\phi^{m-1}) \ln p_k  & + & \sum_{i=1}^n \sum_{k=1}^K t_k(X_i;\phi^{m-1}) \ln f_k(X_i;\theta_k) \\
\ell_{\text{comp}} (\phi;\mathbf{X},\mathbf{Z}^m) & = \sum_{i=1}^n  \sum_{k=1}^K \mathbf{1}_{Z^m_i=k} \ln  p_k  & + & \sum_{i=1}^n  \sum_{k=1}^K \mathbf{1}_{Z^m_i=k}  \ln f_k(X_i;\theta_k)
\end{align*}$$

--

- On obtient alors, dans le cas d'un mélange Gaussien:
$$\begin{align*}
p_k^m & = \frac{1}{n} \sum_{i=1}^n \mathbf{1}_{Z^m_i=k} = \frac{n_k^{(m)}}{n}, \quad \text{avec } n_k^{(m)} \text{ le nb d'obs. classées dans le groupe } k\\
\mu_k^m  & = \dfrac{\sum_{i=1}^n \mathbf{1}_{Z^m_i=k}  X_i}{\sum_{i=1}^n \mathbf{1}_{Z^m_i=k}} = \frac{\sum_{i=1}^n \mathbf{1}_{Z^m_i=k}  X_i}{n_k^{(m)}}\\
\Sigma_k^m  & = \frac{\sum_{i=1}^n \mathbf{1}_{Z^m_i=k} (X_i - \mu_k^m) (X_i - \mu_k^m)^t}{\sum_{i=1}^n Z^m_{ik}} = \frac{\sum_{i=1}^n \mathbf{1}_{Z^m_i=k} (X_i - \mu_k^m) (X_i - \mu_k^m)^t}{n_k^{(m)}}.
\end{align*}$$

---
# Quelques remarques

- En pratique, l'algorithme CEM converge souvent .orange[**plus vite**] que l'algorithme EM

- Plus efficace pour réaliser un compromis biais-variance surtout pour les petits échantillons

- Les estimateurs obtenus avec l'algorithme CEM ne sont .red[**pas sans biais**], même asymptotiquement.


---
# Choix du nombre de classes

- Jusqu'à présent, on a considéré que le nombre de classes $K$ était connu et fixé

- En pratique ce n'est pas toujours le cas &rarr; $K$ est également un paramètre du modèle !

- Quelques pistes :
 - identifier le .orange[**nombre de modes**]

 - faire un .orange[**test d'hypothèses**] portant sur le paramètre $K$

 - comparer plusieurs choix de $K$ à l'aide de critères de .orange[**comparaison de modèles**]

---
# Nombre de modes

- En dimension 1 :

```{r, echo=FALSE, fig.retina=2, fig.width=6, fig.height=5, fig.align='center'}
ggplot(data=iris,aes(x=Petal.Length)) + geom_histogram(bins=15,fill="grey",col="darkgrey")
#hist(iris$Petal.Length,breaks=21,main="",xlab="Longueur des pétales",freq=F)
```


---
# Nombre de modes

- En dimension 1 :

```{r, echo=FALSE, fig.retina=2, fig.width=7, fig.height=5, fig.align='center'}
ggplot(data=iris,aes(x=Petal.Length,fill=Species)) + geom_histogram(bins=15,col="darkgrey")
```


- Certains modes peuvent cacher plusieurs composantes
- Certaines densités de mélange sont **unimodales**
- Plus difficile en dimension > 1

&rarr; en cherchant les modes, on obtient un **minorant** du nombre de groupes

---
# Nombre de modes

Sur l'exemple des données du geyser 'Old faithful', comparaison du modèle obtenu en dimension 1 ou en dimension 2
:

```{r, echo=FALSE, fig.retina=2, fig.width=12, fig.height=5, fig.align='center', warning=F, message=F}
classifd2 <- Mclust(geyser[,c(1,2)],verbose = F)
geyser$classed2 <- classifd2$classification
classifd1bis <-  Mclust(geyser$duration,verbose = F)
geyser$classed1bis <- classifd1bis$classification
p1 <- ggplot(geyser,aes(x=waiting,y=0,col=as.factor(classe))) + geom_point() + ylim(c(-0.1,1)) + xlab("Temps entre 2 éruptions") + theme(legend.position = "none") + ggtitle("Classification sur le temps entre éruptions") + ylab("")
p2 <- ggplot(geyser,aes(y=duration,x=0,col=as.factor(classed1bis))) + geom_point() + xlim(c(-0.1,1)) + xlab("Durée de l'éruption") + theme(legend.position = "none") + ggtitle("Classification sur la durée de l'éruption") + ylab("")
p3 <- ggplot(geyser,aes(x=waiting,y=duration,col=as.factor(classed2))) + geom_point() + xlab("Temps entre 2 éruptions") +
  ylab("Durée de l'éruption") + theme(legend.position = "none") + ggtitle("Classification sur les deux variables")
grid.arrange(p1,p2,p3,nrow=1)
```

---
# Test d'hypothèses

- Exemple : tester
$$H_0 : K=K_0 \quad \text{ contre } \quad H_1 : K=K_0+1$$
à l'aide d'un test du rapport de vraisemblances

--

- Pour mettre en place ce test il faut calculer la statistique de test :
$$T_n = -2 \left( \ell_{obs}(\hat{\phi}_0;\mathbb{x}) - \ell_{obs}(\hat{\phi}_1;\mathbb{x}) \right)$$
--

- Il faut donc calculer l'estimateur du maximum de vraisemblance sous $H_0$ et sous $H_1$

--

- Mais, si les données proviennent du modèle sous $H_0$ :
 - pour calculer $\hat{\phi}_1$, il faut ajuster un modèle à $K_0+1$ composantes sur des données dont la loi de mélange contient $K_0$ composantes
 - on a alors un problème d'identifiabilité
 - les conditions de régularité standards permettant d'obtenir la consistance de l'EMV .red[**ne sont pas vérifiées**]

- De même, les résultats standards sur la loi de la statistique du RV ne s'appliquent pas

---
# Test d'hypothèses

Plusieurs auteurs ont étudiés la consistance des estimateurs dans le cas où on ajuste un modèle avec 1 composante "de trop" :
- soit on converge (quand $n$ tend vers $+\infty$) vers deux composantes ayant les mêmes paramètres
- soit on converge vers une composante avec un poids nul

--

Pour la loi (asymptotique) de la statistique de test :
- son expression exacte est difficile voire impossible à exploiter
- on peut utiliser une approche *bootstrap*

---
# Test d'hypothèses

Test $\quad H_0 : K=K_0 \quad \text{ contre } \quad H_1 : K=K_0+1$

**Procédure bootstrap :**

> 1. Ajuster les deux modèles à $K_0$ et $K_0+1$ composantes par maximum de vraisemblance pour obtenir $\hat{\phi}_0=(\hat{p}_1,\dots,\hat{p}_{K_0},\hat{\theta}_1,\dots,\hat{\theta}_{K_0})$, $\hat{\phi}_1$ et $T_n$
> 2. Simuler $B$ jeux de données selon la loi de mélange à $K_0$ composantes en utilisant $\hat{\phi}_{0}$ :
  - simuler $Z_i^{b,*}$, $i=1,\dots,n$ selon la loi $\mathbb{P}(Z_i^{b,*} = k)=\hat{p}_k$
  - conditionnellement à la valeur de $Z_i^{b,*}$, simuler $X_i^{b,*} \mid Z_i^{b,*} = k \sim f_k(\cdot \mid \hat{\theta}_k)$
3. Sur chaque jeu de données bootstrap $(X_1^{b,*},\dots,X_n^{b,*})$:
    - estimer les deux modèles à $K_0$ et $K_0+1$ composantes (i.e. sous $H_0$ et sous $H_1$) par maximum de vraisemblance.
    - calculer la statistique de test bootstrap $T_n^{b,*}$
4. Estimer la $p$-valeur bootstrap du test par $p_{boot} = \frac{1}{B}\sum_{i=1}^B \mathbb{1}_{T_n^* \geq T_n}$


Mise en place dans plusieurs packages `R` : `mclust`, `mixtools`, `mixR`, ...

---
# Méthodes de comparaison de modèles

On lance l'algorithme en faisant varier le nombre de classes, et on compare les résultats à l'aide de différents critères. On note $q$ la dimension du modèle, c'est-à-dire le nombre total de paramètres.

- AIC : $-2 \ \green{\ell_{\text{obs}} (\hat{\phi} ; \mathbf{X})} + 2 q$

- BIC : $-2 \ \green{\ell_{\text{obs}} (\hat{\phi} ; \mathbf{X})} + q \ln n$

-	ICL : $-2 \ \orange{\ell_{\text{comp}} (\hat{\phi} ; \mathbf{X}, \hat{\mathbf{Z}})} + q \ln n$

- On a ICL = BIC $\blue{- 2 \sum_{i=1}^n \sum_{k=1}^K  \mathbf{1}_{\hat{Z}_i=k} \ln t_k(X_i;\hat{\phi})}$ (BIC + terme d'entropie)

- L'entropie a tendance à augmenter quand le nombre de classes augmente

- ICL sélectionne des modèles avec **moins de classes** que BIC &rarr; ce critère serait meilleur pour sélectionner le nombre de classes dans une classification.

---
# Exemple

Données du geyser 'Old faithful', comparaison de deux modèles en dimension 1 :

.pull-left[
```{r, echo=FALSE, fig.retina=2, fig.width=5, fig.height=5, fig.align='center', warning=F, message=F}
ggplot(geyser,aes(x=waiting,y=0,col=as.factor(classe))) + geom_point() + ylim(c(-0.1,1)) + xlab("Temps entre 2 éruptions") + theme(legend.position = "none") + ggtitle("Modèle à 2 classes") + ylab("")
paste("BIC = ",-classif$bic)
paste("ICL = ",-classif$icl)
paste("Entropie : ",(-classif$icl+classif$bic)/2)
```
]
.pull-right[
```{r, echo=FALSE, fig.retina=2, fig.width=5, fig.height=5, fig.align='center', warning=F, message=F}
classif2 <- Mclust(geyser$waiting,G=3,verbose = F)
geyser$classe2 <- classif2$classification
geyser$tx21 <- classif2$z[,1]
geyser$tx22 <- classif2$z[,2]
geyser$tx23 <- classif2$z[,3]
ggplot(geyser,aes(x=waiting,y=0,col=as.factor(classe2))) + geom_point() + ylim(c(-0.1,1)) + xlab("Temps entre 2 éruptions") + theme(legend.position = "none") + ggtitle("Modèle à 3 classes") + ylab("")
paste("BIC = ",-classif2$bic)
paste("ICL = ",-classif2$icl)
paste("Entropie : ",(-classif2$icl+classif2$bic)/2)
```
]

---
# Exemple

Incertitudes de classification :

.pull-left[
```{r, echo=FALSE, fig.retina=2, fig.width=5, fig.height=5, fig.align='center', warning=F, message=F}
ggplot(data=geyser) + geom_point(aes(x=waiting,y=tx1,col=tx1,shape="Classe 1"),size=2) + geom_point(aes(x=waiting,y=tx2,col=tx2,shape="Classe 2"),size=2) + scale_color_gradient("Proba",high="#32CD32",low="red") +  geom_hline(yintercept = 0.5,col="darkgrey") + ylab("") + xlab("Temps entre 2 éruptions") + scale_shape_manual("",values=c(19,2))
```
]

.pull_right[
```{r, echo=FALSE, fig.retina=2, fig.width=5, fig.height=5, fig.align='center', warning=F, message=F}
ggplot(data=geyser) + geom_point(aes(x=waiting,y=tx21,col=tx21,shape="Classe 1")) + geom_point(aes(x=waiting,y=tx22,col=tx22,shape="Classe 2")) +
  geom_point(aes(x=waiting,y=tx23,col=tx23,shape="Classe 3")) + scale_color_gradient("",high="#32CD32",low="red") +
 ylab("") + xlab("Temps entre 2 éruptions") + scale_shape_manual("",values=c(19,2,12))
```

]

---
# <img src="pen-to-square-regular.svg" width="45" height="45" align="center"> Entropie d'une classification

Pour mieux comprendre l'entropie, plaçons nous dans le cas du modèle de mélange suivant:

$$Y \sim p_1\mathcal{P}(\lambda_1) + p_2\mathcal{P}(\lambda_2) + p_3\mathcal{P}(\lambda_3) + p_4\mathcal{P}(\lambda_4)$$

avec $p_1=p_2=p_3=1/5,p_4=2/5$ et $\lambda_1=1,\lambda_2=2,\lambda_3=3,\lambda_4=4$.

**Exercice**
> 1. Simuler un échantillon de taille $n=20$ selon la loi de $Y$ (voir fiche 3).
> 2. Donner l'expression de $t_k(X_i;\phi)=\mathbf{P}(Z_i = k | X_i ; \phi)$.
> 2. En supposant les $(p_k,\lambda_k)$ connus, calculer pour chaque observation, la probabilité qu'elle appartienne à chacune des 4 classes.
> 3. En se basant sur ces probabilités, proposer une classification.
> 4. Calculer l'entropie de la classification, définie par :
$$H(\phi ; \phi) = - \sum_{i=1}^n \sum_{k=1}^K  t_k(X_i;\phi) \ln t_k(X_i;\phi)$$

> 5. Comparer à l'entropie obtenue en affectant à la classe $k$ les observations pour lesquelles $t_k(X_i;\phi) > 0.5$, lorsque cela est possible



